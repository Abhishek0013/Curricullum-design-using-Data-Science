Title,Location,Company,Salary,Sponsored,Description
BHJOB15656_15470 - Data Engineer (Remote),"Ottawa, ON",Myticas Consulting,None,Organic,"The recruitment team at Myticas Consulting is looking for an experienced Data Engineer who would be interested in a remote contract opportunity offered within the Ottawa, ON region.
Security Clearance Required: Enhanced Reliability.
Project Scope:
IoT solution to ingest data from radio-spectrum analysis was built in Azure and needs to be migrated into AWS. This included some App migration, some containerization and some migration of R to Python.
Requirements:
AWS is critical.
Azure is a benefit.
Knowledge of R and Python.
Containerization are required.
Skills:
R
Python
Spark
SQL
Warehousing
Lakehouse
AWS Data Tools (EMR, Athena, Glue, S3)
AWS App Dev (S3, Lambda, Step Functions)
Docker
C++
Job is also known as: Data Engineer
INDMY"
Data Engineer,"Dorval, QC",Bombardier,None,Organic,"Data Engineer-MON16899

Description

BOMBARDIER

At Bombardier, our employees work together to evolve mobility worldwide - one good idea at a time. If you have a good idea, we’ll provide the environment where it will thrive and grow into a great product or customer experience. Your ideas are our fuel.

You will be part of the Aircraft Health Management System (AHMS) team and work with an emerging Digital Products team that is focused on one of the top priorities within the organization by introducing, implementing and sustaining Digital Products for Bombardier Aviation customers. You will work in close collaboration with cross functional teams and stakeholders across the organization such as Product Owners, User Experience UX/UI, Go-To-Market, Sales and Customer Support specialists. Join us and be an integral part of a dynamic and innovative team in one of the largest Business Aviation corporations.

In your role, you will:

Create and maintain optimal data pipeline architectures

Assemble large, complex data sets that meet functional / non-functional business requirements

Build the cloud-based infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources

Support the Aircraft Health Management System (AHMS) initiatives

Qualifications

As our ideal candidate,

You are able to troubleshoot issues quickly and effectively

You have experience working alongside and supporting multiple Agile development teams

You have experience building and optimizing complex data pipelines, architectures and data sets.

You are able to build processes supporting data transformation, data structures, metadata, dependency and workload management.

You have a successful history of manipulating, processing and extracting value from large disconnected datasets.

You have experience supporting and working with cross-functional teams in a dynamic environment.

We are looking for a candidate with 5+ years of experience in a Data Engineer or Data Science role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:

Experience with relational SQL and NoSQL databases, for example Postgres and Oracle.

Experience with Timeseries data

Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.

Experience with AWS cloud services: EC2, EMR, RDS, Redshift

Experience with data integration and surfacing via APIs

Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

You have the ability to influence others

You have strong oral and written English communications skills and functional French communication skills are desired

Bombardier is an equal opportunity employer and encourages women, Aboriginal people, persons with disabilities and members of visible minorities to apply.

Whether your candidacy is moving on to the next step of the hiring process or not, we will keep you informed by email or by phone. Join us at careers.bombardier.com

Your ideas move people.

Job: Applications Specialists
Primary Location: CA-QC-Montreal Dorval
Organization: Aerospace
Schedule: Full-time
Employee Status: Regular

Job Posting: 13.10.2020, 1:23:12 PM

Unposting Date: Ongoing"
Data Engineer,"Montréal, QC",LANDR,None,Organic,"LANDR is looking for a Data Engineer to join our Analytics team!

In this role, you will work in a client project environment with Product, Architecture and Applied Science Teams to contribute to the translation of complex AI/data science algorithms into scalable software, respectively to the envisioned Architecture to develop direct Business Value for our Products.

Core duties are:
Help to evolve and scale our data platform, with an eye towards growth of our business and reliability of our data
Work closely with the product and engineering teams, as well as other stakeholders from finance, customer success, marketing to understand the data needs of the business and produce processes that enable a better product and support growth decision-making
Generate architecture recommendations and the ability to implement them
Responsible for data governance, setting up alerts, and performing extensive data QA and validation
Improve, manage, and teach standards for script/code maintainability and performance
Help evolve our analytics practice by leveraging new data sources with ETL automation jobs and pipelines


You are a great fit if you have:
You have at least 5 years+ of software/data engineering experience, at least 2 of which has been in a Data Engineer role using a mix of SQL, Python, etc.
You learn quickly, regardless of the languages and technologies used
You always strive to build the minimally viable product before over-engineering
You have experience building scalable data pipelines and powerful reporting tools
You are comfortable working with REST, SOAP, or GRAPH API
You love solving complex problems with logical, well-architected solutions
You are well-versed in data modelling and have experience building distributed systems
Clear communicator who can gather technical requirements and explain technical intricacies
Experience working with Amazon RedShift, Google Cloud Platform, and/or Snowflake a strong asset
Predictive modelling and machine learning experience is a plus

Our office is in the Mile-End startup district of Montreal, conveniently located within walking distance of the orange metro line, and boasts an incredible view spanning the downtown skyline all the way to Mount Royal. Our modern, open-air office concept is complete with an audio-powered stage for hosting music events as fun as our 5@7’s!

We love music. We love musicians. Our mission is to give music makers the freedom to create and be heard.

Our company culture is central to our success and makes for an incredible workplace! We work for musicians by bringing our love of music to the office every day and we are continuously innovating. We are all contributing to the culture we all belong to and we come together to move ideas forward.

Here are a few perks that we enjoy at LANDR:
Competitive compensation, based on your role, experience and expertise
Health insurance coverage
Flexible hours to balance with your personal life
Immediate access to a doctor via Dialogue.co with a family plan
A BIXI pass
Weekly 5@7, library, weekly fruit basket, bagel Monday once a month and more!
All inclusive Social Committee
Sporting and social activities
Your birthday is a free day
Great open offices in the middle of the Mile End, with many conference rooms available to work in teams or individually quietly
LANDR Band
Apple computers (or Windows, to your liking)
Partnerships with local music events & festivals (MUTEK, Pop Montreal, Igloofest…)

We accept people as they are and firmly believe diversity is essential to our success! If you’re applying to LANDR, it’s because you want to contribute to our amazing company and culture, and we’re thankful for that!

We work hard to create the best experience for our employees and we’d love for you to be a part of it!

At LANDR, we don’t just accept diversity — we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products, and our community. LANDR is striving every day to create a more diverse workplace, and we strongly encourage applications from people of colour, women, LGBTQ people, people of any religion, age, or belonging to any marginalized community."
BHJOB15656_15505 - Data Engineer (Cloud) (Remote),"Ottawa, ON",Myticas Consulting,None,Organic,"The recruitment team at Myticas Consulting is looking for an experienced Data Engineer who would be interested in a remote contract opportunity offered within the Ottawa, ON region.
Security Clearance Required: Enhanced Reliability.
Requirements:
Azure (est 7 Years)
SQL/NoSQL DB
Apache Spark
Power BI
Data Infrastructure in Cloud Experience
Azure Data Factory / DataBricks
Event Hub
Job is also known as: Data Engineer, Cloud Engineer
INDMY"
Sr Data Engineer,"Montréal, QC",Thales,None,Organic,"Location: Montreal, CanadaDans des marchés en rapide évolution, les clients travers le monde font confiance Thales. Thales est une entreprise où les personnes les plus brillantes du monde entier se regroupent pour mettre en commun leurs idées et ainsi s'inspirer mutuellement. Dans tous les secteurs où œuvre Thales, notamment laérospatiale, le transport, la défense, la sécurité et l'espace, nos équipes darchitectes conçoivent des solutions innovantes qui rendent demain possible dès aujourdhui.Carrefour mondial de lintelligence artificielle, Montréal est le foyer du nouveau centre de recherche et de technologie spécialisé en intelligence artificielle (cortAIx) collaborant avec les principaux groupes canadiens de recherche en intelligence artificielle Montréal et Toronto. Sappuyant sur ses compétences dans les principaux marchés industriels, Thales donne vie l'intelligence artificielle au profit de ses clients tout en créant de passionnants emplois pour les chercheurs et les développeurs experts en intelligence artificielle en vue de trouver des solutions qui transformeront notre monde, du fond des océans aux confins de l'univers et du cyberespace. Ayant très tôt opté pour le modèle dinnovation ouverte et collaborative, Thales procède actuellement la création de la structure du centre de recherche et de technologie spécialisé en intelligence artificielle (cortAIx). Piloté par Thales, le centre cortAIx, en collaboration avec l'Institut québécois d'intelligence artificielle (MILA), l'Institut de valorisation des données (IVADO) et lInstitut Vector de Toronto, est situé dans le célèbre quartier Petite-Italie, au cœur de la communauté de linnovation Montréal.In fast changing markets, customers worldwide rely on Thales. Thales is a business where brilliant people from all over the world come together to share ideas and inspire each other. In aerospace, transportation, defence, security and space, our architects design innovative solutions that make our tomorrow's possible.Montreal a world leading AI hub, is home to new Centre of Research & Technology in Artificial Intelligence eXpertise (cortAIx) collaborating with leading Canadian AI research groups in Montreal and Toronto. With competencies in major industrial markets Thales is bringing artificial intelligence to life for our customers creating exciting jobs for AI researchers and developers who will create solutions that will transform our world from the bottom of oceans to the depths of space and cyberspace. As an early adopter of open, creative and collaborative innovation model, Thales is building the Centre of Research and Technology in Artificial Intelligence eXpertise (cortAIx). Led by Thales, cortAIx, in collaboration with the MILA (Artificial Intelligence Institute of Quebec), the IVADO (Institute of Data Valorization) and the Vector Institute of Toronto, is located in Montreals famous Little Italy, in the heart of Montreals innovation community.

Guavus is a young and fast-growing company whose mission is to provide Communication Service Providers (CSPs) with a competitive advantage in the ability to accurately understand their mobile subscribers behaviors and extract value from this knowledge.

We are at pivotal point in our history where big data innovation can impact businesses and individuals in new and unforeseen ways, but we need exceptionally smart people to join our team who are:
Passionate about getting the job done,
Relentless about flawless execution,
Committed to solving problems creatively, and
Believe in the collective intelligence to design, build and engineer extraordinary products and solutions that are useful to all.
If this sounds like you, please reach out we would love to hear from you.

Guavus is looking for a highly motivated and talented Sr. Data Engineer to participate in the development of the most advanced solutions in the Big Data space by using agile methodologies. The developer will actively participate and collaborate with data team to design and implement data pipelines integrating advanced AI/ML models.

Key Responsibilities:
Develop and maintain batch and streaming data pipelines with big data technologies such as Spark, Kafka, Hive, HDFS, HBase, Phoenix, Impala etc.
Analyze and implement proof of concepts related to big data technologies
Analyze new technologies (DB, Storage, Compute Engines)
Produce quality code that is well documented
Participate in code reviews and mentoring
Required Skills:
Degree in Computer Science or Engineering
Five (5) years of experience in a data engineer position
Experience in Cloud and non-Cloud based Hadoop ecosystem
Experience in data warehousing and ETL development
Fluent in Java & with some Scala knowledge
Fluent in SQL
Experience in performant and highly scalable applications
Experience in distributed framework and technologies e.g. Columnar Database, NoSQL and Hadoop
Experience in Linux and shell scripting
Basic knowledge or interest in Python
Fluent in English, both written and spoken
Preferred Skills:
Speaking French is an asset
Guavus est une jeune entreprise croissance rapide dont la mission est de fournir aux fournisseurs de services de communication (CSP) un avantage concurrentiel dans la capacité comprendre avec précision les comportements de leurs abonnés mobiles et tirer de la valeur de ces connaissances.

Nous sommes un moment charnière de notre histoire où l'innovation du Big Data peut avoir un impact sur les entreprises et les individus de manière nouvelle et imprévue, mais nous avons besoin de personnes exceptionnellement intelligentes pour rejoindre notre équipe qui sont:
Passionné de faire le travail,
Implacable pour une exécution sans faille,
Engagé résoudre les problèmes de manière créative, et
Croyez en l'intelligence collective pour concevoir, construire et concevoir des produits et des solutions extraordinaires qui sont utiles tous
Si cela vous ressemble, veuillez nous contacter, nous serions ravis de vous entendre.

Guavus est la recherche d'un Sr. Data Engineer hautement motivé et talentueux pour participer au développement des solutions les plus avancées dans l'espace Big Data en utilisant des méthodologies agiles. Le développeur participera activement et collaborera avec l'équipe de données pour concevoir et mettre en œuvre des pipelines de données intégrant des modèles avancés d'IA / ML.

Principales responsabilités:
Développez et maintenez des pipelines de données par lots et en streaming avec des technologies Big Data telles que Spark, Kafka, Hive, HDFS, HBase, Phoenix, Impala, etc.
Analyser et mettre en œuvre la preuve de concepts liés aux technologies Big Data
Analyser les nouvelles technologies (base de données, stockage, moteurs de calcul)
Produire un code qualité bien documenté
Participer aux revues de code et au mentorat
Compétences requises:
Diplôme en informatique ou en génie
Cinq (5) ans d'expérience dans un poste d'ingénieur de données
Expérience dans l'écosystème Hadoop Cloud et non basé sur le Cloud
Expérience de l'entreposage de données et du développement ETL
Parle couramment Java et avec quelques connaissances Scala
Maîtrise du SQL
Expérience dans des applications performantes et hautement évolutives
Expérience dans le cadre et les technologies distribués, par ex. Base de données en colonnes, NoSQL et Hadoop
Expérience dans les scripts Linux et shell
Connaissances de base ou intérêt pour Python
Maîtrise de l'anglais, écrit et parlé
Compétences préférées:
Parler français est un atout
Chez Thales, nous proposons des CARRIÈRES passionnantes, pas de simples emplois. Fort de ses 80 000 collaborateurs dans 68 pays, Thales a mis en place une politique de mobilité permettant, chaque année, des milliers d'employés de faire progresser leur carrière tant dans leur domaine dexpertise que dans de nouveaux domaines de compétences, cela aussi bien dans leur pays dorigine qu l'étranger. Ensemble, nous pensons quadopter une politique de flexibilité est une manière plus actuelle de travailler. Cest ici que commence votre parcours exceptionnel, postulez sans tarder!At Thales we provide CAREERS and not only jobs. With Thales employing 80,000 employees in 68 countries our mobility policy enables thousands of employees each year to develop their careers at home and abroad, in their existing areas of expertise or by branching out into new fields. Together we believe that embracing flexibility is a smarter way of working. Great journeys start here, apply now!Thales sengage promouvoir un lieu de travail diversifié et inclusif pour tous. Thales sengage fournir des accommodements toute les étapes du processus de recrutement. Les candidats retenus pour une entrevue qui ont besoin daccommodement sont priés den informer la suite de linvitation pour une entrevue. Nous travaillerons avec vous pour répondre vos besoins. Toutes les informations relatives l'accommodement fourni seront traitées de manière confidentielle et utilisées uniquement dans le but de fournir une expérience de candidat accessible.Thales is committed to a diverse and inclusive workplace for all. Thales is committed to providing accommodations in all parts of the interview process. Applicants selected for an interview who require accommodation are asked to advise accordingly upon the invitation for an interview. We will work with you to meet your needs. All accommodation information provided will be treated as confidential and used only for the purpose of providing an accessible candidate experience."
Data Engineer,"Toronto, ON",Impressico Business Solutions,None,Organic,"*Data Engineer, Impressico Business Solutions, Canada*
Impressico Business Solutions, a leader in providing result-driven digital transformation consulting is looking for an experienced, innovative, and highly motivated Data Engineer for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow. This is contract role based out in North York, Ontario.
Primary Responsibilities
Design, build and launch extremely efficient & reliable data pipelines to move data to our Data Warehouse/Data Mart.
Own end-to-end data quality for the data pipelines you build
Develop ETL routines to populate databases from multiple disparate data sources and create aggregates
Create and run data migrations across different servers and different databases including Enterprise CRM and ERP applications.
Perform complex data transformations, create/update stored procedures/functions, and optimize existing stored procedures/functions using indexing, temp tables, views, logic changes, etc.
Design/develop new systems and tools to enable stakeholders to consume and understand data faster
Data cleansing and manipulation using your expert SQL & programming skills
Troubleshoot data issues and present solutions to the issues
Prepare activity and progress reports regarding database & data health and status
Design and improve agile development processes as it applies to data and data structure design
Design, code and automate data quality checks, metrics, standards and guidelines
Work across multiple teams in high visibility roles and own the solution end-to-end
Requirements
BS or MS in Computer Science, Information Management, or related field
5+ years of experience as a Data Engineer.
Candidate must have a deep understanding of logical and physical data modeling for OLTP and OLAP systems.
Ability to translate a logical data model into a relational or non-relational solution as appropriate
Familiar with multiple relational platforms, recent MSSQL Server experience is required.
Hands-on expertise in database development using views, T-SQL, MSSQL and/or SQL scripts and SSIS packages and transformations. Experience building and troubleshooting SSAS cubes.
Fluent in using tools like SQL Server Management Studio or similar.
Recent experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies
Programming/Scripting experience in Windows (C#, PowerShell) as well as Unix/Linux environments (Python, Bash)
Job Types: Full-time, Contract
Experience:
Data Engineering: 6 years (Preferred)
Work remotely:
Yes, temporarily due to COVID-19"
Data Engineer Consultant,"Toronto, ON",Accenture,None,Organic,"ARE YOU READY to step up and take your technology expertise to the next level?

There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.

Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
As a data engineer, you will be building and testing data integration solutions for our client’s advertising arm Spectrum Reach
Develop analytics-based solutions that produce quantitative and qualitative business insights
Work with partners as necessary to integrate systems and data quickly and effectively, regardless of technical challenges or business environments
In this fast-paced, agile environment you and your teammates will build, enhance, and test customized ETL solutions to enable and develop data products for internal clients and third parties

WHO WE´RE LOOKING FOR?
Minimum 5 years of experience as a Data Engineer developing in a Data Warehouse and integration environment
Must have Strong SQL and UNIX/shell scripting skills
Nice to have: Agile and Teradata Proficient
Understands Data Warehouse Fundamentals
Good knowledge with Github
Clear understanding of the SDLC
Experience/Understand of Agile Dev
Eligibility to travel to US (once per quarter for meetings)

Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
WHAT´S IN IT FOR YOU?
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.


We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.
Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.
Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 505,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com."
Data Engineer,"Toronto, ON",Technical Standards & Safety Authority (TSSA),None,Organic,"Location
Office

City/Town
Toronto

Department Name
Information Services

Description
Since 1997, the Technical Standards and Safety Authority (TSSA) has delivered public safety services on behalf of the government of Ontario. With headquarters in west Toronto, the TSSA is a self-funded, not-for-profit organization that employs approximately 400 staff across Ontario. The roles within the TSSA are as diverse as the Province we serve. It takes all kinds of people with varied skillsets and backgrounds to meet our organizational goals, but our employees share a few key traits:
We are passionate about public safety.
We are highly skilled, problem-solvers.
We are collaborators who are invested in the communities we work in.
We are more than employees; we are serving the greater good.
We are committed to life-long learning and development.
We operate with the highest integrity.
TSSA provides our employees with competitive compensation, excellent benefits, and — most importantly — a sense of purpose. Our work is vital to the success and safety of the Province, and it’s our people who make it possible.

JOB PURPOSE
This individual supports TSSA Data and Business Intelligence team.

JOB RESPONSIBILITIES
Prepare and maintain data for extraction, transformation, and loading of data from a wide variety of data sources ensuring necessary mechanisms to guarantee data quality, and to support analytics initiatives.
Create effective ETLs/ELTs to move large volumes of data from various operational systems to dimensional data models.
Develop data pipelines leveraging various services on Azure and local data quality for data flows into the enterprise data warehouse .
Execute various aspects of data manipulation such as data profiling, data standardization / validation / cleansing, root cause analysis / remediation, data quality business rules technical specification and implementation, data quality measurement and reporting, metadata management, reference and master data management.
Ensure that the data sets provided to users are compliant with established governance and security policies.
Perform data quality assurance and testing according to risk assessment guidelines
Work closely with Data Manager on data modelling, consolidation and advanced data quality techniques and tools.
Execute data automation strategy by using innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable data preparation and integration tasks in order to minimize manual processes.
Ensure all automated processes preserve data integrity by managing the alignment of data availability and integration processes

JOB REQUIREMENTS
Experience in Data Integration, ETL, Reporting, ESB, Data Warehousing, Data Migration.
Minimum 6 years of hands-on experience working in data operations, out of which minimum 3 + years of experience in MDM solutions, preferably in complex and diverse systems environment.
Hands-on experience an enterprise data quality management tool, data integration or MDM tool. (Atacama, Talend, Calibra, etc)
Experience in data integration platforms and application integration.
Strong skills in logical/conceptual data modeling.
Strong skills in SQL, at least two relational database systems.
Knowledge of data governance concepts
Hands-on experience in Unix shell commands, XML, SOAP / REST / JSON, and application servers.
Skills in Java, Spark, and Databricks.
Experience working with BI tools and technologies (Power BI, Java, etc)
Experience with Cloud Services.
Knowledge of two or more scripting/programming languages (e.g., R, Python, VBA).
CRITICAL VALUES AND ATTRIBUTES
Strategic Leadership
Analytical Thinking/Data Integrity
Business Innovation
Financial Management
Adaptability/Change Management
Customer Focus
Teamwork
Communication
Ethics & Compliance

ORGANIZATIONAL STRUCTURE
Position reports to the Manager of Data and BI
Position has #/no direct reports

LOCATION
345 Carlingview Dr., Toronto, ON

Equity Statement

At TSSA, we value the diversity in our workplace and we are committed to employment equity. We encourage all qualified persons to apply. Only those selected for an interview will be contacted."
Principal Data Engineer,"Toronto, ON",Nomis Solutions,None,Organic,"Data Architect / Principal Data Engineer

We are seeking a talented Data Architect / Principal Data Engineer with a strong technical background and experience with fast paced technology startups.

Who We Are & What We Build

We partner with Banks and FinTechs on their journey to best-in-class pricing technology and analytics so that they deliver more value to their customers, employees and shareholders. Our top-notch people, proven technology, and innovative analytics are tackling big data challenges at banks and lenders every day. We deliver market-leading cloud-based Pricing & Profitability Management solutions and insights for the Banking & Financial Services industry leveraging cutting-edge behavioral data science. We are a Blue Chip venture-backed company with the vision to transform the consumer banking landscape.

Who You Are

Nomis is developing a highly scalable, super-efficient, big data platform that feeds our predictive analytics and software products. You are an ideal candidate if you have hands-on experience implementing data management frameworks that power analytical applications using relational, unstructured AND big data technologies.

Job Responsibilities:

Design, develop, and maintain highly scalable infrastructure for data pipelines to ingest data from external sources, store in data stores that are scalable, secured, and accessible by other services in the organization
Work in a cross-functional team of software architects, modeling scientists, project managers and other key stake holders to drive overall architecture.
Work with professional services to operationalize this data platform for various customer implementations.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics

Qualifications:

Extensive experience in software development, serverless architecture, and design-driven development
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets
Experience building processes supporting data transformation, data structures, metadata, dependency and workload management
Experience with AWS cloud services: Lambda, EC2, EMR, RDS, Redshift, S3
Experience with big data tools: Hadoop, HIVE / PIG, Spark, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Mongo.
Experience with data pipeline and workflow management tools such as Airflow

Experience:

10+ years of experience in Data Architecture
4+ years of experience in Big Data Architecture
4+ years of experience on AWS
Strong background in relational databases
Strong background in MapReduce/Hadoop frameworks
Experience in ETL frameworks
Expertise in job scheduling (custom and open source), data quality, metadata management and monitoring systems
Experience handling retail bank/financial institution data a plus
BS in computer science, or other related discipline"
Senior Data Engineer,"Toronto, ON",Coursera,None,Organic,"Coursera is a leading online learning platform for higher education, where 71 million learners from around the world come to learn skills of the future. More than 200 of the world’s top universities and industry educators partner with Coursera to offer courses, Specializations, certificates, and degree programs. Thousands of companies trust the company’s enterprise platform Coursera for Business to transform their talent. Coursera for Government equips government employees and citizens with in-demand skills to build a competitive workforce. Coursera for Campus empowers any university to offer high-quality, job-relevant online education to students, alumni, faculty, and staff. Coursera is backed by leading investors that include Kleiner Perkins, New Enterprise Associates, Learn Capital, and SEEK Group.

Data Engineering is unique at Coursera. Our team doesn’t simply build reports on demand. Rather, we build the semantic infrastructure and products that empower our internal and external customers with the data to innovate and perform their jobs better.

We’re looking for a talented and driven senior data engineer with a keen eye for data and business to help us build and scale our platform. Our ideal candidate is an independent, analytically-minded individual with strong data modeling and software engineering skills, who shares our passion for education. In this role, you’ll directly work with cross-functional teams(product, engineering, services) to design, develop, and deploy data solutions for our enterprise learners and admins.
Your responsibilities:
Architect scalable data models and build efficient and reliable ETL pipelines to bring the data into our core data lake
Design, build, and launch visualization and self-serve analytics products that empower our internal and external customers with flexible insights
Build data expertise, and partner with data scientists and product engineers to define and standardize business rules and maintain high-fidelity data
Partner with other engineers in the development of new tools to enable our customers to understand and access data more efficiently
Work cross-functionally (eg: product managers, engineers, business teams) to support new product and feature launches
Your skills:
5+ years experience in a data-related field, including data engineering, data warehousing, business intelligence, data visualization, and/or data science
Strong software engineering skills and at least one scripting language (e.g., Python)
Proficient with relational databases and SQL
Familiarity and experience with big data technologies (eg: Hive, Spark, Presto) preferred
Ability to communicate technical concepts clearly and concisely
Independence and passion for innovation and learning new technologies
If this opportunity interest you, you might like these courses on Coursera -
Big Data Specialization
Big Data Essentials - HDFS, MapReduce and Spark
Data Warehousing for Business Intelligence
Coursera is an Equal Employment Opportunity Employer and considers all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, age, marital status, national origin, protected veteran status, disability, or any other legally protected class.

If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process, please contact us at accommodations@coursera.org.

Please review our CCPA Applicant Notice here."
Google Cloud Data Engineer,"Toronto, ON",Accenture,None,Organic,"Job Description
Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements and the way we collaborate, operate and deliver value provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.
As part of our Intelligent Software Engineering practice, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a growing network of technology experts who are highly collaborative taking on today’s biggest, most complex business challenges. We will nurture your talent in an inclusive culture that values diversity. Come grow your career in technology at Accenture!
Google Cloud Platform (GCP) Data Engineers will be responsible for architecting transformation and modernization of enterprise data solutions on GCP cloud integrating native GCP services and 3rd party data technologies. A solid experience and understanding of considerations for large scale architecting, solutioning and operationalization of data warehouses, data lakes and analytics platforms on GCP is a must. We are looking for candidates who have a broad set of technology skills across these areas and who can demonstrate an ability to design right solutions with appropriate combination of GCP and 3rd party technologies for deploying on GCP cloud.
Key responsibilities may include:
Work with implementation teams from concept to operations, providing deep technical subject matter expertise for successfully deploying large scale data solutions in the enterprise, using modern data/analytics technologies on premise and cloud
Experience in building solution architecture, provision infrastructure, secure and reliable data-centric services and application in GCP
Work with data team to efficiently use Hadoop/Cloud infrastructure to analyze data, build models, and generate reports/visualizations
Integrate massive datasets from multiple data sources for data modelling
Implement methods for automation of all parts of the predictive pipeline to minimize labor in development and production
Formulate business problems as technical data problems while ensuring key business drivers are captured in collaboration with product management
Knowledge in machine learning algorithms especially in recommender systems
Extracting, Loading, Transforming, cleaning, and validating data
Designing pipelines and architectures for data processing
Creating and maintaining machine learning and statistical models
Querying datasets, visualizing query results and creating reports

Basic Qualifications:
Minimum 3 year of designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of GCP data and analytics services in combination with 3rd parties - Spark, Cloud DataProc, Cloud Dataflow, Apache Beam, BigTable, Cloud BigQuery, Cloud PubSub, Cloud Functions, etc.
Minimum 1 year of hands-on experience analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on GCP cloud using GCP/3rd party services
Minimum 1 year of designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Java, Python, Scala etc.
Minimum 1 year of architecting and implementing next generation data and analytics platforms on GCP cloud
Minimum 1 year of designing and implementing data engineering, ingestion and curation functions on GCP cloud using GCP native or custom programming
Minimum 1 year of experience in performing detail assessments of current state data platforms and creating an appropriate transition path to GCP cloud
Hands-on GCP experience with a minimum of 1 solution designed and implemented at production scale
Bachelor's degree or equivalent (minimum 12 years) work experience. If Associate Degree, must have minimum 6 years work experience
Ability to meet travel requirements (100% Monday-Thursday)
Preferred Qualifications:
Minimum 1 year of experience in architecting large-scale data solutions, performing architectural assessments, crafting architectural options and analysis, finalizing preferred solution alternative working with IT and Business stakeholders
1 year of hands-on experience designing and implementing data ingestion solutions on GCP using GCP native services or with 3rd parties such as Talend, Informatica
1 year of hands-on experience architecting and designing data lakes on GCP cloud serving analytics and BI application integrations
Minimum 1 year of experience in designing and optimizing data models on GCP cloud using GCP data stores such as BigQuery, BigTable
Minimum 1 year of experience integrating GCP or 3rd party KMS, HSM with GCP data services for building secure data solutions
Minimum 1 year of experience introducing and operationalizing self-service data preparation tools (e.g. Trifacta, Paxata) on GCP
Minimum 1 year of architecting and operating large production Hadoop/NoSQL clusters on premise or using Cloud services
Minimum 1 year of architecting and implementing metadata management on GCP
Architecting and implementing data governance and security for data platforms on GCP
Designing operations architecture and conducting performance engineering for large scale data lakes a production environment
Craft and lead client design workshops and provide tradeoffs and recommendations towards building solutions
2+ years of experience writing complex SQL queries, stored procedures, etc
Google Cloud Platform certification is a plus
Professional Skill Requirements:
Excellent communication (written and oral) and interpersonal skills
Proven ability to work creatively and analytically in a problem-solving environment.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.
Our Commitment to Inclusion & Diversity At Accenture, inclusion and diversity are fundamental to our culture and embedded in our core values. We are committed to creating a workforce where our people can feel comfortable, be themselves and contribute. Like Canada itself, Accenture employees represent a tremendous variety of cultures, ethnicities, beliefs, backgrounds and languages. We offer an inclusive environment regardless of personal characteristics such as ethnicity, religion, gender, sexual orientation, gender identity or expression, age or disability. Requesting an Accommodation Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired. If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter. Other Employment Statements Job candidates are not required to disclose any offence for which a pardon has been granted."
"Senior Software Development Engineer – Big Data, AWS EMR","Toronto, ON","Amazon Web Services Canada, In",None,Organic,"Bachelor’s Degree in Computer Science or related field or equivalent work experience.
7+ years professional experience in software development.
Strong knowledge of Computer Science fundamentals in object-oriented design, data structures, algorithm design, and problem solving.
Production level experience in one modern programming language such as Java, Scala or .
Want to change the world with Big Data and Analytics? Come join us on the Amazon EMR team in Amazon Web Services!

Amazon EMR is a web service which enables customers to run massive clusters with open-source data processing frameworks like Apache Hadoop, Spark, Presto, Hive, HBase and more, with the ability to effortlessly scale up and down as needed. We run large number of customer clusters, enabling a variety of customer use cases including analytics and web-scale machine learning.

We are doing innovative work on the open source data processing frameworks with the goal of making the EMR the best place to run big data workloads. We’re looking for top engineers to design and build features and accelerate the performance.

You will work backwards from the customer needs and you will get to do everything from designing and building large scale systems and cutting-edge features for the savviest customers in the business.

You will have a chance to work with the open source community and contribute significant portions its software to open source projects including Hadoop, Spark, Presto, Hive and HBase. You need to not only be a top software developer with excellent programming skills, an understanding of distributed systems and parallel data processing, and a stellar record of delivery but also excel at leadership and customer obsession and have a real passion for massive-scale computing. If you want to truly test your mettle against the hardest challenges in distributed systems to build solutions for large scale problems in a wide variety of domains, come join our group.

Your responsibilities will include:
Designing and building the next-generation technologies that will make EMR the best environment to run large-scale data processing workloads.
Working on complex problems in distributed systems and query engines.
Translation of complex functional and technical requirements into detailed architecture and design.
Deliver systems and features with top-notch quality, on time.
Own the software development process end-to-end, including: working with engineers and product managers to develop requirements; designing, architecting, planning, implementing, and testing new systems and features; deploying, and operating the production EMR systems.
By joining our team, you will get to work with a minimum of technical supervision, while playing a variety of roles as needed to respond efficiently to multiple program priorities. You will get to collaborate with some of the best and brightest minds in the industry. You'll enjoy a competitive salary, great benefits, a creative and agile work environment, and the exciting opportunity to be part of a fast-paced and growing team and one of the most innovative technology companies - but most of all, you will get the satisfaction of making products that millions use everyday to great effect!

To learn more about EMR and the team:
AWS EMR: https://aws.amazon.com/emr/
AWS Big Data Blogs: https://aws.amazon.com/blogs/big-data/
Knowledge of and contribution to the Hadoop ecosystem.
Experience with building distributed systems, query engines and database systems.
Experience developing complex production-quality software systems.
Ability to take a project from scoping requirements through actual launch of the project.
Experience in communicating with users, other technical teams, and management to collect requirements, describe software product features, and technical designs.
Experiencing mentoring junior software development engineers and driving engineering excellence.
Strong knowledge of Computer Science fundamentals in object-oriented design, data structures, algorithm design, and problem solving.
Meets/exceeds Amazon’s leadership principles requirements for this role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us"
Ingénieur de données/ Data Engineer,"Montréal, QC",GenAIz,None,Organic,"GenAIz, une division d'Uni3T, est une jeune et dynamique société de développement de logiciels qui est active dans le domaine des sciences de la vie et de l'industrie pharmaceutique. Notre mission est d'accroître le bien-être collectif en accélérant la création de meilleurs produits, processus et traitements, grâce à un assistant d'innovation de pointe.

Nous recherchons un ingénieur de données avisé pour rejoindre notre équipe croissante d'experts en analyse. Il sera chargé d'étendre et d'optimiser l'architecture de notre pipeline de données, ainsi que d'optimiser le flux et la collecte de données pour les équipes interfonctionnelles. Le candidat idéal est un créateur de pipeline de données et un collecteur de données expérimenté qui aime optimiser les systèmes de données et les construire à partir de la base. L'ingénieur en données, qui relève du directeur de la science des données, soutiendra nos développeurs de logiciels, nos architectes de bases de données, nos analystes de données et nos scientifiques en matière de données dans le cadre des initiatives relatives aux données et veillera à ce que l'architecture optimale de livraison des données soit cohérente tout au long des projets en cours. Il doit être autonome et à l'aise pour répondre aux besoins en données de plusieurs équipes, systèmes et produits. Le candidat idéal sera enthousiaste à l'idée d'optimiser ou même de reconcevoir l'architecture de données de notre entreprise pour soutenir notre prochaine génération de produits et d'initiatives de données.

Le générique masculin est utilisé sans discrimination et uniquement dans le but d'alléger le texte.

Responsabilités :
Concevoir, construire et maintenir des pipelines de données évolutifs pour la collecte, le traitement, la transformation et le stockage de grands ensembles de données.
Identifier, concevoir et mettre en œuvre des améliorations des processus internes : automatisation des processus manuels, optimisation de la livraison des données, reconception de l'infrastructure pour une plus grande extensibilité et une haute disponibilité.
Construire l'infrastructure en nuage en temps réel, évolutive et hautement disponible nécessaire à l'extraction, la transformation et le chargement optimal de données provenant d'une grande variété de sources de données.
Collaborer étroitement avec les parties prenantes, notamment les équipes chargées de la direction, des produits, des données et de la conception, afin de les aider à résoudre les problèmes techniques liés aux données et de répondre à leurs besoins en matière d'infrastructure de données.
Mettre en œuvre et maintenir des services de données et des outils de consommation de données qui utilisent le pipeline de données pour fournir des informations exploitables sur les mesures clés de performance de l'entreprise.
Participer et partager vos idées lors de discussions sur la conception technique et l'architecture.
Établir des politiques, des normes et des pratiques pour la gouvernance des données : classification, inventaire, modélisation, ETL et technologies de streaming.
Gérer, organiser et hiérarchiser le travail de manière indépendante.

Qualifications et compétences requises :
Minimum 5 ans d'expérience dans la construction et l'optimisation de pipelines de données, d'architectures et de grands ensembles de données dans le passé ;
Au moins 2 ans d'expérience dans la création et le soutien de pipelines de données en temps réel dans le nuage ;
Capable de comprendre et de configurer une infrastructure en nuage évolutive, automatisée et hautement disponible (de préférence sur Google Cloud) pour l'ingestion, le streaming, la transformation et le chargement de données ;
Être à l'aise dans un environnement AGILE ;
Maîtrise du langage de programmation Python ;
Expérience pratique des techniques de streaming en utilisant Kafka et Spark (PySpark) ;
Bonne connaissance des principes de sécurité applicables aux solutions de pipeline de données distribuées
Connaît bien les normes de données relatives au domaine des soins de santé (par exemple, HIPAA, FHIR, etc.)
Doit savoir comment traiter les données non structurées et semi-structurées ;
Doit connaître les fichiers de données au format JSON et leur traitement ;
Connaissance d'au moins une base de données NoSQL (MongoDB de préférence) ;
Doit avoir mis en place au moins un pipeline de données en production ;
Doit avoir au moins une expérience de la mise en œuvre de la construction et du déploiement d'un pipeline de données dans l'infrastructure Google Cloud ;
Communicateur exceptionnel, à l'oral comme à l'écrit, avec une forte passion pour la réalisation de projets dans un environnement où les délais sont courts ;

Avantages :
Salaire de base compétitif + bonus (basé sur les efforts de l'entreprise, de l'équipe et de l'individu) ;
Couverture d'assurance complète ;
Crédit téléphonique et kilométrage pendant les missions couvertes par l'entreprise ;
Grande culture d'entreprise avec des possibilités d'évolution de carrière ;
Poste permanent à temps plein ;
Situé entre Griffintown et le Vieux-Montréal, dans le quartier de la Cité du Multimédia, qui fait maintenant partie du centre holistique de Montréal pour l'innovation, l'éducation et l'entrepreneuriat : le Quartier de l'Innovation.

Tous les candidats doivent être légalement autorisés à travailler au Canada au(x) lieu(x) spécifié(s) ci-dessus et, le cas échéant, doivent posséder un permis de travail ou un permis d'études valide qui leur permet de satisfaire aux exigences du poste.

GenAIz est fier d'être un employeur souscrivant au principe de l'égalité des chances. Tous les candidats seront pris en considération pour un emploi sans tenir compte de l'âge, de la couleur, du congé pour raisons familiales ou médicales, de l'identité ou de l'expression du sexe, de l'état civil, de l'état médical, de l'origine nationale, du handicap physique ou mental, de l'affiliation politique, de la race, de la religion, du sexe (y compris la grossesse), de l'orientation sexuelle ou de toute autre caractéristique protégée par les lois, règlements et ordonnances applicables.

Nous remercions tous les candidats pour leur intérêt, mais seuls les candidats sélectionnés seront contactés.

******************************************************************************************************************************
GenAIz, a division of Uni3T, is a young and dynamic software development company that is active in the life science and pharmaceutical industry. Our mission is to Increase collective well-being by accelerating the creation of better products, processes and treatments, through a state-of-the-art innovator’s assistant.

We are looking for a savvy Data Engineer to join our growing team of analytics experts and will be responsible for expanding and optimizing our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer, reporting to the Director of Data Science, will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities:
Design, build and maintain scalable data pipelines for the collection, processing, transformation, and storage of large datasets.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability and high availability.
Build the real-time, scalable, and highly available cloud infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Collaborate closely with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Implement and maintain data services and data consumption tools that utilize the data pipeline to deliver actionable insights into key business performance metrics.
Participate and share your ideas in technical design and architecture discussions.
Establish Policy, Standards and Practices for Data Governance Classification, Inventory, modeling, ETL and streaming technologies.
Manage, organize, and prioritize work independently.

Qualifications and skills required:
Min 5+ years experience building and optimizing data pipelines, architectures, and large data sets for the past;
Min 2 years experience building and supporting real-time cloud data pipelines;
Able to understand and configure scalable, automated, highly available cloud infrastructure (preferably on google cloud) for data ingestion, streaming, transformation, and load;
Comfortable in an AGILE environment;
Well versed with Python programming language;
Hands-on experience with streaming techniques using Kafka and Spark (PySpark);
Well versed with security principles applicable for distributed data pipeline solutions
Well acquainted with data standards related to the healthcare domain (for ex. HIPAA, FHIR etc.)
Must know how to handle unstructured and semi-structured data;
Should be familiar with JSON format data files and their processing;
Knowledge of at-least one NoSQL database (MongoDB preferred);
Must have implemented at least one data pipeline in production;
Must have at least one implementation experience in building and deploying a data pipeline in the google cloud infrastructure;
Exceptional verbal and written communicator, with a strong passion to get things done in a fast-paced deadline‐oriented environment;

Benefits:
Permanent Full-time position
Competitive base salary + bonus
Comprehensive insurance coverage
Dynamic company culture with career development opportunities
Located between Griffintown and the Old Montreal, in the Cité du Multimédia neighborhood, now part of Montreal’s holistic hub for innovation, education, and entrepreneurship: the Quartier de l’Innovation

GenAIz Group is an equal opportunity employer. All applicants will receive consideration for employment without regard to age, color, family or medical care leave, gender identity or expression, marital status, medical condition, national origin, physical or mental disability, political affiliation, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.

We thank all applicants for their interest, only selected candidates will be contacted."
Data Engineer II,"Toronto, ON",AIR MILES,None,Organic,"It's fun to work in a company where people truly BELIEVE in what they're doing !
As part of the Data Hub team at AIR MILES, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flows and collection for cross functional teams. The pipeline needs to be scalable, repeatable, and secure. You will work with some of the largest and most varied data sets (both batch and real-time) in Canada. You will expand and develop the AIR MILES Cloud analytical platform that enables business users, data analysts and data scientists to make data driven decisions, build innovative data products and roll out advanced analytics.
What will you bring?
Ability and desire to work in our collaborative environment: open team room, pair programming and fluid interactions with all products and operations teams.
Focusing on building solutions utilizing an agile approach: close relationships with Product Managers, communicating and digesting real time feedback, and working smartly to build story cards on daily basis.
Passionate about Big Data and the latest trends and developments. We strongly believe in and encourage continuous learning.
You are self-driven, need minimal supervision and comfortable pushing your own projects and getting things done.
Experience with Python, Spark, and SQL
Experience building ‘big data’ pipelines, architectures, and datasets
Experience with Amazon AWS and other cloud platforms
Experience with Databricks
Experience with Agile methodologies as well as familiar with CI/CD tools (Jenkins, Travis, github)
Experience in ETL and Data Modeling preferred
Experience in designing and implementing streaming applications is preferred
Fully understand standard architecture methodologies, processes and best practices
About AIR MILES
Today, there are more ways than ever to engage shoppers. At AIR MILES, we believe that understanding the people behind the purchase is key to winning their hearts – and their wallets. For over two decades and from more than fifty locations around the globe, we have paired expertise in shopper behavior with advanced analytics to uncover the data-driven insights that drive successful loyalty, marketing and merchandising solutions. At AIR MILES, we know that in coming together we are at our strongest – and that together we can help shape the future for our clients, their shoppers and our communities. AIR MILES is an Alliance Data company.
Alliance Data is an Equal Employment Opportunity employer. Accordingly, we will make reasonable accommodations to respond to the needs of people with disabilities in accordance with legislation.
Alliance Data participates in E-Verify.
Information Systems
Job Type:
Regular"
Data Engineer Azure,"Ottawa, ON",Bevertec CST Inc,None,Organic,"Job 11528
Data Engineer (all levels)
6-month initial term contract
Location: Remote during Covid19. Onsite in either Ottawa OR Toronto
Current Government of Canada Security Clearance is a MUST HAVE
WHO WE´RE LOOKING FOR?
Minimum 5 years of experience as a Data Engineer
Must have hands-on experience with Spark and Hadoop
Must have experience with one of the Cloud Technologies (preferably with Azure)
Azure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS
Experience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.
2+ years of experience with at least one SQL language such as T-SQL or PL/SQL
2+ years of work experience with ETL and data modeling
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience in both batch and stream processing technologies
Experience with object-oriented/object function scripting languages: Java, C++, Scala
Machine learning experience with Spark or similar
Must be eligible for security clearance
Microsoft /AWS Certified: Azure Data Engineer Associate/ AWS Certified Data Analytics (Specialty)
Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work
Requirements:
Azure is critical.
Knowledge of R and Python.
Containerization are required.
Skills:
R
Python
Spark
SQL
Warehousing
Lakehouse
AWS Data Tools (EMR, Athena, Glue, S3)
AWS App Dev (S3, Lambda, Step Functions)
Docker
C++
About Bevertec:
For 35 plus years Bevertec has been at the forefront of providing top level technology professionals to our clients across Canada and beyond. Bevertec specializes in technology professionals but looks to fill all professional roles that help expand and grow our client’s business and advance our candidates careers.
Remaining at the forefront of providing staffing solutions has been no easy task for us, that is why we depend on top-caliber candidates like yourself and our team of business development managers to provide you with the opportunities to work with leaders within the IT realm to meet your aspirations. As proof of our continued success Bevertec places candidates with leaders in the public and private sector and continuous remains a vendor of record with large organizations across the country.
Lean more at www.bevertec.com
How to apply:
Click the “Apply to Job” button and follow the instructions to submit your resume or contact the lead recruiter, Mark Shaw at mshaw@bevertec.com or 416-695-7525 ext: 2241.
Thank you for your interest in your next great opportunity with Bevertec
Only those selected for an interview will be contacted, Bevertec is committed to Employment Equity and encourages applications from all qualified candidates. In accordance with the Accessibility for Ontarians with Disabilities Act Bevertec will provide accommodations throughout the recruitment process, should you require such accommodations please contact our Human Resources department.
BEV123"
"Azure Data Engineer - Toronto, Canada","Toronto, ON",New Signature,None,Organic,"Join a team of passionate thought leaders in a dynamic and collaborative environment! New Signature's Global Delivery Center is growing fast and we're looking for our next Azure Data Engineer to join us.

What impact will you have in this role?

Every role at New Signature is equally important in the grand scheme of things and everything we do is team work. Through team work, building relationships internally and at times with clients, understanding context at all times and what is important, and getting comfortable with influence and persuasion you will stand out as an expert in your field.
As we continue to scale we're looking for the market's best Azure Data & AI specialists to help us grow our business' fastest growing practice. You'll be working with the industry's biggest players, delivering innovative greenfield Data Platform builds, Data Integration programmes and implementing bespoke High-Level Data Architectural designs.

What type of experience do you need to be successful in this role?

Strong experience using the Microsoft Azure Data Stack (ADFv2, Azure SQL DB, Azure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Analysis Services, Cosmos DB)
Azure data migration patterns
Knowledge of C# essential
Agile methodology experience essential
CI/CD, Azure DevOps experience, highly desirable
Customer/client-facing consulting skills
Ability and desire to mentor junior team members.
3+ years current and deep experience with implementing large engagements. Proven experience managing projects through the entire project lifecycle. This includes managing multi-phase/multi-dimensional/multi-resource projects to conclusion while maintaining high customer satisfaction
3+ years experience assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms
2+ years of experience and advanced domain knowledge in one or more vertical industries: manufacturing, financial services, government, legal, healthcare, property management

What personality traits and other capabilities are important for this role?

Our consultants are self-motivated and pragmatic with strong problem-solving skills and a passion for crafting great solutions coming with a wealth of experience or talent that enables quality software delivery. They understand the best approach to architecture and development is through blending technologies and methodologies appropriate to the task at hand. The goal is to contribute to the clients' long lasting success to enable us to expand our business and clientele.

Security Responsibility:
All employees must act in accordance with New Signature's corporate security standards.
ABOUT NEW SIGNATURE
New Signature is a cloud-first, full-service Microsoft partner committed to delivering innovative technology solutions that solve human challenges. Behind every interaction is our dedication to provide outstanding experiences and to build authentic relationships with those around us. We are passionate about driving transformational results for clients across all company sizes, geographies and industries. The New Signature team delivers full lifecycle solutions—from project inception and planning, through deployment to ongoing support and maintenance.
New Signature was named the top Microsoft partner in the United States and the United Kingdom in 2014 and again in the United States in 2015—becoming the first partner ever to win the prestigious US Partner of the Year award two years in a row. With over 600 individual technology certifications, New Signature is a recognized expert at the forefront of Microsoft advancements and couples these powerful technologies with exceptional services to empower our customers, colleagues, and community.
OUR CORE VALUES
Our employees are driven by our values and know that they make a positive difference every time that they help a customer to solve their challenges. Our focus on delivering great customer experiences empowers our people to build rewarding relationships that contribute to our positive work environment. You can learn more about our culture here: New Signature Culture


Human
We use our hearts and minds to collaborate for success.
We harness technology to drive business, but we never let that replace our human connections. We use our hearts and minds to collaborate for success and instill confidence in our customers through relationships forged from trust.


Generous
We are giving and respectful.
With our efforts to always be generous, we elevate our service level with empathetic and considerate communications and actions. We always find a way to support our customers and colleagues by giving of our time and talent and equally respecting the time and talent of others.


Authentic
We tell it as it is, with positive intent.
Being authentic helps to nurture our strong and trusted relationships. We are honest, transparent, and reliable. When you partner with New Signature, you are partnering with a group of purposeful, outcome-driven and results-oriented professionals.


Innovative
We push the boundaries at the intersection of people, process and technology.
For us, there are no limit to our dreams. We continually innovate and push boundaries at the intersection of people, process, and technology to bring our customers and colleagues the best solutions first.

EQUAL EMPLOYMENT OPPORTUNITY
As a Global Cloud Transformation Consultancy business, New Signature understands diversity and inclusion in the workplace brings benefits to our customers, our business and most importantly, our people. We are committed to being an inclusive employer and we provide equal employment opportunities to all employees and applicants for employment.
New Signature prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other factors protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including all aspects of the recruiting and employment life-cycle at New Signature.

EMPLOYMENT ELIGIBILITY
New Signature requires the candidate to prove eligibility to work in the United States or Canada (depending on the location of the job) within three days of being employed. All final candidates will be asked to complete a background check in both the US and Canada. These record checks can include any or all of the following: education verification, employment verification, drug screening and criminal record check. Positions that require significant travel may also require a driving record check.
For US Applicants: New Signature is an E-Verify employer. E-Verify is a web-based system that allows enrolled employers to confirm the eligibility of their employees to work in the United States. E-Verify employers verify the identity and employment eligibility of newly hired employees by electronically matching information provided by employees on the Form I-9, Employment Eligibility Verification, against records available to the Social Security Administration (SSA) and the Department of Homeland Security (DHS). E-Verify is only used upon acceptance of a job offer and completion of the Form I-9.
Click here for the E-Verify Participation Poster (available in English and Spanish)
If you require this notice in another language provided by DHS, please contact us at 202.452.5923
Click here for more information on Your Right to Work or Visit the USCIS website here for more information on E-Verify
For Canadian Applicants: New Signature is committed to working with and providing reasonable accommodation to individuals with disabilities. In accordance with the Accessibility for Ontarians with Disabilities Act, 2005 and the Ontario Human Rights Code, New Signature will provide accommodations throughout the recruitment and selection process to applicants with disabilities. To request a reasonable accommodation, please call 416.971.4267. Please ensure to provide your name, the best way to contact you, a detailed description of the nature of any accommodation that you may require (including any materials or processes that can be used to ensure your equal participation)."
Senior Data Engineer,"Toronto, ON",Accenture,None,Organic,"ARE YOU READY to step up and take your technology expertise to the next level?

There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.
Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
As a data engineer, you will be building and testing data integration solutions for our client’s advertising arm Spectrum Reach
Develop analytics-based solutions that produce quantitative and qualitative business insights
Work with partners as necessary to integrate systems and data quickly and effectively, regardless of technical challenges or business environments
In this fast-paced, agile environment you and your teammates will build, enhance, and test customized ETL solutions to enable and develop data products for internal clients and third parties

WHO WE´RE LOOKING FOR?
Minimum 8 years of experience as a Data Engineer developing in a Data Warehouse and integration environment
Must have Strong SQL and UNIX/shell scripting skills
Nice to have: Agile and Teradata Proficient
Understands Data Warehouse Fundamentals
Good knowledge with Github
Clear understanding of the SDLC
Experience/Understand of Agile Dev
Eligibility to travel to US (once per quarter for meetings)
Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
WHAT´S IN IT FOR YOU?
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.


We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.
Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.
Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 505,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com."
Senior Data Engineer,"Montréal, QC",Lightspeed POS (DU),None,Organic,"As a part of Lightspeed's Retail product group, you will be contributing to initiatives that will expand our reach into new markets and countries while enhancing our omnichannel commerce platform. Your work will also have a direct impact on supporting new and existing customers on their quest to enter the cloud era. At Lightspeed, we are dedicated to bringing cities and communities to life by powering SMBs. Come and help us build our communities!
What you'll be responsible for
Work alongside some of the most brilliant minds in the industry, you will contribute to omnichannel product by building new features and creative workflows.
Report to the Development Manager and have frequent interactions with Product Managers, Product Designers as well as Platform Teams.
Take part in daily agile ceremonies, brainstorm on innovative ideas that challenge the status quo and implement solutions
Act as an advocate for highest code quality and ownership, while contributing to the health of the infrastructure
Raise the bar and elevate your team by sharing knowledge and best practices, while always seeking improvement and progress
What you'll be bringing to the team
Senior level experience developing reliable, highly available and scalable software (preferably in a scalable SaaS setting)
Strong experience building modern cloud infrastructures
Experience programming in Python
Experience with data modeling and writing optimized SQL
Experience with relational and non-relational databases
Strong knowledge of Data Warehousing and Big Data Technologies
Experience with TDD and writing automated unit and functional tests
Proven skills in server side resource profiling, optimization and debugging
Good understanding of Software Design Patterns
Basic proficiency in a UNIX/Linux environment
Excellent communication skills and ability to mentor teammates
Experience working in an Agile development environment
Experience with Git/Github
Even better if you have, but not necessary
Experience in a continuous delivery model
Experience with Golang and/or PHP
Experience in building APIs
Experience with event based and messaging systems
Experience deploying and maintaining software in a production environment
Who we are
Lightspeed (TSX/NYSE: LSPD) powers small and medium-sized businesses with its cloud-based, omni-channel commerce platforms in over 100 countries around the world. With smart, scalable, and dependable point of sale systems, Lightspeed provides all-in-one solutions that help restaurants and retailers sell across channels, manage operations, engage with consumers, accept payments, and grow their business.
Headquartered in Montréal, Canada, Lightspeed is trusted by favourite local businesses, where the community goes to shop and dine. Lightspeed has offices in Canada, USA, Europe, and Australia.
We're passionate about enabling people to do their best work. Come work with us and find out what you can do!"
Azure Data Engineer/Architect,"Mississauga, ON",Compest Solutions Inc.,$42 - $78 an hour,Organic,"Job Description:
· Minimum 2 years of hands on experience in ADLS and Blob Storage (analyzing log files, IoT data, click streams, large datasets)
· Minimum 2 years of experience in working on Azure Event Hub and managing real time data/event ingestion service; along with Azure Stream Analytics performing analytics on data in flight
· Collaborate with business stakeholders to identify and meet the data requirements to implement data solutions that use Azure data services.
· Responsible for data-related implementation tasks that include provisioning data storage services.
· Experience with multiple various file formats like Avro, Parquet, ORC, and JSON etc.
· Implement data solutions that use the following Azure services: Azure Data Factory, Azure Cosmos DB, Azure SQL Database, Azure Synapse Analytics, Azure Data Lake Storage, Blob, Azure Databricks, Azure Stream Analytics.
· Implement security requirements and implement data retention policies.
· Proficient and deep knowledge of Azure Analysis Services, Microsoft SQL Server database programming and optimization
· Proficient with creating multiple complex pipelines and activities using both Azure and On-Prem data stores for full and incremental data loads into a Cloud D
· Proficient with Azure SQL DW. Understanding of when to use Azure SQL DW vs Azure SQL Server/DB and loading patterns to move data from blob or ADLS into Azure SQL DW
Job Type: Full-time
Pay: $42.00-$78.00 per hour
Schedule:
8 hour shift
Experience:
ADLS and Blob Storage: 2 years (Required)
Azure Event Hub: 2 years (Required)
Avro, Parquet, ORC, and JSON: 2 years (Required)
Azure Data Factory, Azure Cosmos DB, Azure SQL Database: 1 year (Required)
Azure Synapse Analytics,: 1 year (Required)
Azure SQL DW: 1 year (Required)
Azure Data Lake Storage, Blob,: 1 year (Required)
Azure Databricks, Azure Stream Analytics.: 1 year (Required)
Work remotely:
Yes, temporarily due to COVID-19"
Senior Data Engineer,"Montréal, QC",Lightspeed POS (DE),None,Organic,"As a part of Lightspeed's Retail product group, you will be contributing to initiatives that will expand our reach into new markets and countries while enhancing our omnichannel commerce platform. Your work will also have a direct impact on supporting new and existing customers on their quest to enter the cloud era. At Lightspeed, we are dedicated to bringing cities and communities to life by powering SMBs. Come and help us build our communities!
What you'll be responsible for
Work alongside some of the most brilliant minds in the industry, you will contribute to omnichannel product by building new features and creative workflows.
Report to the Development Manager and have frequent interactions with Product Managers, Product Designers as well as Platform Teams.
Take part in daily agile ceremonies, brainstorm on innovative ideas that challenge the status quo and implement solutions
Act as an advocate for highest code quality and ownership, while contributing to the health of the infrastructure
Raise the bar and elevate your team by sharing knowledge and best practices, while always seeking improvement and progress
What you'll be bringing to the team
Senior level experience developing reliable, highly available and scalable software (preferably in a scalable SaaS setting)
Strong experience building modern cloud infrastructures
Experience programming in Python
Experience with data modeling and writing optimized SQL
Experience with relational and non-relational databases
Strong knowledge of Data Warehousing and Big Data Technologies
Experience with TDD and writing automated unit and functional tests
Proven skills in server side resource profiling, optimization and debugging
Good understanding of Software Design Patterns
Basic proficiency in a UNIX/Linux environment
Excellent communication skills and ability to mentor teammates
Experience working in an Agile development environment
Experience with Git/Github
Even better if you have, but not necessary
Experience in a continuous delivery model
Experience with Golang and/or PHP
Experience in building APIs
Experience with event based and messaging systems
Experience deploying and maintaining software in a production environment
Über Lightspeed POS Inc.
Lightspeed POS Inc. (TSX: LSPD) ist eine cloudbasierte Handelsplattform, die von kleinen und mittelständischen Unternehmen in über 100 Ländern der Welt genutzt wird. Mit intelligenten, skalierbaren und zuverlässigen Kassensystemen ist es eine Komplettlösung, die Restaurants und Einzelhändler dabei unterstützt, kanalübergreifend zu verkaufen, den Betrieb zu verwalten, mit Kunden in Kontakt zu treten, Zahlungen zu akzeptieren und ihr Geschäft zu erweitern.
Lightspeed mit Hauptsitz in Montreal, Kanada, ist auf über 900 Mitarbeiter angewachsen, mit Niederlassungen in Kanada, USA, Europa und Australien."
Data Engineer (Full Stack),"Montréal, QC",Avanade,None,Organic,"We Are:
The Data & AI Team -- Emerging Technologies. We are the people who love using data to tell a story. A strong and passionate group of data scientists, data engineers, and experts in machine learning and AI.

A great day for us? Solving big problems using the latest tech, serious brain power. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services.

You Are:
A software engineer and technologist at heart, with a passion for big data! You are a hand-on-keyboard developer, and you know how to find, store, and present a range of information from different sources so that everyone can access what they need quickly and simply, and use it effectively.

You are someone who thrives in a team setting where you can use your creative and analytical prowess to obliterate problems. You’re passionate about digital technology, and you take pride in making a tangible difference. You have communication and people skills in spades.

You thrive in front of complex issues thanks to your razor-sharp critical thinking skills.

The Work:
Our software engineers, with a specialization in Data, define strategies and develop solutions that enable the collection, processing and management of information from one or more sources, and the subsequent delivery of information to audiences in support of key business processes.

The Job:
As a Data Engineer, you know the importance of data to business. You design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.

Day-to-day You:
*

Use your knowledge to plan and deliver data warehouses and storage
*

Take part in crafting and running bespoke data services for individual projects
*

Stay up to date with business best practice in using and retrieving data
*

Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
*

Customize storage and extraction, metadata, and information repositories
*

Build and use effective metrics and monitoring processes
*

Help to develop business intelligence tools
*

Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources

Specifically, Your Skills And Experience Include:
*

Strong knowledge of Python, Spark, and T-SQL
*

Strong OOP fundamentals
*

Database, storage, collection and aggregation models, techniques, and technologies - and how to apply them in business
*

Experience in structured problem solving
*

Knowledge of Azure tools such as Azure Data Factory, Azure Data Lake, Azure SQL DW or Azure SQL
*

Knowledge of Big Data tools such as Hadoop / Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics
*

Experience preparing data for Data Science and Machine Learning
*

Crafting and building Data Pipelines using streams of IoT data
*

Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals

Here’s What You Need:
*

Bachelor's degree in Computer Science, Engineering, Technical Science
*

Minimum 2 years of designing, building and operationalizing large-scale enterprise solutions as a software developer in either a full-stack, or back-end focused role
*

Exposure to one or more of Azure / AWS / GCP data and analytics services in combination with custom solutions - Spark, Azure Data Lake, HDInsights, SQL DW, DocumentDB, Search, Elastic Pool etc.
*

Exposure to designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Java, Python, Scala etc.

HUGE Bonus Points if:
*

Experience designing and implementing data ingestion solutions on Azure
*

Experience in designing and optimizing data models on Azure
*

Experience integrating AZURE security services with Azure / AWS / GCP data services for building secure data solutions.
*

Experience introducing and operationalizing self-service data preparation tools (e.g. Trifacta, Paxata) on AZURE.
*

Designing operations architecture and conducting performance engineering for large scale data lakes a production environment.
*

Craft and lead client design workshops and provide tradeoffs and recommendations towards building a solutions."
Big Data Engineer (Canada Remote),"Toronto, ON",Congero Technology Group,"$71,001 - $100,000 a year",Organic,"Mandatory:
Minimum 2 years experience in Spark with Scala or Python or Core Java, Hive, Hbase, and other related Hadoop technologies. Should have sound knowledge on creating Spark jobs for data transformation and aggregation and produce unit tests for Spark transformations and helper methods
Minimum 2 years prior experience in sourcing, transforming and analyzing vast amounts of raw data from various systems using Spark to provide ready-to-use data to the business team.
Sound knowledge about the Hadoop ecosystem. Hands-on experience in Map-Reduce, Hive, Hue, Yarn, Sqoop and Oozie
Hands on experience and a deep understanding of GCP Services and Cloud native tools
Strong understanding of design patterns used in Big Data
Hands-on experience in GCP Big Data services like DataFlow, DataProc, BigQuery and other orchestration technologies like AirFlow
Hands on expertise in Scala/Java or Python – specifically collections, Multithreading, Design Patterns, OOPS concepts.
Familiar with DevOps tools such as Jenkins, Git/Bitbucket, Jira, etc.
Experience with performance identification and enhancement techniques on Spark and other related BigData technologies
Working knowledge of next generation data platforms including Streaming technologies, Kafka, Spark, NoSQL, GraphdB, Data Virtualization, in-memory datastores and self-service data platforms
Experience delivering solutions using an Agile/Scrum methodology
Excellent communication skills and experience working remotely and within a cross-region team
Nice to have:
AWS/GCP/Azure certification in development, architecture or data analytics areas
Big data or Data Engineering certification
Hands-on experience working on Talend as Big Data and/or data integration tool
Expected start date: 2020-11-23
Job Types: Full-time, Permanent
Pay: $71,001.00-$100,000.00 per year
Additional pay:
Bonus pay
Benefits:
Dental care
Extended health care
Paid time off
Vision care
Schedule:
Monday to Friday
Experience:
Big Data: 2 years (Required)
GCP Big Data: 2 years (Preferred)
Work remotely:
Yes, temporarily due to COVID-19"
Data Engineer (Remote),"Guelph, ON",Innosphere Recruiting,"$100,000 - $140,000 a year",Organic,"Data Engineer (Remote)
The Data Engineer uses big data and cloud technologies to produce production quality code. They are responsible for determining the most efficient way to store, access, and process data. They engage in performance tuning and scalability engineering.
Responsibilities:
Analyzing system requirements and design responsive algorithms and solutions
Using big data and cloud technologies to produce production quality code
Engaging in performance tuning and scalability engineering
Working with team, peers, and management to identify objectives and set priorities
Performing related SDLC engineering activities like sprint planning and estimation
Working effectively in small agile teams
Providing creative solutions to problems
Identifying opportunities for improvement and execution
Qualifications:
University Degree in Computer Science or related program
A minimum of 5 years work experience in software development
Experience with one or more programming languages like Scala, C#, or Java
Exposure to Amazon Web Services
Exposure to Spark, Kafka, Athena, Glue, Aurora, QuickSight
About Innosphere
Founded in 1997, Innosphere provides technology staffing solutions to clients across North America. We have specialized in remote work for over 15 years. Our staffing solutions include fully managed development teams coupled with expert consulting, full-time hiring, and flexible staffing.

Innosphere is a permanent remote company but we do have offices in Guelph and Kitchener-Waterloo to provide employees opportunities to work intermittently in an office environment. We have collaborative space for meetings, lunches, and company events. Our employees have the freedom to choose where they prefer to work.
Innosphere is committed to providing its employees with endless possibilities to learn new things and to work with the latest technologies.
When we hire, we look for people who embody our core values:
Honesty - We believe in acting with authenticity and integrity—being candid and transparent in all that we do.

Understanding – We value taking time and investing resources in order to deeply understand those that we interact with.

Adaptability - If something is no longer working, we are not too proud to change. Our strength lies in our ability and willingness to grow, adjust, and try something new.

Partnership - We believe in active collaboration, working together toward the same goals, and celebrating success together.

Life as an Innospherian
Who we are and how we work:
Work from home or office
Flexible work schedule
Open door policy
Team owned deliverables
Virtual water cooler talk
Open office floor plan
Team based strategic planning
Daily stand up
Group brainstorming sessions
Charity Initiatives
Promote from within
Casual dress
Some other perks:
Breakfast, pop, snacks, beer, and coffee always on tap in our kitchen
Frequent catered lunches
Generous time off
Free gym membership
Above average benefits (Dental, Health, Vision, STD, LTD, AD&D, Life, Dependant Life)
Yearly professional development budget
Regular company outings and family events (VR night, Raptors games, axe throwing and more)
Employee lotteries (for Raptors tickets, cool tech etc.)
Parking
Lunch and Learns
Innosphere welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process. Only applicants meeting the criteria outlined above will be contacted as part of the shortlisting process. Must be currently residing in Canada and legally permitted to work in Canada.

Please reply directly to this post with a cover letter and resume. No phone calls please."
Stagiaire en génie des données généraliste / Full Stack Data...,"Kirkland, QC",Merck,None,Organic,"Job Description
Notre équipe du numérique innove dans la manière dont nous comprenons nos patients et leurs besoins. En travaillant de manière transversale, nous inventons de nouvelles façons de communiquer, de mesurer et d’interagir avec nos clients et nos patients au moyen de canaux et de technologies numériques.
Stagiaire en génie des données généraliste (Janvier- avril)
Notre société est un chef de file mondial du domaine de la santé avec une gamme diversifiée de médicaments d’ordonnance, de vaccins et de produits de santé animale. La différence entre le potentiel et l’accomplissement réside dans l’étincelle qui stimule l’innovation et l’inventivité; il s’agit de l’espace où notre société a codifié son héritage de 125 ans. Le succès de notre société est soutenu par l’intégrité éthique, l’évolution et une mission inspirante pour franchir de nouvelles étapes dans le domaine des soins de santé mondiaux.
Dans ce contexte, le stagiaire en génie des données généraliste contribuera à la création de solutions de données évolutives et de qualité professionnelle pour transformer le cheminement de nos patients. Ce poste inclut des responsabilités comme le développement et l’industrialisation des pipelines de données, le développement et le déploiement de modèles de science des données, et le développement et le test de procédures automatisées de qualité des données.
Qualifications :
Formation universitaire en cours en génie logiciel, en sciences informatiques, en mathématiques appliquées ou dans un domaine technique connexe.
Intérêt pour les mégadonnées et la technologie infonuagique (p. ex., EC2, S3, EMR, Glue, Lake Formation, SageMaker ou Redshift).
Expérience en techniques d’apprentissage automatique (supervisé et non supervisé, en agrégation, système de recommandation et techniques de reconnaissance d’images par intelligence artificielle).
Expérience théorique ou pratique avec les données. P. ex. : bonne compréhension des bases de données, de l’intégration de données et de la préparation préalable de données (API, moissonnage du Web, ETL), gouvernance, techniques d’apprentissage automatique et visualisation.
Connaissance avancée de Python, SQL, Java ou d’un langage de programmation connexe.
Expérience solide en mathématiques.
Esprit orienté vers la résolution de problèmes
Solides aptitudes en communication de problèmes techniques complexes à tout public cible.
Notre division Santé humaine défend l’idéologie « le patient d’abord, les bénéfices ensuite ». L’organisation est composée de professionnels des ventes, du marketing, de l’accès au marché, de l’analyse numérique et du commerce qui sont passionnés par leur rôle dans la commercialisation de nos médicaments à nos clients du monde entier.
Nous sommes fiers d’être une société qui connaît la valeur d’un effectif diversifié, talentueux et dévoué. La façon la plus rapide d’innover est de rassembler des personnes de diverses opinions dans un environnement inclusif. Nous encourageons nos collègues à remettre en question avec respect les idées de chacun et à aborder les problèmes de manière concertée. Nous sommes un employeur souscrivant au principe de l’égalité d’accès à l’emploi et nous sommes déterminés à favoriser un milieu de travail inclusif et diversifié.
Qui sommes-nous?
Nous sommes connus sous le nom de Merck & Co., Inc., Kenilworth, New Jersey, USA aux États-Unis et au Canada et MSD partout ailleurs. Depuis plus d'un siècle, nous inventons pour la vie, en proposant des médicaments et des vaccins pour de nombreuses maladies parmi les plus difficiles au monde. Aujourd'hui, notre entreprise reste à la pointe de la recherche pour proposer des solutions de santé innovantes et faire progresser la prévention et le traitement des maladies qui menacent les personnes et les animaux dans le monde.
Que cherchons-nous?
Dans un monde d’innovation rapide, nous cherchons des inventeurs courageux qui veulent avoir un impact dans tous les aspects de notre entreprise, permettant ainsi des percées qui auront une incidence sur les générations à venir. Nous vous encourageons à accroître la valeur de notre organisation en y apportant votre raisonnement perturbateur, votre esprit de collaboration et votre perspective diversifiée. Ensemble, nous continuerons à inventer pour la vie, à avoir une incidence sur la vie et à inspirer votre carrière.
INVENT.
IMPACT.
INSPIRE.
Secondary Language(s) Job Description:
Our Digital team is innovating how we understand our patients and their needs. Working cross functionally we are inventing new ways of communicating, measuring and interacting with our customers and patients through digital channels and technologies.
Full Stack Data Engineer Intern (January to April)
Our Company is a global health care leader with a diversified portfolio of prescription medicines, vaccines and animal health products. The difference between potential and achievement lies in the spark that fuels innovation and inventiveness; this is the space where Our Company has codified its 125-year legacy. Our Company's success is backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.
In this context, the Full Stack Data Engineer Intern will help create scalable and production-grade data solutions to transform our patient’s journey. This would include responsibilities such as: develop / industrialize data pipelines, develop / deploy data science models, and develop / test automated data quality procedures.
Qualifications:
Ongoing university training in software engineering, computer science, applied mathematics, or a related technical field.
Interest in Big Data and cloud technology (EC2, S3, EMR, Glue, Lake Formation, SageMaker, Redshift).
Experience with Machine Learning techniques (supervised and unsupervised: clustering, recommendation system, AI Images recognition techniques).
Academic or hands on experience with data. Ex: good understanding of databases, data ingestion & wrangling (APIs, webscraping, ETL), governance, machine learning techniques, visualization.
Advanced knowledge of Python, SQL, Java or related programming.
Strong mathematical background.
Problem-solving mindset.
Strong ability to communicate complex technical problems to all audiences.
Our Human Health Division maintains a “patient first, profits later” ideology. The organization is comprised of sales, marketing, market access, digital analytics and commercial professionals who are passionate about their role in bringing our medicines to our customers worldwide.
We are proud to be a company that embraces the value of bringing diverse, talented, and committed people together. The fastest way to breakthrough innovation is when diverse ideas come together in an inclusive environment. We encourage our colleagues to respectfully challenge one another’s thinking and approach problems collectively. We are an equal opportunity employer, committed to fostering an inclusive and diverse workplace.
Who we are …
We are known as Merck & Co., Inc., Kenilworth, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.
What we look for …
In a world of rapid innovation, we seek brave Inventors who want to make an Impact in all aspects of our business, enabling breakthroughs that will affect generations to come. We encourage you to bring your disruptive thinking, collaborative spirit and diverse perspective to our organization. Together we will continue Inventing For Life, Impacting Lives while Inspiring Your Career Growth.
INVENT.
IMPACT.
INSPIRE.
Search Firm Representatives Please Read Carefully
Merck & Co., Inc., Kenilworth, NJ, USA, also known as Merck Sharp & Dohme Corp., Kenilworth, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.
Employee Status:
Intern/Co-op (Fixed Term)
Relocation:
No relocation
VISA Sponsorship:
No
Travel Requirements:
Flexible Work Arrangements:
Work Week
Shift:
Valid Driving License:
Hazardous Material(s):
Number of Openings:
1
Requisition ID:R83184"
Data Engineer,"Mississauga, ON",Bond Brand Loyalty Inc,None,Organic,"At Bond, we design creative and innovative solutions for our clients, all with the goal of helping them build ever-stronger loyalty to their brands. That can take us in some pretty amazing directions, and as a Data Engineer, you’ll have your hands on the wheel as we drive the future of loyalty.

Working on the bleeding edge of exciting technology, you're afforded the opportunity to experiment with new tools and attempt radically different approaches than traditional software engineering affords. Every day with the Data Engineering team is different and each project presents its own set of new and exciting challenges. Things shift very quickly in our industry and we rely on the Data Engineering team to keep us ahead of the curve and moving in the right direction.
Here's what we want:
Problem Solver: You are curious and loves exploring multiple approaches to find the most efficient, scalable solution and solve a problem
Collaborative: You work well with other people
Passionate: A passion for Big Data and an interest in the latest trends and developments constantly researching new tools and data technologies
Self-starter: You are comfortable helping your team get things done
Here's what you'll be doing:
Design, implement, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services
Identify, design, and implement system performance improvements
Identify, design, and implement internal process improvements
Automate manual processes and optimize data delivery
Useful skills/background: You may or may not tick off every box, and that's ok. Each person brings a different background and different skills. If you think you are a good match for what we are looking for tell us why, and tell us what you are doing to improve yourself and we'll see what we can do to help!
A degree in Computer Science/Engineering or related field
2-4 years of experience in a software engineering environment
Experience with SQL and NoSQL systems
Knowledge of Hadoop, Spark, Kafka or other equivalent technologies
Proficiency in some of the following languages: Scala, Java, Python, Bash
Experience with automated testing systems
Mentorship, collaboration, and communication skills
Knowledge of data modelling, data warehousing, ETL processes, and business intelligence reporting tools
Experience working with CI/CD, containerization, and virtualization tools such as Gitlab, Jenkins, Kubernetes, Docker
Experience with tools like Databricks, Snowflake or PowerBI

Why Join Us?
Why Join Us?
You can see the code getting to production faster than you used to; you will try your Big Data skills, where precise and robust code really matters; you will work with the 3.5 Billion-record tables; you will learn how difference between European and Australian privacy laws can affect your design decisions.
Bond Brand Loyalty is proud to be recognized as one of Canada’s Best Managed Companies.
We’re 400(ish) people working tirelessly together to make the world a more loyal place. You’ll be joining a hyper-talented team with a galaxy of skillsets ranging from research to creative to digital and beyond. You’ll have an excellent opportunity to grow, learn and make an impact as we tackle some of our client’s biggest business challenges.
If you’re looking to build your career, build your skills and build bonds apply today!
Bond Brand Loyalty welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process."
Big Data Engineer--Toronto,"Toronto, ON",Rocket Homes,None,Organic,"Preferred Qualifications
Bachelor’s degree in computer science or equivalent experience
2 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi, Hive and/or Sqoop
2 years of experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or Glue
2 years of experience with stream-processing systems: Spark-Streaming, Kafka Streams and/or Flink
3 years of experience with object-oriented/object function scripting languages: Java (preferred), Python and/or Scala
2 years of experience with relational SQL and NoSQL databases like MySQL, Postgres, Cassandra and Elasticsearch
2 years of experience working in a Linux environment
Expertise in designing/developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworks
Demonstrated ability to performance-tune MapReduce jobs
Strong analytical and research skills
Demonstrated ability to work independently as well as with a team
Ability to troubleshoot problems and quickly resolve issues
Strong communication skills
What'll Make You Special
Experience with managing real estate data
Experience leading a team of engineers on a large enterprise data platform build
Job Summary
The Big Data Engineer is responsible for the full life cycle of the back-end development of a data platform. This team member creates new data pipelines, database architectures and ETL processes, and they observe and suggest what the go-to methodology should be. They gather requirements, perform vendor and product evaluations, deliver solutions, conduct trainings and maintain documentation. They also handle the design and development, tuning, deployment and maintenance of information, advanced data analytics and physical data persistence technologies.
This team member establishes analytic environments required for structured, semi-structured and unstructured data. They implement the business requirements and business processes, build ETL configurations, create pipelines for the data lake and data warehouse, research new technologies and build proofs of concept around them. This person carries out monitoring, tuning and database performance analysis and performs the design and extension of data marts, meta data and data models. They also ensure all data platform architecture code is maintained in a version control system.
The Big Data Engineer is responsible for sharing knowledge with fellow team members, allowing the entire team to grow and become proficient to further build out and enhance the data platform.

Responsibilities
Focus on scalability, performance, service robustness and cost trade-offs
Design and implement high-volume data ingestion and streaming pipelines using Apache Kafka and Apache Spark
Create prototypes and proofs of concept for iterative development
Learn new technologies and apply the knowledge in production systems
Develop ETL processes to populate a data lake with large data sets from a variety of sources
Create MapReduce programs in Java and leverage tools like AWS Athena, AWS Glue and Hive to transform and query large data sets
Monitor and troubleshoot performance issues on the enterprise data pipelines and the data lake
Follow the design principles and best practices defined by the team for data platform techniques and architecture
Who We Are
Rocket Homes Real Estate LLC is a Detroit-based, tech-driven company with a passion for simplifying real estate. Our mission is to create a seamless home buying and selling experience by combining the process of searching for homes, connecting with a trusted real estate agent and getting a mortgage. Since 2006, we’ve partnered with our sister company, Rocket Mortgage® by Quicken Loans, and our nationwide network of top-rated real estate agents to help over 500,000 clients with their real estate needs.
Disclaimer
This is an outline of the primary responsibilities of this position. As with everything in life, things change. The tasks and responsibilities can be changed, added to, removed, amended, deleted and modified at any time by the leadership group.
The Company has policies to support applicants with disabilities, including, but not limited to, policies regarding the provision of accommodations that take into account an applicant‘s accessibility needs due to disability. For more information, please call us at (800) 411-JOBS or email us at Job@MyRocketCareer.com."
"Senior Data Engineer, Corporate Systems","Toronto, ON",CPP Investments,None,Organic,"Company Description
Make an impact at a global and dynamic investment organization
When you invest your career in CPP Investments, you join one of the most respected and fastest growing institutional investors in the world. With current assets under management valued in excess of $400 billion, CPP Investments is a professional investment management organization that globally invests the funds of the Canada Pension Plan (CPP) to help ensure long-term sustainability. The CPP Fund is projected to exceed $450 billion by 2025. CPP Investments invests in all major asset classes, including public equity, private equity, real estate, infrastructure and fixed-income instruments, and is headquartered in Toronto with offices in Hong Kong, London, Luxembourg, Mumbai, New York City, San Francisco, São Paulo and Sydney.
CPP Investments attracts and selects high-calibre individuals from top-tier institutions around the globe. Join our team and look forward to:
Diverse and inspiring colleagues and approachable leaders
Stimulating work in a fast-paced, intellectually challenging environment
Accelerated exposure and responsibility
Global career development opportunities
Being motivated every day by CPP Investments’ important social purpose and unshakable principles
A deeply rooted culture of Integrity, Partnership and High Performance
If you share a passion for performance, value a collegial and collaborative culture, and approach everything with the highest integrity, here’s an opportunity for you to invest your career at CPP Investments.

Job Description
The Data Engineering team is looking for people who are passionate about working in agile delivery environments and resolving the engineering challenges of building robust and scalable data systems aligned to enterprise data strategy.
As Senior Data Engineer You will be responsible for developing, constructing and testing large-scale data processing systems based on AWS cloud that will help address the disparate data consumption/integration/operation challenges of a growing organization.
Through close partnership with investment professionals, you will see firsthand how your contribution is delivering long-term value to the CPP Fund for the benefit of 20 million CPP contributors and beneficiaries. You are encouraged to bring an entrepreneurial, innovative mindset to tackle complex business requirements in the investment industry.
The opportunity:
Design solutions aligned with long-term architecture and technology strategy using Amazon Web Services (AWS) for Cloud development.
Participate in the development life cycle from start to completion - requirements analysis, development, testing, and deployment.
Work in a fast-paced environment collaborating with developers, data engineers and architects.
Develop dataset processes for data modelling, mining and production.
Ensure architecture will support the requirements of CPP Investments business.
Prepare, transform, combine and manage structured and unstructured data for use by CPP Investments business users.
Recommend ways to improve data reliability, efficiency and quality.
Define and shape CPP Investments’ future technology and research process.

Qualifications
University degree in Engineering or Computer Science preferred.
Deep experience working with big data including cleaning/transforming/cataloging/mapping/ etc.
Familiar with cloud technology best practices to enable the distribution and analysis of big data on the cloud (formatting/partitioning/etc.).
Experience of ETL pipelines, managing multiple datasets and providing necessary support.
Familiarity building applications in an AWS environment
Familiarity working with data lakes using S3/Redshift.
Exposure to big data workflows and analytics tools (Spark/EMR/Databricks/Cassandra).
Deep proficiency in Python with experience using Spark, Pandas or PySpark.
Familiar with one or more analytic tools such as Tableau or Qlik.
An understanding of CI/CD pipelines and experience with DevOps.
Experience building flexible solutions that can adapt quickly to changing requirements.
Ability to work in an entrepreneurial environment and be a self-starter.
Interests in the financial industry.
Exemplify CPP Investments' Guiding Principles of Integrity, Partnership and High Performance.

Additional Information
Visit our Linkedin Career Page or Follow us on Linkedin. #LI-POST
At CPP Investments, we are committed to diversity and equitable access to employment opportunities based on ability.
We thank all applicants for their interest but will only contact candidates selected to advance in the hiring process.
Our Commitment to Inclusion and Diversity:
In addition to being dedicated to building a workforce that reflects diverse talent, we are committed to fostering an inclusive and accessible experience. If you require an accommodation for any part of the recruitment process (including alternate formats of materials, accessible meeting rooms, etc.), please let us know and we will work with you to meet your needs.
Disclaimer:
CPP Investments does not accept resumes from employment placement agencies, head-hunters or recruitment suppliers that are not in a formal contractual arrangement with us. Our recruitment supplier arrangements are restricted to specific hiring needs and do not include this or other web-site job postings. Any resume or other information received from a supplier not approved by CPP Investments to provide resumes to this posting or web-site will be considered unsolicited and will not be considered. CPP Investments will not pay any referral, placement or other fee for the supply of such unsolicited resumes or information."
Senior Big Data Engineer--Toronto,"Toronto, ON",Rocket Homes,None,Organic,"Minimum Qualifications
Bachelor’s degree in computer science, information technology, or a related field or equivalent experience
Preferred Qualifications
4 years of experience with big data/Hadoop distribution and ecosystem tools, such as Hive, HBase, Spark, Kafka, NiFi and Oozie
4 years of experience developing batch and streaming ETL processes
4 years of experience working with relational and NoSQL databases, including modeling and writing complex queries
Master’s degree in computer science, information technology, or a related field or equivalent experience
Experience with programming languages, such as Python, Java or C#
Experience with Linux system administration, Linux scripting and basic network skills
Experience coding against and developing REST APIs
Job Summary
The Senior Big Data Engineer is responsible for engaging in the design, development and maintenance of the big data platform and solutions. This includes analytical solutions that provide visibility and decision support using big data technologies.
The role involves administering a Hadoop cluster, developing data integration solutions, and working with data scientists, system administrators and data architects to ensure the platform meets business demands.
Responsibilities
Develop ETL processes from various data repositories and APIs across the enterprise, ensuring data quality and process efficiency
Develop data processing scripts using Spark
Develop relational and NoSQL data models to help conform data to meet users’ needs using Hive and HBase
Integrate platform into the existing enterprise data warehouse and various operational systems
Develop administration processes to monitor cluster performance, resource usage, backup and mirroring to ensure a highly available platform
Address performance and scalability issues in a large-scale data lake environment
Who We Are
Rocket Homes Real Estate LLC is a Detroit-based, tech-driven company with a passion for simplifying real estate. Our mission is to create a seamless home buying and selling experience by combining the process of searching for homes, connecting with a trusted real estate agent and getting a mortgage. Since 2006, we’ve partnered with our sister company, Rocket Mortgage® by Quicken Loans, and our nationwide network of top-rated real estate agents to help over 500,000 clients with their real estate needs.
Disclaimer
This is an outline of the primary responsibilities of this position. As with everything in life, things change. The tasks and responsibilities can be changed, added to, removed, amended, deleted and modified at any time by the leadership group."
Cloud Data Engineer,"Burnaby, BC",None,None,Organic,"Our Technology team has been on an incredible journey the last few years by creating first-to-market initiatives for our customers, adding the latest development languages to our tech stack and establishing Best Buy Canada as the best Omni-channel experience within the retail industry.
We believe empowered people and teams make smarter, faster, and more creative decisions, which is why we operate in a truly Agile environment where the distance between any one person and senior leadership is microscopic. Here, you’ll work on something big, small, or super cool and before you can blink 100,000 people will see it. You’ll create fast, learn fast, and develop fast! Oh, and sometimes you’ll fail fast too. That’s ok. (Honestly.) It’s all part of the process.
The Cloud Data Engineer reports to the Manager Data Platform Products and also works closely with the Manager Data Analytics and Manager (Web Analytics – Testing Platforms) as they provide guidance and vision towards developing, constructing and maintaining our enterprise data capabilities. You will work with large scale data processing system that collects data from a variety of structured and unstructured data sources, stores data in a scale-out data lake and prepare the data using ELT techniques in preparation for the data science data exploration and analytic modeling.
As a Cloud Data Engineer you will…
Harness, model and transform data (structured, unstructured) from several sources that empowers users to collaborate and analyze data in different ways leading to better and faster decision making.
Build, design and launch data pipelines, raw landing zones and data lakes that store, transform and move data.
Monitor data quality processes and compliance processes in accordance with industry and data governance COP best practices.
Collect, combine and integrate data from our omni and multi-channel footprints using our enterprise tech-stack that intuitively conveys insights to the business regarding data trends and consumer behavior.
Design, Build and write code for cloud-compatible CI/CD frameworks to deploy solutions on Cloud Data Platforms.
We hope you are passionate about…
Big Data and Data Labs – you creatively navigate through a complex network of data structures, pull relevant data, apply ML/AI and extract valuable insights.
Team Work – you work collaboratively with your team to connect the dots and provide valuable information to the business.
Relationship Building - you are a communicator, able to influence, gain buy-in and provide guidance to key stakeholders.
The experience we need…
5+ Years building, designing and implementing data pipelines.
5+ years of experience with Azure Data stack development (Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Factory, Databricks and Python)
2+ Years experience with Qlik Replicate, Azure Stream Analytics, Azure Data Factory and Informatica Power Center
Experience with MSSQL or Oracle database
Good understanding of a tech stack such as Microsoft (Azure Data Lake Storage, Azure Data Factory, Azure Streams, Databricks) or Google (BigQuery, Dataflow, Datafusion, Dataproc)
Bonus points…
Familiarity with agile/scrum methodologies
Experience designing and developing data applications for a retail environment
Bachelor’s Degree or Diploma in Computer, Science, Electrical, Engineering or related discipline/experience"
Data Engineer - BNSJP00016223,"Toronto, ON",Ian Martin,None,Organic,"Hiring Manager: Senior Manager, Technology Operations & Engineering
Location Address: WFH (40 King Street W, 14th Floor)
Contract Duration: 1 year
Number of Positions: 1
Hours: 37.5 hours / week
Story Behind the Need
Business group: The GTS - Technology Operations & Engineering is managing a large scale enhancements to the systems data lakes.
Need: The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver.
Reason for request: Workload
Candidate Value Proposition
The successful candidate will have the opportunity to work on multiple initiatives on non-relational databases , gaining exposure to multiple bank streams and utilizing new technology.
The individual is comfortable working with business and technical staff ensuring systems are designed and maintained according to enterprise architectural standards. Collaborating with team members, they will utilize agile best practices and metrics to build high quality technology solutions in line with the product's vision.
Responsibilities:
Work closely with Product Owner and other SMEs to interpret requirements and translate them to software solutions.
Ensure delivery of negotiated product/function while following standards and methodologies. This includes the design, development, and unit testing of solutions to optimize, create efficiencies, address root cause incidents or
Create and maintain detailed design documents as well as supporting the lifecycle of those documents.
Build and test integration software solutions.
Help maintain code quality, organization, and performance.
Participate in technical meetings with client's technical specialists.
Provide support for testing efforts and defect resolution.
Provide deployment and post deployment support (Ex. warranty support, command center services, process, review release content and coordinate with clients.
Operations: Maintain and troubleshoot production & non-production environments including certificate management, patch management. Create and maintain alerts and dashboards, access control management and perform system admin activities. Create deployment and operations guides. Perform NFT testing on the ecosystem to ensure resilience & peak performance.
L2 Support Services: Restore services within agreed upon SLA. Respond to Major Incidents impacting technical components, seek opportunities for improvement.
focus on logical support work, like configuration, data management, application performance and tuning, application troubleshooting
Must Have Skills:
4+ years hands on experience with Big Data software designs, development and operations, specifically Hadoop and Hive
3-4 years back end development with Python, Java OR Scala
3 + years Data ingestion tools such as: Diyotta, Data Stage, MapReduce
Strong understanding of HDFS and Cloud Services, proven through recent project experience (1-2 Recent projects)
Strong communication skills to clearly articulate code tuning and performance issues to development team
Nice to have Skills:
Experience with continuous integration
Financial Industry experience
Education:
Completion of a post-secondary education in Computer Science, Engineering, or a technology related field.
Candidate Review & Selection
2 Step Process: Microsoft Teams Interview (1 Technical + 1 Non-technical with senior leadership)
Hiring Managers availability to interview: ASAP
Additional Notes:
The incumbent should be a quick learner and must be adept at troubleshoot and problem solving in a highly integrated big data environment. Experience with continuous integration and deployment methodology is an asset.
It is essential that the incumbent to be proactive, eager to learn, have a 'Can Do' attitude, and demonstrate initiative and eagerness to succeed."
Data Analyst / Engineer,Ontario,J.D. Power,None,Organic,"Title: Data Analyst Engineer
Location: Canada (Remote)

The Role:
As a Jr. Data Analyst, your job is to work with the clients to analyze website, lead, sales and digital media data in order to identify products and strategies that will make them more successful. You will need a deep understanding of analytical techniques and tools and it is helpful to have a strong understanding of automotive retailing and digital marketing.

Responsibilities:
Develop a deep understanding of operational, customer and financial data
Coordinate with leadership team to define key business metrics.
Support ad-hoc analytical reporting to drive business insight.
Collaborate with client's operational, sales and dealer teams to plan data and analytic needs.
Develop, support, and refine dashboards and reports evaluating the effectiveness of digital marketing campaigns

Qualifications:
Bachelor's degree in business, marketing, statistics, business analytics, economics or other appropriate field, or an equivalent combination of education and experience
1-3 years of work experience in business analytics, reporting and data visualization
Ability to communicate complex quantitative analyses and business needs in a clear, precise, and actionable manner to diverse audiences
High level of comfort with large relational data sets, database mining and manipulation.
Clear written and oral communication skills
Strong Excel and SQL skills
Familiarity with set-up and management of online reporting tools
Strong analytical, conceptual, and problem-solving abilities
Experience using platform analytics tools such as Google Analytics, Adobe, and social media monitoring tools and managing paid campaigns using Google AdWords or Adobe
Knowledge of Business/Automotive a plus but not required
Experience with media and web analytics
Experience with data modeling and KPI definition
Experience working with Marketing, Product and Sales teams

The merger of Autodata Solutions with J.D. Power – announced in December of 2019 – created a valuable combined resource of data and analytics for the automotive industry. The integrated organization operates under the J.D. Power name and includes the brands Autodata Solutions, ChromeData and UnityWorks. As our collective company expands, we continue to build upon our industry-leading source of automotive data, analytics and software solutions.
We're proud of what we do. At Autodata Solutions, we transform complex data into marketing and sales solutions for the automotive industry. Our innovative technologies enable industry partners to optimally support and promote sales. For over 30 years Autodata Solutions has been a trusted and proven partner to the automotive industry. We house over 700 employees throughout five locations in Canada and the U.S.
When you work for Autodata Solutions, you join a team of dedicated professionals who look for new ways to raise the bar. Our corporate culture is results focused, and our offices are informal with the goal of inspiring collaboration and outside-the-box thinking. It takes talent and ambition to meet the challenges of our rapidly evolving market — and that's all part of the fun.

To all recruitment agencies: Autodata Solutions does not accept unsolicited agency resumes and we are not responsible for any fees related to unsolicited resumes."
Data Engineer - Business Intelligence,"Toronto, ON",PayPal,None,Organic,"Who we are: Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 286 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
Job Description Summary: The successful candidate will be responsible for working with the business on their requests and building BI solutions using a mix of Microsoft stack (SSIS, SSRS) and a Hadoop datalake and Tableau. They will join the BI and Reporting team while migrating our core application to expand our current capabilities and keep consistency for our customers. The Migration creates a new source of record and the BI developer will support both the internal data integrations as well as the datamart supporting client facing reports in that transition.
Job Description:
Key Responsibilities and Accountabilities
Work with ETL development with SSIS to maintain and expand datamart and integrations
Works with departments and customers to define business reporting requirements.
Evaluate systems needs and come up with the recommendations for application integrations with reporting system and improvements within reporting processes
Performs data analysis and report development activities, such as the creation of SQL queries and stored procedures to extract data for reports.
Maintains expertise in the Hyperwallet platform data model. Analyzes source system data to understand the data structure, definitions, and anomalies.
Documents report designs and processes in a clear and concise manner.
Maintain the data governance process as metrics are developed
Performs other duties as required.
Assists in the training, support, maintenance, and ongoing administration of reporting system
Qualifications
Education: Degree in Computer Science or Information Technology.
Expertise in MS SQL
Extensive experience working with SSRS and SSIS
Experience with MySQL is preferred
Experience with data visualization software (Power BI, Tableau, or Qlik)
Experience with Hadoop an asset
Experience with OpenReports an asset
Excellent data modelling skills
Strong knowledge of Microsoft Excel.
Experience with business reporting systems.
Financial or payment system reporting experience is an asset.
Must have good interpersonal and communications skills including well developed verbal and written English
Good analytical skills for complex problem solving
Able to work independently as well as in a team environment
Detail oriented with good organizational skills
Can communicate in both technical and business terms
We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities."
Data Engineer III,"Toronto, ON",HOMER,None,Organic,"HOMER has a newly created opportunity for a Data Engineer to join our growing team! This role will be based in Toronto, Ontario.
Reporting into our Director of Technology, the Data Engineer will be responsible for improving and maintaining our data infrastructure and owning parts of it. You will focus on accuracy, reliability and timeliness of data that the whole company uses to generate insights, and deploying data science models that tackle sophisticated business problems. As you grow, you'll take on new responsibilities, from designing and defining our processes around data management to scaling our data pipeline.
HOMER is the essential early learning program for kids 2-8. Powered by the HOMER Method, the most effective way for your child to learn— our expert-designed proprietary learning framework is research-backed and kid-tested to give your child the best start to their learning journey. Focused on both academic and personal skills, HOMER develops confident learners prepared for both school and life.
We are a high-growth business with an amazing set of investors. We offer competitive compensation including equity and full benefits in a creative, flexible environment that invests in professional development.
You will:
Collaborate effectively with our data scientists, analysts, engineers and other stakeholders to design and maintain data models
Design scalable infrastructure to build, train, and deploy machine learning models, with an eye on efficiency
Own efficacy and quality for data pipelines and ETL processes you build and maintain
Develop, maintain and improve tools to enable team members to rapidly consume and understand data
Implement and integrate solutions using Python, SQL, and other languages as necessary
Improve and expand our product recommendation systems to be more powerful, helping children using our products to have greater success
Help build and add new features to our in-house experimental tools to support A/B testing
Must Haves:
Demonstrable expertise in Python, SQL, Node, relational databases (e.g. PostgreSQL), non-relational databases (e.g. Couchbase or MongoDB), and column store (e.g. Vertica/Redshift)
Experience building and maintaining data infrastructure (including batch and streaming ETL processes)
Sound Computer Science fundamentals (Bachelor's degree in CS or related fields preferred)
Strong interpersonal and communication skills
At least 3 years of relevant work experience
Nice-to-Haves:
Working experience with AWS cloud
Knowledge of machine learning, statistics, and data visualization solutions (e.g. Looker)
Knowledge of MapReduce and some distributed data processing methodologies and frameworks (e.g. Hadoop, Hive, Pig, Presto, Spark)
Experience with Databricks
Experience with mobile development in Objective-C, Swift or Javascript
We like people who:
Results oriented, hands-on and passionate about our industry (education) and our consumers (children, parents and educators)
Collaborative, and thrive in startup environments
Able and willing to learn new technologies and styles
What you'll get:
HOMER offers competitive compensation including equity and full benefits.
Smart, passionate, and engaged co-workers.
Excellent top-tier Medical/Dental/Vision benefits
401k with 4% company match
The chance to have a big impact, quickly
The rare opportunity to make a dent in the universe. We're bringing a love of reading and learning to children globally!
HOMER is an equal opportunity employer and enthusiastically encourages people from a wide variety of backgrounds and experiences to apply. HOMER does not discriminate on the basis of race, color, religion, sex (including pregnancy), gender, national origin, citizenship, age, mental or physical disability, veteran status, marital status, sexual orientation or any other basis prohibited by law."
TDS Platform Data Engineer,"Toronto, ON",TD Bank,None,Organic,"Tell us your story. Don't go unnoticed. Explain why you're a winning candidate. Think ""TD"" if you crave meaningful work and embrace change like we do. We are a trusted North American leader that cares about people and inspires them to grow and move forward.

Stay current and competitive. Carve out a career for yourself. Grow with us.

Department Overview

Department Overview

The TDS Platform is the cross-asset pricing and risk management platform for TD Securities. TDS is also
the strategic storage and compute infrastructure for numerous business-aligned products used by front office users aswell as risk management functions in the Investment Bank.

These products include many user-facing applications used for real-time risk, scenario risk, electronic trading, and
scalable data analytics and are utilized by a variety of users including traders, desk managers, risk managers, and sales teams.

Job Description

As a Software Engineer for the TDS Platform Data Engineering you are expected to build and evolve a variety of custom software and hardware solutions that enable these products and ensure their future growth and viability from an architectural and technical perspective. These components will in-turn have a direct and tangible impact on the bottom line of all products and business areas that depend on them.

Technology

The TDS Platform is predominantly a system composed of numerous microservices built on open source
technologies. We pride ourselves on leveraging modern tools and technologies that best solve our business problems.
The technology stack that is in place today utilizes the likes of:
Java 8 and Scala for general purpose programming
Graphite, Graphana and ELK for metrics
Linux as the operating system (RHEL 7)
Job Requirements

Job Requirements

Desired Qualifications
Solid understanding of data structures and algorithms
Proficient in asynchronous and concurrent programming
Experience building and delivering scalable and distributed systems using various programming languages
Solid understanding of various approaches to data storage
Strong network programming experience
Experience building large scale distributed systems that have been successfully delivered to customers
Solid understanding of test-driven development and familiarity with best-of-breed tools and technologies
In-depth understanding of the Linux operating system
Desired Interpersonal Skills
Takes great personal pride in building robust software
Strong sense of ownership
Passionate about programming and computer science
Enjoys working in a fast-paced environment
Has excellent written and verbal communication skills
Has strong customer focus
Additional Information

Our team is made up of a number of technology and software engineering enthusiasts coming from a variety of diverse
backgrounds and industries. Last year we welcomed the third Computer Science PhD to the team which further
solidified our commitment to thought leadership and excellence in Computer Science and Software Engineering.
We participate in a number of industry events and engage with a variety of companies to find relevant and modern
solutions to technical and business challenges that we face. In the past year various team members took part in:

OSCON - Open Source Convention
Strata + Hadoop World
QCon
AWS Summit
Atlassian Summit
Hashiconf
…and dozens of local and regional meetups that we attended as a team.""

Inclusiveness

At TD, we are committed to fostering an inclusive, accessible environment, where all employees and customers feel valued, respected and supported. We are dedicated to building a workforce that reflects the diversity of our customers and communities in which we live and serve. If you require an accommodation for the recruitment/interview process (including alternate formats of materials, or accessible meeting rooms or other accommodation), please let us know and we will work with you to meet your needs.

Job Family

Engineering

Job Category - Primary

Technology Solutions

Hours

37.5

Business Line

TD Securities

Time Type

Full Time

Employment Type

Regular

Country

Canada

**Province/State (Primary)

Ontario

City (Primary)

Toronto

Work Location

TD Centre - East - 222 Bay Street

Apply to job
Save
Send to friend"
Senior Data Engineer,"Vancouver, BC",Uplight,None,Organic,"The Position
Do you dream about creating a more sustainable future? At Uplight, we are motivating energy users and providers to accelerate the clean energy ecosystem. We’re working with over 80 of the world’s leading electric and gas utilities to provide a personalized end-to-end customer energy experience that helps meet and accelerate carbon reduction goals. We are a certified B Corp, enabling us to put our values into action by not only making decisions for the benefit of our shareholders, but also for our customers, environment, employees, and community.
We are seeking a 6-month Contract Senior Data Engineer to join our team and help us achieve our ambitious goals for our business and for the planet.
What you get to do:
We are searching for a disciplined Senior Software Engineer with expertise in working with data who is passionate about changing the way millions of people save energy. You’ll work within the Engineering Pillar to build and improve our platforms (especially our API layers and data processing) to deliver flexible and creative solutions to our utility partners and end-users. You’ll be a part of developing a robust architecture to manage massive amounts of data and leveraging it to deliver a compelling experience to users across our platforms. And you’ll work on a great team with excellent benefits.
You should:
Be excited to work with talented, committed people in a fast-paced environment
Be passionate about energy consumption and making a difference in the world
Love developing great software - some of your proudest moments involve GitHub
Be capable of prioritizing your work to adhere to a deliverable schedule and ensure successful delivery that exceeds expectations
Be ready, able, and willing to jump onto a call with a partner or customer to help solve problems
Have a strong eye for detail and quality of your code
Not be afraid to dig into hard problems, and enjoy experimenting to come up with simple, elegant solutions
What you bring to Uplight:
A love of collaborative problem solving and a deep desire to be an excellent teammate
You are an experienced developer - you ideally have 3-7 years of professional experience
Python experience, preferably both 2.7 and 3.x
Deep experience building ETLs and data pipelines using tools such as Apache Airflow and Spark
Experience working with large data sets in both SQL and NoSQL databases (PostgreSQL, DynamoDB, etc.)
Google Cloud Platform or other cloud service provider experience
Demonstrable experience with writing unit and functional tests
Application of industry security best practices to application and system development
Ability to deliver against several initiatives simultaneously
BS in Computer Science / Engineering, or demonstrable industry experience
Experience mentoring peers and leading technical teams through the decision making
Bonus Points:
Experience with cloud infrastructure automation such as terraform, cloud formation, or similar
Solid CI/CD experience
Automation of multi-step repetitive tasks
Serverless architecture, preferably Google Cloud Functions
You work on the command line confidently and are familiar with all the goodies that the Linux toolkit can provide
You are a Git guru and revel in collaborative workflows
Pandas / SciPy / Numpy / R / SciKitLearn
What makes working at Uplight amazing:
In addition to all the standard medical and dental benefits, that kick in Day 1, we:
Are proud to be over 300+ rebels with an important cause by helping to create a more sustainable planet.
Are committed to the environment, our employees, and our communities
Focus on career growth by following defined career ladders
Take our work and mission seriously and…. we love to laugh!
Uplight provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws."
Data Engineer - 298310,"Toronto, ON",Procom,None,Organic,"Data Engineer
On behalf of our client in the Banking Sector, PROCOM is looking for a Data Engineer.
Data Engineer – Job Description
The GTS - Technology Operations & Engineering is managing a large scale enhancements to the systems data lakes.
The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver.
Work closely with Product Owner and other SMEs to interpret requirements and translate them to software solutions.
Ensure delivery of negotiated product/function while following standards and methodologies. This includes the design, development, and unit testing of solutions to optimize, create efficiencies, address root cause incidents or
Create and maintain detailed design documents as well as supporting the lifecycle of those documents.
Build and test integration software solutions.
Help maintain code quality, organization, and performance.
Participate in technical meetings with client's technical specialists.
Provide support for testing efforts and defect resolution.
Provide deployment and post deployment support (Ex. warranty support, command center services, process, review release content and coordinate with clients.
Operations: Maintain and troubleshoot production & non-production environments including certificate management, patch management. Create and maintain alerts and dashboards, access control management and perform system admin activities. Create deployment and operations guides. Perform NFT testing on the ecosystem to ensure resilience & peak performance.
L2 Support Services: Restore services within agreed upon SLA. Respond to Major Incidents impacting technical components, seek opportunities for improvement.
Focus on logical support work, like configuration, data management, application performance and tuning, application troubleshooting
Data Engineer – Mandatory Skills
4+ years hands on experience with Big Data software designs, development and operations, specifically Hadoop and Hive
3-4 years back end development with Python, Java OR Scala
3 + years Data ingestion tools such as: Diyotta, Data Stage, MapReduce
Strong understanding of HDFS and Cloud Services, proven through recent project experience (1-2 Recent projects)
Strong communication skills to clearly articulate code tuning and performance issues to development team
Completion of a post-secondary education in Computer Science, Engineering, or a technology related field.
Data Engineer – Nice to Have Skills
Experience with continuous integration
Financial Industry experience
Data Engineer - Assignment Start Date
ASAP – 12 months to start
Data Engineer - Assignment Location
Toronto, ON – Working Remotely"
"Senior Software Engineer - Backend, Big Data","Markham, ON",The Nielsen Company - Engineering,"$80,000 - $130,000 a year",Organic,"Job Summary
We seek passionate, creative and innovative developers to join our team and help build leading edge technologies. With a balance of development, design, and cutting-edge technology, daily routine will never show its boring face.
As part of this role, you will be working on developing solutions to ingest and process large volumes of data, apply complex algorithms using Spark, develop backend services, and more!
Qualifications and Skills
Important note, please read: This position requires a very good understanding of distributed data processing, backend development concepts, and strong fundamentals in Computer Science. Through the interview process, you are required to provide adequate solutions to data processing problems independent of a particular framework, which includes Spark.
Please do not apply for this position if you're just interested in getting into Spark development. For that please apply for our ""Junior or Senior Developer"" positions.
The ideal candidate will have:
Familiarity with data processing frameworks, e.g. Spark or similar
Solid understanding of the foundations of Computer Science and Software Engineering
Solid understanding of data structures and fundamental algorithms (sort, select, search, queue, encryption).
At least 3 years of experience in software development with Java and/or Scala.
Experience with Networking and IO, REST service development, and multi-threading.
Familiarity with Linux
Familiarity with Docker, AWS, Node JS, and Python are assets
Eager to learn and willing to take initiatives
Authorized to legally work in Canada.
The interview process is very straightforward. So, give us a go!
We provide competitive salaries and benefits.
Job Type: Full-time
Salary: $80,000.00-$130,000.00 per year
Benefits:
Casual dress
Dental care
Disability insurance
Extended health care
Life insurance
Paid time off
RRSP match
Vision care
Schedule:
Monday to Friday
Experience:
Software Engineering: 3 years (Required)
Work remotely:
Temporarily due to COVID-19"
Data Engineer - Montreal,"Montréal, QC",Randstad,None,Organic,"Data Engineer
6 Months Contract
Montreal

Must have requirement(s)

Design, implement, and maintain data pipelines with complex data transformations;
Assemble large, complex data sets that meet functional / non-functional business requirements;
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs;
Experience working with business teams to translate functional requirements into technical requirements;
Conduct business and functional requirements gathering and provide projects estimates;
Excellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked;
Experience supporting and working with cross-functional teams in a dynamic environment;
Strong software engineer background;

Technologies required

Strong Experience with PySpark,
Strong Experience with Databricks
Strong experience in AWS ecosystem
A minimum of 8 years industry experience working with data, coding and scripting in Python/Java/Scala/SQL/JS/Bash, design and testing
Good experience with major Big Data technologies and frameworks including but not limited to Hadoop, HIVE, MapReduce, Spark, Cassandra, Kafka, Elasticsearch
Experience with relational SQL and NoSQL databases, including Oracle, Postgres and Cassandra.

Other requirements

Knowledge of data visualization best practices and cloud warehouses (i.e. Snowflake, Redshift) would be an asset"
Remote Data Engineer,"Montréal, QC",MonetizeMore,None,Organic,"MonetizeMore builds industry leading ad technology that is seen by more than 300M people per month. The company has been running for 10 years achieving consistent double digit growth each year with a team of 100+ team members spread across the globe.
MonetizeMore offers location and schedule freedom to every one of its team members. That means that you would have the lifestyle autonomy to choose to work from anywhere in the world, during the time of day you prefer. This new-age work lifestyle would enable you to engineer your ideal lifestyle. Say goodbye to endless commutes, stuffy business attire and the arbitrary 9 – 5 work day. Take your life back into your hands by joining the MonetizeMore team!

The product team is the fastest and most innovative team in the company. Build greenfield technology that is disrupting the ad technology industry. Solve problems that have never been solved before. Join a company culture that replaces constant meetings and interruptions with innovation that continues to break boundaries. Take your skillset to the next level with some of the best minds in the ad technology industry to make a real difference with MonetizeMore.

Responsibilities
The day-to-day work of a MonetizeMore Data Engineer includes:
Building and maintaining data pipelines using Kubernetes, Airflow and AWS Stack
Proficient in writing complex and nested SQL queries. Experience with AWS Athena is a plus.
Analyze data using Python Pandas, Apache Spark DataFrames, ElasticSearch Kibana.
Building JavaScript applications capable of tracking and responding to billions of requests per month.
Developing APIs and integrating with 3rd party APIs to automate manual tasks.
Integration of services to maximize ad revenues and maintain strong user experience.
Planning and prototyping new applications.
Defect resolution of existing and new issues.
Unit testing new features to ensure they conform to MonetizeMore’s quality standard and meet requirements.
Code reviews.
Running performance benchmarking tests.
Staying up to date with new trends and advancements in web development and ad tech.
Attending daily stand-up meetings (30 mins) and other Scrum meetings (Every 2 weeks).

Attributes
MonetizeMore Data Engineer attributes include:
Teamwork Attributes:
Collaboration: Working remotely on complex projects necessitates that you work together with your team and share knowledge.
Communication Skills: You are comfortable communicating in English at all levels, have strong spoken and written communication skills and are an active listener.
Teamwork: You value team synergy and are excited about helping your team succeed.
Interpersonal Skills: You are able to get along, work well and coordinate with others.
Conflict Management: As a team, we are proactive in dealing with conflict.You are able to find constructive ways of resolving issues with other team members.
Technical Attributes:
Technology: A MonetizeMore developer is proficient in all stages of web development, from conception to deployment. You are a one-person army, ready and willing to attack any technical challenge that crosses your path
Analytical and Problem Solving Skills: You work hard to understand technical issues and to resolve them in an effective manner.
Detail Orientation: You work on many parts of an application or system at the same time and are able to focus on each detail meticulously.
Initiative: You work well in a team, with little supervision, making well-reasoned and effective technical decisions.
Reliability and Responsibility: You demonstrate reliability at all times. You give reasonable expectations within Agile Scrum framework and work hard and smart to achieve and surpass those expectations. You communicate what you are going to do, then meet that commitment.
Thought Leadership: You analyze MonetizeMore’s tech stack, systems and processes with the goal to iterate on a regular basis. You look for opportunities to improve to increase value to MonetizeMore and suggest them to the team.
If you think you are a good fit to join the MonetizeMore product team, please apply below and give specific reasons what sets you apart. We hire individuals not robots so don’t be afraid to show a little personality ;)

Powered by JazzHR
NhoveNgBx5"
Azure Data Engineer,"Toronto, ON",Mantu,None,Organic,"Company Description
Amaris Consulting is an independent technology consulting firm providing guidance and solutions to businesses. With more than 1000 clients across the globe, we have been implementing solutions in major projects for over a decade – this is made possible by an international team of 7000 people from 95 different nationalities, spread across 5 continents and more than 60 countries. Our solutions focus on four different Business Lines: Information System & Digital, Telecom, Life Sciences and Engineering. We’re focused on building and nurturing a top talent community where all our team members can achieve their full potential.
Amaris is your steppingstone to cross rivers of change, meet challenges and achieve all your projects with success.
Your Role
We are looking to grow our Information Systems & Digital team with the expertise of a Data Engineer with Azure skills to be able to support process data warehouse project activities with one or different clients in the pharmaceutical and/or Food & Beverage Industry.
Your main responsibilities:
Perform ETL activities.
Maintain legacy process data systems and applications.
Perform some process control web based dashboards and monitoring tools to track raw materials, in-house process tests and final product release criteria.
Perform graphical user interfaces and scripts for data analysis and visualization.
Perform some codes with language-specific frameworks and environments.
Provide technical support and software debugging to all use.
Qualifications:
Minimum 2 to 3 years of experience as Data Engineer or related.
Experience with Azure Database services – SQL, MySQL, AAS.
Solid skills in Python
Strong analytical skills
Speak and write good English and French
Why us?
Be part of an international, multicultural environment of 7,750 talented people over 60 countries and 5 continents
Join a fast-growing global group with a turnover of €565M and over 1,000 clients across the world
Explore opportunities to grow quickly with a tailor-made career path: 70% of our key senior talent joined the company at entry level
Take advantage of over 500 training courses in our ACADEMY catalogue, which includes programs in interpersonal communication, team management, project management, etc.
Get the opportunity to support nonprofit organizations thanks to our Foundation initiatives and volunteering platform OneSmallStep
Depending on your performance and eligibility criteria, you can benefit from international mobility opportunities and progress your career worldwide, and/or experience other departments and sectors
Unlock your full potential, both professionally and personally."
ADF Expert/Data Engineer (ETL/SQL),"Toronto, ON",Next Pathway Inc.,None,Organic,"Next Pathway - The Automated Cloud Migration Company
Listed as one of Canada’s hottest start-ups by the Globe and Mail, Next Pathway is a technology services company providing clients a pathway from existing to emerging technologies. Our automation technology helps our customers accelerate the migration of complex applications and workloads to the cloud.
Next Pathway is full of bright and diverse thinkers. With deep exposure to AI, Machine Learning and Robotic Process Automation, our team members have opportunities to be trailblazers in the technology space. We encourage self-starters, transparency and team connectivity. We know diverse teams make strong teams. We welcome people of diverse backgrounds, experiences, and perspectives.
Our work environment is based on 3 core principles:
· Emphasize quality first, each and every time
· Put people in roles where they will succeed and feel challenged
· Build a team of well qualified individuals that can share ideas and learn from each other
Next Pathway rewards people for hard work, loyalty, innovation and mutual support. We aim to match people’s strengths, skills and talents to our requirements. Identifying this ideal match between attitude, skill and need, leads to success.
Next Pathway is located in the heart of the Financial District, minutes from Union Station and the Subway.
We are looking for skilled ADF Expert/Data Engineer (ETL/SQL) to join our team!
The candidate needs to have extensive experience as follows:
· Minimum of 5 years of proven experience in a core competency
· Writing well designed, testable, efficient code
· Develop and maintain unit tests and integration tests, and test automation.
· Proven hands-on Software Development experience
· Strong understanding and experience of relational database, include design and implementation
· Strong Data Warehouse knowledge and experience, include data modeling, data ingestion, transformation, data consumption patterns
· Strong SQL knowledge and experience, including developing and optimizing complex queries, creating efficient UDFs to extend the functionalities
· Experience with Cloud Technologies (Azure, GCP, Snowflake, AWS, Yellowbrick)
· Experience migrating large amounts of ETL Jobs from Datastage(or Informatica, SSIS, Talend) to Azure Data Factory.
· Experience in at least 3 of below RDBMS: Teradata, Netezza, SQL Server, Greenplum, Oracle, Sybase, DB2, MySQL, Redshift, SQLDW.
· Experience with ETL tools (Datastage, Informatica, SSIS, Talend)
· Experience with major programming languages (Java, Python, Scala, C++, Java Script)
· Good experience using industry-standard design patterns, practices and Cloud strategies.
· Ability to collaborate and communicate effectively with all Stakeholders (i.e. Developers, Business Analysts, Business Stakeholders).
· Adoption of Agile and Scrum development methodology
· Experience working with Multi-Vendor, Multi-Culture Development teams in dynamic and complex environment
· BA/MS/PHD degree in Computer Science, Engineering or a related subject
Other Technologies you Might Work with Include:
· Microsoft Office Suite (Excel, Word, PowerPoint, Outlook)
· Basecamp: Project Management & Team Communication
· Slack: Messaging Platform
· Zoom: Meetings and Video Conferencing
· Confluence: Collaboration Tool
· JIRA: Project Management Software
Other Skills:
· Team Player with Excellent Interpersonal and Communication Skills (Written and Verbal)
· Strong Work Ethic with a Positive Attitude and a Passion for Data and Development
· Strong Analytical and Problem-Solving Skills
· Excellent Time Management Skills
Contract length: 6 months
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Experience:
Data Warehouse: 5 years (Preferred)
ETL/SQL: 5 years (Preferred)
Azure Data Factory: 2 years (Required)
Education:
Bachelor's Degree (Required)
Location:
Toronto, ON (Required)
Work remotely:
No"
Data Engineer,"Toronto, ON",Avanciers,None,Organic,"Title: Data Engineer
Location: Toronto, ON (Remote until the further notice)
Experience: 8+ years
Job Description:
Should have good experience in analyzing data and proposing solutions according to client need
Should have experience in getting data from production to the test environment.
Should have experience in hiding or masking production data before moving to other environment.
Should be well aware of the data setup process.
Must possess strong written and verbal communications
Experience in Health domain will be an added advantage but not necessary
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Experience:
Data Analyst: 5 years (Preferred)
Work remotely:
Yes, temporarily due to COVID-19"
Data Engineer – Hadoop/Hive - 298234,"Toronto, ON",Procom,None,Organic,"Data Engineer - Hadoop/Hive
On behalf of our client in the Banking Sector, PROCOM is looking for a Data Engineer - Hadoop/Hive.
Data Engineer - Hadoop/Hive – Job Description
The incumbent should be a quick learner and must be adept at troubleshoot and problem solving in a highly integrated big data environment.
It is essential that the incumbent to be proactive, eager to learn, have a 'Can Do' attitude, and demonstrate initiative and eagerness to succeed.
The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver.
Work closely with Product Owner and other SMEs to interpret requirements and translate them to software solutions.
Ensure delivery of negotiated product/function while following standards and methodologies. This includes the design, development, and unit testing of solutions to optimize, create efficiencies, address root cause incidents or
Create and maintain detailed design documents as well as supporting the lifecycle of those documents.
Build and test integration software solutions.
Help maintain code quality, organization, and performance.
Participate in technical meetings with client's technical specialists.
Provide support for testing efforts and defect resolution.
Provide deployment and post deployment support (Ex. warranty support, command center services, process, review release content and coordinate with clients.
Operations: Maintain and troubleshoot production & non-production environments including certificate management, patch management. Create and maintain alerts and dashboards, access control management and perform system admin activities. Create deployment and operations guides. Perform NFT testing on the ecosystem to ensure resilience & peak performance.
L2 Support Services: Restore services within agreed upon SLA. Respond to Major Incidents impacting technical components, seek opportunities for improvement.
Focus on logical support work, like configuration, data management, application performance and tuning, application troubleshooting
Data Engineer - Hadoop/Hive – Mandatory Skills
7+ years’ hands on experience with Big Data software designs, development and operations, specifically Hadoop and Hive
5 years’ back end development with Python, Java OR Scala
4 + years’ Data ingestion tools such as: Diyotta, Data Stage, MapReduce
Strong understanding of HDFS and Cloud Services, proven through recent project experience (1-2 Recent projects)
Strong communication skills to clearly articulate code tuning and performance issues to development team
Completion of a post-secondary education in Computer Science, Engineering, or a technology related field.
Data Engineer - Hadoop/Hive – Nice to Have Skills
Experience with continuous integration
Financial Industry experience
Data Engineer - Hadoop/Hive - Assignment Start Date
ASAP – 12 months to start
Data Engineer - Hadoop/Hive - Assignment Location
Toronto, ON – Work Remotely"
Data Migration Engineer - 298085,"Ottawa, ON",Procom,None,Organic,"Data Migration Engineer
On behalf of our private sector client, Procom is seeking a Data Migration Engineer for a contract opportunity.

The Data Migration Engineer will assist our client with their migration from Azure to AWS.

Their current IoT solution to ingest data from radio-spectrum analysis was built in Azure and needs to be migrated into AWS. This included App migration, containerization and some migration of R to Python.

Location: Ottawa (Remote work)
Security requirement: Government of Canada Enhanced reliability or higher

Technical Skills Required:
AWS
Knowledge of R and Python
Containerization (Kubernetes/docker)

Assets:
Azure
Spark
SQL
Warehousing
Lakehouse
AWS Data Tools (EMR, Athena, Glue, S3)
AWS App Dev (S3, Lambda, Step Functions)
Docker"
Data Engineer,"Toronto, ON",Lixar I.T,None,Organic,"Lixar I.T. is looking for a Data Engineer to join our data team. As a Data Engineer, you will be part of a team supporting and participating in the ongoing development of leading edge applications. The ideal candidate will be a senior developer who has a strong background in Big Data with a mix of general programming and some exposure to data visualization.
AS AN EXPERIENCED DATA ENGINEER, YOU HAVE:
Post-secondary education in engineering or computer science or equivalent work experience
A proven track record using the Apache Hadoop ecosystem (Spark, Data Lake, Hive, HDFS, Impala) to tackle “big data” problems
A master of all things SQL (and NoSQL)
5+ years of programming experience in Python
Proven experience using RESTful Web Services & JSON
Good experience using Cloud based data solutions (AWS/Azure)
Experience working with production systems
Knowledge of ELT, ELT, Lambda and Kappa data architectures
PREFERABLY, YOU ALSO HAVE:
Knowledge of Continuous Integration and Source Control systems (e.g. Gradle, Maven, Bamboo, TeamCity, Git)
Experience with DataBricks
Some Data Visualization experience in Power BI, Tableau, or similar
Exposure to data science, machine learning or statistics
Some experience using Docker
AS THE IDEAL CANDIDATE, YOU:
Have a sense of humour
Know that your routine is in fact, not routine
Communicate exceptionally well with management, peers, and clients
Have “Attention to detail” as your middle name
Don’t blame others for your mistakes
Get things done!
Job Type: Full-time
Additional pay:
Bonus pay
Benefits:
Dental care
Extended health care
Paid time off
RRSP match
Wellness program
Work from home
Schedule:
8 hour shift
Experience:
Python: 4 years (Preferred)
Big Data: 2 years (Required)
Spark: 2 years (Required)
Azure or AWS: 1 year (Required)
Work remotely:
Yes"
Data Engineers and Analysts,"Greater Toronto Area, ON",Joon Solutions,None,Organic,"We are currently looking for multiple candidates interested in work on a per project basis. We are currently expanding in Canada and are looking for more data engineers and analysts to join our team! Remote work is ok and Vietnamese speakers are a bonus (to communicate more effectively with some of our staff in Vietnam)
1. Senior Data Engineer
+ 5 years enterprise-level experience
Large data sets (10 petabytes plus)
Data Lakes and Data warehouse architecture (preferably GCP)
Building and maintaining ETL and data pipelines
Hadoop, Spark, BiqQuery, BigTable, Pig, Hive, Python Project management
2. Junior Data Engineer
Experience in:
Large data sets
Datalakes and Data warehouse architecture (preferably GCP)
Building and maintaining ETL and data pipelines
SQL, python, Machine learning
3. Analyst
Experience with:
PowerBI
SQL
BiqQuery
We thank you for your interest in seeking employment with Joon Solutions. Please send your resume to the designated email in the job posting. Please note that only eligible candidates will be contacted for an interview. For more information on our company, please visit www.joonsolutions.com. Thank you.
Job Types: Full-time, Part-time, Contract, Freelance
Benefits:
Flexible schedule
Work from home
Schedule:
Monday to Friday
On call
Weekends
Work remotely:
Yes, always"
Data Engineer,"Toronto, ON",Precision Nutrition,None,Organic,"Wanted: An ambuition Data Engineer who will help us better understand our customers

PN is looking for an experienced database engineer who’s passionate about health, fitness and helping people live their best life.

Precision Nutrition is growing, and we need a team-oriented, inquisitive engineer who will help us improve our database management and build a better product. As one of the largest nutrition coaching companies in the world, PN is bigger than it’s ever been — and it’s about to get bigger.

To help us scale, we need a motivated, approachable jack-of-all-trades who knows how to work with stakeholders and users to gather requirements, design ETL and API processes, and take part in execution and QA so we can do what we do best: help millions more people live healthier, happier lives.

Join our Adventure

We’re embarking on a new adventure, and we need a multi-talented engineer to improve our database management

We’ve got an exciting journey ahead of us… Over the next 10 years, Precision Nutrition will be expanding its world-class education and coaching services into a wide range of new fields, including yoga, group exercise, sports coaching, athletics, manual therapy and more. To help us reach our goals and keep us moving forward, we need a data engineer who knows how to build the pipe end to end—from requirements collection to post-rollout support and QA.


Who we’re looking for

A collaborative self-starter, with a keen attention to detail

You know the tools of the trade. You have a solid understanding of data warehousing tools and technology like Snowflake, as well as ETL tools like FiveTran and DBT. You’re experienced with reporting tools such as Tableau, and you can manipulate and analyze data in Python and SQL like a pro. Other similar tools like Heap, Google Analytics, Stripe, and Salesforce are a plus too.
You’re self-motivated. Once you are confident in what you need to do, you can run with it and don’t need a lot of direction. You have strong self-structure and discipline and have worked remotely either currently or in the past so you know the benefits and the challenges. At the same time, you don’t silo yourself, and are always eager to learn more and share with the team.
You like variety. Not only will you be handling all kinds of systems, apps, and technologies — like the tools mentioned above—but you’ll need to help with execution, QA, and other aspects of our database management. You relish new challenges and get excited by the prospect of wearing multiple hats.
You’re flexible. You have the ability to prioritize and re-prioritize your work in a changing environment. You understand that things change as new information is made available, and can flip from one project to another at the drop of a hat when your attention is needed.
You’re a stickler for the details. You are careful, cautious and incredibly meticulous. Some may even call you nitpicky (in an endearing way, of course). You can spot the smallest mistake before it throws a database off course. Nothing slips by the net when you're the goalie.
You work well with others. You aren’t just here to take orders—you’re an expert in your field who knows how to take charge without shutting others out of the process. You can take ideas from multiple sources and synthesize them into a clear course of action—and then execute it with aplomb.
You’re an exceptional communicator. You know how to get your message across clearly and concisely. You know how to say it, write it and present it in both layman’s terms and a bit more formally if needed. You know how to be heard and energize people around you no matter who you are interacting with, and you’re quick to respond when someone needs your help.

You’re adaptable. You’re comfortable working with cross-functional teams, and you can switch gears as smoothly as a Formula 1 driver. You may be working on requirements gathering one day, and doing support and exception handling another, and you relish the idea of taking ownership of the entire data pipeline.
What you’ll be doing

Learning the company’s data needs...

You’ll work with people across the company to assess what data we need to gather. That means:

Interviewing stakeholders, the analytics team, and users. From those people, you’ll gather requirements for data marts that the marketing, client care, and finance teams can use to better understand our customers.
Optimizing the process. You’ll review our existing requirements gathering process, make it better, and design and develop new processes for doing so on future projects.
...and turning that into action

You’ll then build the data model structure and support it from end to end. That includes:

Designing the process. You’ll develop the ETL processes, leveraging existing APIs or working with the tech team to build new ones.
Build and document everything. You’ll build, test, and fine-tune the entire data warehouse, and create detailed documentation so our team isn’t lost when you aren’t around.
Gather feedback and enter support mode. Your job isn’t done once the process is rolled out—you’ll also design new QA, security, privacy, and exception handling procedures, not to mention collect a backlog of enhancement requests and plan out the next release while working on other projects.

A few important caveats

This is a dream job, if you’re the right person for it.

Must-Have #1: You must be experienced.

You’ve spent 5+ years working in a data engineering role of some sort. You’re familiar with some or all of the following: registration and user behavior data, digital marketing information, customer support systems, and survey responses. And you have a track record of obtaining requirements, designing, building, implementing, and supporting ETL processes, dimensional data models, and data repositories for business intelligence and analytics purposes.

Must-Have #2: You must be results-oriented.

You know how to work with others and pay attention to the details—without getting slowed down by either. You know when to take more time and you know when it’s time to move full throttle and get things done. You aren’t satisfied until you’ve seen a project through to completion.
Must-Have #3: You thrive on collaboration.

You’re a self-starter, but you know that great things aren’t built by just one person. You don’t just play nicely with others, you excel at it—instead of being told what to do, you want to meld minds with your coworkers, bring out their best ideas, and build something truly awesome.

Wondering if this is the right path for you?

Every year, professionals at the top of their field choose to join Precision Nutrition.

Here are six reasons why.

Reason #1: We give you the freedom to “do you.”

Unlike most companies, we don’t have rigid rules about how and when to do things. You’ll always be free to work independently, whenever, wherever, and however you want.

Reason #2: You can work from anywhere.

We’re a 100% remote company, and have been working remote for 17 years. When you join our team, you’re no longer shackled to one desk, one office, one city, or even one country!

Reason #3: Meetings are optional. (No, really.)

You’re an adult. You’re capable of deciding how your time should be spent. If you don’t think you need to attend a meeting, you don’t. Simple as that.

Reason #4: You’ll always feel supported.

In a regular office, it’s easy to feel like a hamster in a wheel, powerless to change things. At Precision Nutrition, we work as a team to overcome issues & barriers that stand in each other’s way, and we treat each other with enthusiasm, compassion, and care.

Reason #5: You’ll never be bored.

People often come to Precision Nutrition after hitting a plateau in their old jobs. At PN, you’ll get the chance to conquer new challenges, learn from the best, and reach thrilling new heights of personal and professional growth.
Reason #6: You’re free to be you.
At Precision Nutrition, we want everyone to live healthier, happier lives—no matter your race, age, gender, religion, or sexual orientation. That's why diversity is a key ingredient of our workforce, so we can best represent the people we serve. Everyone is welcome—as an inclusive workplace, we want our employees to bring their authentic whole selves to work. Be you.
Sound like we might just be the perfect company for you? Then you might just be the perfect fit for us. Click Apply for this Job below to get started!"
Data Engineer,"Vancouver, BC","Amazon Data Services CAN, Inc.",None,Organic,"This position requires a Bachelor's Degree in Computer Science or a related technical field, and 5+ years of relevant employment experience.
At least 3 year experience developing ETL/ ELT solutions within a Data Warehouse environment
At least 3 years experience using SQL, PL/SQL or Shell scripting

We are looking for an outstanding Data/Database Engineer who is data-driven, uncompromisingly detail oriented, smart, efficient, and driven to help our business succeed. You have passion for technology. You are keen to leverage existing skills while trying new approaches. You are not tool-centric; you determine what technology works best for the problem at hand and apply it accordingly. You can explain complex concepts to your non-technical customers in simple terms.

We are moving from traditional database technologies to near real time data processing and advanced reporting services built around natural language processing and machine learning.

You will help us analyze large amounts of data, discover and solve real world problems and build metrics and business cases to help business teams to make decisions. You should be motivated self-starter that can work independently in a fast paced, ambiguous environment.

Oracle and SQL performance tuning
Strong critical thinking and attention to detail
Ability to work and communicate effectively with developers and Business users
Experience in big-data using Hadoop, Hive, and other open-source tools/technologies
Familiar with AWS tools
Ability to work independently with minimum supervision
Experience in designing and building large data warehouse systems
Strong organizational and multitasking skills with ability to balance competing priorities
Good work experience in BI Reporting tools and databases in a business environment.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us ."
Data Engineer,"Kitchener, ON",Index Exchange,None,Organic,"Data is a big deal at Index Exchange. Our advertising exchange generates multiple terabytes of auction-related information each day and our Data Engineering team builds tools and infrastructure to help manage this vast amount of data. Working within our larger R&D team, our Data Engineers use a specialized suite of tech and hardware to pipe, parse and maintain our data, including the oversight of a multi-petabyte cluster. We're looking for Engineers to join our team and help build the next generation of big data solutions at Index Exchange.
Working on the bleeding edge of exciting technology, you're afforded the opportunity to experiment with new tools and attempt radically different approaches than traditional software engineering affords. Every day with the Data team is different and each project presents its own set of new and exciting challenges. Things shift very quickly in our industry and we rely on the Data team to keep us ahead of the curve and moving in the right direction.
We are a global technology company that connects the largest publishers and marketers across the globe through programmatic advertising. We ensure publishers are able to secure their bottom line so they can provide content to consumers and bring news and information to society at large. Our mission is to democratize digital advertising, all while helping marketers reach the audiences that matter most to them across digital channels. For over 20 years, IX has been known as the change agents of digital advertising – innovators who help shape how our industry reacts to new challenges and shifts in the market. As a result, we've been able to partner with some of the most prominent players in the media industry including Hearst, Disney, and Meredith Corporation among many others.
Here's what we want:
Problem Solver: You are curious and loves exploring multiple approaches to find the most efficient, scalable solution and solve a problem
Collaborative: You work well with other people
Passionate: A passion for Big Data and an interest in the latest trends and developments constantly researching new tools and data technologies
Self-starter: You are comfortable helping your team get things done
Here's what you'll be doing:
Design, implement, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services
Identify, design, and implement system performance improvements
Identify, design, and implement internal process improvements
Automate manual processes and optimize data delivery
Useful skills/background:
You may or may not tick off every box, and that's ok. Each person brings a different background and different skills. If you think you are a good match for what we are looking for tell us why, and tell us what you are doing to improve yourself and we'll see what we can do to help!
A degree in Computer Science/Engineering or related field
2-4 years of experience in a software engineering environment
Experience with SQL and NoSQL systems
Knowledge of Hadoop, Spark, Kafka or other equivalent technologies
Proficiency in some of the following languages: Scala, Java, Python, Bash
Experience with automated testing systems
Mentorship, collaboration, and communication skills
Knowledge of data modelling, data warehousing, ETL processes, and business intelligence reporting tools
Experience working with CI/CD, containerization, and virtualization tools such as Gitlab, Jenkins, Kubernetes, Docker
Why you'll love working here:
A bright and inviting shared office space in Downtown Kitchener-Waterloo easily accessible by public transit or bike*
Our teams are built around cooperation, teamwork, and respect
We don't have a dress code and offer flexible work hours
Annual celebrations (Holiday Party, All Hands, Hack Days) and milestone outings
100% coverage for health and dental benefits, 12 incidental sick days per year, 5% RRSP matching, and monthly gym subsidy.
EQUAL EMPLOYMENT OPPORTUNITY Index Exchange is an equal opportunity employer who recognizes the value of every person in creating success for our customers, business partners, shareholders, employees and communities. We are committed to recruiting, hiring, developing and promoting employees without discrimination. Index Exchange believes equal opportunity and inclusion are essential to motivate, empower and recognize the best in everyone.
Index Exchange is committed to working with and providing access and reasonable accommodations to applicants with disabilities. Please let us know if you'd like to request a reasonable accommodation.
ACCESSIBILITY FOR APPLICANTS WITH DISABILITIES Index Exchange is committed to working with and providing reasonable accommodation to individuals with disabilities. In accordance with the Accessibility for Ontarians with Disabilities Act, 2005 and the Ontario Human Rights Code, Index Exchange* will provide accommodations throughout the recruitment and selection process to applicants with disabilities. If you would like to request a reasonable accommodation, please email hr.support@indexexchange.com or call 416.785.5908 ensure to provide your name, the best way to contact you, a detailed description of the nature of any accommodation that you may require (including any materials or processes that can be used to ensure your equal participation).
Considering COVID-19, until further notice, everyone is required to Work from Home. When offices re-open, you will be required to follow the mandate of your local office."
Senior Data Engineer - Analytics,"Waterloo, ON",SkyWatch,None,Organic,"At SkyWatch, we believe that everyone on the planet should benefit from the thousands of satellites that orbit the earth. We are building a platform to put the world’s Earth observation data in one place — a simple API any developer can use. Our team of extraordinarily talented people has built a cutting-edge microservices platform on a serverless stack, and we’re growing fast.
OUR TEAM
Let’s face it, the best jobs boil down to a handful of things — getting the chance to work with amazing people in a great culture with leaders who have your back while pursuing a vision that you can get passionate about.

Our team consists of NASA award-winning developers, Earth observation scientists, big data specialists, and unicorn startup veterans from places like Facebook, IBM, and Berkeley and who come from five of the seven continents. Our leadership team includes the Chair of Canada’s first Space Accelerator for startups, an Earth observation veteran who has led multiple companies through IPOs, a planetary robotics graduate from the International Space University, and a petabyte-scale data leader with patents in location AI and contextual analytics. We are Techstars ‘16 NYC and Google for Entrepreneurs alumni. We have a unique blend of space/satellite authorities and consumer application veterans. And we are just getting started!

OUR CULTURE
Culture is the personality of a company. At SkyWatch, our culture is incredibly important to us, and we practice it every day. As a company, we value innovation, dependability, and accessibility. In our product, we value simplicity, affordability, and informability.

In engineering, we specifically value our team’s happiness over unrealistic product planning (People First), flexibility over a rigorous top-down process (Freedom & Autonomy), and improving our overall system over dwelling on unintended mistakes or bugs (Just Culture).

Our mission and values are on the walls of our office. We don’t just talk the talk, we walk the walk.

OUR PRODUCT
Every day, hundreds of trillions of pixels of our planet are captured from space. With new applications for this data such as precision farming, construction planning, disaster relief operations, and retail revenue prediction (to name a few), the demand for satellite imagery is growing. However, the current process for obtaining satellite imagery is very time-consuming and tedious.

While the number of sensors observing our planet is growing exponentially, we are seeing each of these instruments creating their own unique dataset. Getting access to many of these catalogs is expensive and time-consuming. And once access is achieved, is it difficult to integrate data from multiple sources into one system. And we all know the most impactful analytics happen when large volumes of data are normalized and can be easily incorporated into powerful models.

Using our team's experience in building NASA-award winning software, we are building the SkyWatch API, a simple way to discover and access the world’s remote sensing data. We are making it easy for organizations and developers to monitor the progress of crops, predict the markets, track ships and airplanes, measure global warming, or create other game-changing applications. We do the data work behind the scenes so developers are free to build the apps that change the world.

Sound exciting? You’re right — it is.

ABOUT THE ROLE
Where do you fit in as a Senior Data Engineer? Great question.

ABOUT YOU
You have the ability to solve problems, not just complete tasks. You are an artist and a craftsperson, not an assembly line worker. You empathize with those who use what you’ve built. You want to solve their problems.

You have strong technical chops but can also describe technical things to non-technical people. You can tell a ten-year-old how an API works and not have them glaze over with boredom. Or a CEO. Or your significant other. OK, maybe not your significant other.

You have a diverse background. Volunteer as a Big Sister? Founded a Meetup? Love to paint? Play a sport? We love diversity and value how much it adds to our team.

You have a thirst for learning. We learn something every day. We find it intoxicating. If you’d rather master one thing and do that every day, this probably isn’t the role for you.

You have thoughtful opinions that you feel comfortable sharing. We have a culture that makes sharing opinions safe. If everyone in a meeting always thinks the same way, we have too many people in that meeting.

You have the ability to give more than you take. And with an incredible team, we want you to make it even more incredible!

YOUR TECHNICAL ARSENAL
You understand the importance of code reviews and writing clean, maintainable code
You believe in automated testing and can explain why it’s so important
You’ve built software of some kind to solve a problem in your life outside of work.
You have experience with designing and building data pipelines, both instrumenting code and building out the backend infrastructure
You have a passion for uncovering what user behaviour is really going on and determining the signal through the noise
You love working with other engineers, product management, and others throughout the organization on highly impactful projects

YOUR TYPICAL DAY
9:00 am — you roll in, catch up on Slack, email, system status, and important metrics.

10:00 am — you participate in our daily stand up, sharing what you did yesterday, what you plan on doing today, and if there are any roadblocks.

10:15 am — you dig into building our newest microservice, validating the problem it will solve, and designing inputs and outputs.

12:00 pm — you hit an awesome take out place in Uptown Waterloo and chow with the team.

1:00 pm — you put on the headphones and get dialed in to build that microservice.

3:30 pm — happy with the code, you deploy to dev and if tests pass, staging.

4:00 pm — you attend a design session for a new feature, contributing thoughts, opinions, and experience to the discussion.

5:00 pm — you wrap up unit tests, prep for deploy tomorrow, and pack up for the day.

A FEW MORE THINGS TO KNOW
We offer a solid compensation package including a competitive salary, stock options, and a comprehensive benefits plan for you, and your family.
We have flexible work hours and flexible remote working options.
In normal times, our HQ is in Uptown Waterloo, at the Communitech Data Hub, where we are surrounded by other awesome startups focused on data, AI, machine learning, and IOT."
Data Engineer,"Toronto, ON",Precision Nutrition,None,Organic,"Wanted: An ambuition Data Engineer who will help us better understand our customers

PN is looking for an experienced database engineer who’s passionate about health, fitness and helping people live their best life.

Precision Nutrition is growing, and we need a team-oriented, inquisitive engineer who will help us improve our database management and build a better product. As one of the largest nutrition coaching companies in the world, PN is bigger than it’s ever been — and it’s about to get bigger.

To help us scale, we need a motivated, approachable jack-of-all-trades who knows how to work with stakeholders and users to gather requirements, design ETL and API processes, and take part in execution and QA so we can do what we do best: help millions more people live healthier, happier lives.

Join our Adventure

We’re embarking on a new adventure, and we need a multi-talented engineer to improve our database management

We’ve got an exciting journey ahead of us… Over the next 10 years, Precision Nutrition will be expanding its world-class education and coaching services into a wide range of new fields, including yoga, group exercise, sports coaching, athletics, manual therapy and more. To help us reach our goals and keep us moving forward, we need a data engineer who knows how to build the pipe end to end—from requirements collection to post-rollout support and QA.


Who we’re looking for

A collaborative self-starter, with a keen attention to detail

You know the tools of the trade. You have a solid understanding of data warehousing tools and technology like Snowflake, as well as ETL tools like FiveTran and DBT. You’re experienced with reporting tools such as Tableau, and you can manipulate and analyze data in Python and SQL like a pro. Other similar tools like Heap, Google Analytics, Stripe, and Salesforce are a plus too.
You’re self-motivated. Once you are confident in what you need to do, you can run with it and don’t need a lot of direction. You have strong self-structure and discipline and have worked remotely either currently or in the past so you know the benefits and the challenges. At the same time, you don’t silo yourself, and are always eager to learn more and share with the team.
You like variety. Not only will you be handling all kinds of systems, apps, and technologies — like the tools mentioned above—but you’ll need to help with execution, QA, and other aspects of our database management. You relish new challenges and get excited by the prospect of wearing multiple hats.
You’re flexible. You have the ability to prioritize and re-prioritize your work in a changing environment. You understand that things change as new information is made available, and can flip from one project to another at the drop of a hat when your attention is needed.
You’re a stickler for the details. You are careful, cautious and incredibly meticulous. Some may even call you nitpicky (in an endearing way, of course). You can spot the smallest mistake before it throws a database off course. Nothing slips by the net when you're the goalie.
You work well with others. You aren’t just here to take orders—you’re an expert in your field who knows how to take charge without shutting others out of the process. You can take ideas from multiple sources and synthesize them into a clear course of action—and then execute it with aplomb.
You’re an exceptional communicator. You know how to get your message across clearly and concisely. You know how to say it, write it and present it in both layman’s terms and a bit more formally if needed. You know how to be heard and energize people around you no matter who you are interacting with, and you’re quick to respond when someone needs your help.

You’re adaptable. You’re comfortable working with cross-functional teams, and you can switch gears as smoothly as a Formula 1 driver. You may be working on requirements gathering one day, and doing support and exception handling another, and you relish the idea of taking ownership of the entire data pipeline.
What you’ll be doing

Learning the company’s data needs...

You’ll work with people across the company to assess what data we need to gather. That means:

Interviewing stakeholders, the analytics team, and users. From those people, you’ll gather requirements for data marts that the marketing, client care, and finance teams can use to better understand our customers.
Optimizing the process. You’ll review our existing requirements gathering process, make it better, and design and develop new processes for doing so on future projects.
...and turning that into action

You’ll then build the data model structure and support it from end to end. That includes:

Designing the process. You’ll develop the ETL processes, leveraging existing APIs or working with the tech team to build new ones.
Build and document everything. You’ll build, test, and fine-tune the entire data warehouse, and create detailed documentation so our team isn’t lost when you aren’t around.
Gather feedback and enter support mode. Your job isn’t done once the process is rolled out—you’ll also design new QA, security, privacy, and exception handling procedures, not to mention collect a backlog of enhancement requests and plan out the next release while working on other projects.

A few important caveats

This is a dream job, if you’re the right person for it.

Must-Have #1: You must be experienced.

You’ve spent 5+ years working in a data engineering role of some sort. You’re familiar with some or all of the following: registration and user behavior data, digital marketing information, customer support systems, and survey responses. And you have a track record of obtaining requirements, designing, building, implementing, and supporting ETL processes, dimensional data models, and data repositories for business intelligence and analytics purposes.

Must-Have #2: You must be results-oriented.

You know how to work with others and pay attention to the details—without getting slowed down by either. You know when to take more time and you know when it’s time to move full throttle and get things done. You aren’t satisfied until you’ve seen a project through to completion.
Must-Have #3: You thrive on collaboration.

You’re a self-starter, but you know that great things aren’t built by just one person. You don’t just play nicely with others, you excel at it—instead of being told what to do, you want to meld minds with your coworkers, bring out their best ideas, and build something truly awesome.

Wondering if this is the right path for you?

Every year, professionals at the top of their field choose to join Precision Nutrition.

Here are six reasons why.

Reason #1: We give you the freedom to “do you.”

Unlike most companies, we don’t have rigid rules about how and when to do things. You’ll always be free to work independently, whenever, wherever, and however you want.

Reason #2: You can work from anywhere.

We’re a 100% remote company, and have been working remote for 17 years. When you join our team, you’re no longer shackled to one desk, one office, one city, or even one country!

Reason #3: Meetings are optional. (No, really.)

You’re an adult. You’re capable of deciding how your time should be spent. If you don’t think you need to attend a meeting, you don’t. Simple as that.

Reason #4: You’ll always feel supported.

In a regular office, it’s easy to feel like a hamster in a wheel, powerless to change things. At Precision Nutrition, we work as a team to overcome issues & barriers that stand in each other’s way, and we treat each other with enthusiasm, compassion, and care.

Reason #5: You’ll never be bored.

People often come to Precision Nutrition after hitting a plateau in their old jobs. At PN, you’ll get the chance to conquer new challenges, learn from the best, and reach thrilling new heights of personal and professional growth.
Reason #6: You’re free to be you.
At Precision Nutrition, we want everyone to live healthier, happier lives—no matter your race, age, gender, religion, or sexual orientation. That's why diversity is a key ingredient of our workforce, so we can best represent the people we serve. Everyone is welcome—as an inclusive workplace, we want our employees to bring their authentic whole selves to work. Be you.
Sound like we might just be the perfect company for you? Then you might just be the perfect fit for us. Click Apply for this Job below to get started!"
Data Engineer (Full Time Permanent Remote Work Opportunity),"Greater Toronto Area, ON",BDO,None,Organic,"Your opportunity:
Lixar, fueled by BDO Canada, is looking for a dynamic and dedicated Data Engineer to join our remote Canadian team. As a Data Engineer, you will be part of a team supporting and participating in the ongoing development of leading edge applications.
The ideal candidate will be a senior developer who has a strong background in Big Data with a mix of general programming and some exposure to data visualization.

As an experienced Data Engineer, you have:

Post-secondary education in engineering or computer science or equivalent work experience
A proven track record using the Apache Hadoop ecosystem (Spark, Data Lake, Hive, HDFS, Impala) to tackle ""big data"" problems
A master of all things SQL (and NoSQL)
5+ years of programming experience in Python
Proven experience using RESTful Web Services & JSON
Good experience using Cloud based data solutions (AWS/Azure)
Experience working with production systems
Knowledge of ELT, ELT, Lambda and Kappa data architectures

Preferably, you also have:

Knowledge of Continuous Integration and Source Control systems (e.g. Gradle, Maven, Bamboo, TeamCity, Git)
Experience with DataBricks
Some Data Visualization experience in Power BI, Tableau, or similar
Exposure to data science, machine learning or statistics
Some experience using Docker

How do we define success for your role?
You demonstrate BDO's core values through all aspect of your work: Integrity, Respect and Collaboration
You understand your client’s industry, challenges, and opportunities; client describe you as positive, professional, and delivering high quality work
You identify, recommend, and are focused on effective service delivery to your clients
You share in an inclusive and engaging work environment that develops, retains & attracts talent
You actively participate in the adoption of digital tools and strategies to drive an innovative workplace
You grow your expertise through learning and professional development.

Why BDO?

Our firm is committed to providing an environment where you can be successful in the following ways:
We enable you to engage with the firm's strategic plan, and be a key contributor to the success and growth of the firm.
We help you be the best professional you can be in our services, industries and markets.
Achieve your personal goals outside of the office and make an impact on your community.

Giving back, it adds up: Where company meets community. BDO is actively involved in our communities by supporting local charity initiatives. We support staff with local and national events where you will be given the opportunity to contribute to your community.

Total rewards that matter: We pay for performance with competitive total cash compensation that recognizes and rewards your contribution. We provide flexible benefits from day one, and a market leading personal time off policy. We are committed to supporting your overall wellness beyond working hours, and provide reimbursement for wellness initiatives that fit your lifestyle.

Everyone counts: We believe every employee should have the opportunity to participate and succeed. Through leadership by our Chief Inclusion and Diversity Officer, we are committed to a workplace culture of respect, inclusion, and diversity. We recognize and celebrate the valuable differences among each of us, including race, religious beliefs, physical or mental disabilities, age, place of origin, marital status, family status, gender or gender identity and sexual orientation. If you require accommodation to complete the application process, please contact us.

Ready to make your mark at BDO? Click “Apply now” to send your up-to-date resume to one of our Talent Acquisition Specialists.

To explore other opportunities at BDO, check out our careers page.

#LI-MM1"
Data Engineer,"Ottawa, ON",Micro Focus,None,Organic,"Job Description:

At Micro Focus, everything we do is based on a simple idea: The fastest way to get results is to build on what you have. Our software solutions enable organizations to do just that. Secure and scalable, with analytics built in, they bridge the gap between existing and emerging IT—fast-tracking digital transformations across DevOps, Hybrid IT, Security, and Predictive Analytics. In the race to innovate, Micro Focus customers have the clear advantage. Our portfolio spans the following areas: DevOps | IT Operations | Cloud | Security | Info Governance | Big Data, Machine Learning, & Analytics.
We are currently looking to fill a development position focused on developing new, and improving existing data ingest processors for Interset environments built on new or existing Hadoop and Vertica environments. This role includes creating new NiFi processors for reuse as well as custom processors for specific and individual customer needs. You will also work on Smart Connector ingest development and deployment.
If you’re passionate, creative and love to solve problems, we are looking for you!
Although there is a lot of uncertainty in the market today, especially considering the COVID-19 crisis, we are set up to accommodate fully remote work, and a fully virtual interview, selection, and onboarding process.
What you'll do
Work closely with other development, quality, performance, support and field operations teams, to develop and deliver standardized and custom NiFi processors for data ingest
Continuously develop and improve existing processors to ensure that clients have the best and most performant solutions available
Own and manage the development and deployment of ingest pipelines to customers
Work to deploy ArcSight Smart Connectors to ingest customer data, including creating parser overrides as needed and validating all components in the data flow
Who you are
You are passionate about providing customers with solutions that are surprisingly simple to use, masking the underlying complexity
You see big data analytics is a new frontier with tremendous opportunities to blaze new trails
You care about the success of the team, not just your own
Your sense of humour, passion and enthusiasm shines through in everything you do
The need to learn and grow your skills is part of your DNA!
You’re not afraid to have an opinion and you have solid reasoning to back it up
Job requirements
In order to be considered, you must have:
An undergraduate or Master’s degree in Computer Science or equivalent engineering experience.
Experience developing in a JVM environment (Java, Scala, Clojure)
At least two years of experience developing with or using big data technologies, such as Hadoop, Vertica, HBase, Spark, Elasticsearch, SOLR
Experience with data manipulation, handling and ingest technologies
Experience in using Linux systems (e.g. RHEL, Ubuntu) using command line
Exposure Linux scripting languages, such as Bash or Python
Familiarity with virtualization and containerization technologies (e.g. VMWare ESX, Open Stack, Docker)
Familiarity with software development tools such as GitHub and JIRA
We’d also love it if you had the following (though not required):
Understanding of modern cyber security practices, challenges, tools and techniques
Experience with Smart Connectors, NiFi, Kafka
Exposure to scripting, continuous integration systems, configuration managements systems. Experience with virtualization technologies are a plus.
Experience with cloud platforms such as Amazon Web Services or Microsoft Azure
Interest in understanding and analyzing diverse types of data
About Interset:

We use big data and advanced behavioural analytics to detect and prevent the theft of intellectual property...simply put, we catch bad guys with math. Part of the Micro Focus group of companies, we are a fast-paced, all hands on deck kind of environment where you are respected and listened to from day one. We have a startup feel within the stability and structure of a large global company. We hire people with a wide scope of knowledge and experience that want to jump into self-organizing, cross-functional teams. We manage our own schedules, we support our teammates, and we always make time for fun.
#LI-DK1
#Dice-DK
Job:
Marketing

Micro Focus is proud to be an Equal Opportunity Employer. Prospective employees will receive consideration without discrimination because of race, colour, religion, creed, gender, national origin, age, disability, marital or veteran status, sexual orientation, genetic information, citizenship or any other legally protected status"
Big Data Platform Engineer,"Toronto, ON",Canadian Tire,None,Organic,"Help us boldly shape retail in Canada
Canadian Tire Corporation’s (CTC) rich heritage of serving Canadians from coast-to-coast dates back to 1922. Our vision is to become the #1 retail brand in Canada by 2022 and we are focused on innovating and making important investments in our business, especially when it comes to our people. To reach our goal, we need the best talent to help us evolve and drive change across the business – and boldly help shape Canada’s retail industry. As we strive to be at the forefront of a complex and vastly changing retail industry, it is an exciting time to join the Canadian Tire family of companies.
The Big Data Platform Team is accountable for the engineering and operations of the Big Data Analytics and Business Intelligence platforms at Canadian Tire. Our ultimate goal is to enable the business-critical Big Data use cases by providing secure, highly available, easy-to-use, and cost-effective data platforms. We accomplish this goal by collaborating with internal and external partners, and our platform user community at Canadian Tire.
What you’ll do
Evaluate, deploy and productionize high-availability Big Data platforms (in public cloud, and on premise), for use cases in Advanced Analytics and AI/ML
Migration of data and workflow from legacy platforms to the new technology stacks
Establish best practice and conduct knowledge sharing with platform users on the secure and efficient use of the Big Data platform resources – compute, storage and network bandwidth
Big Data platforms’ operations and support
Cluster performance tuning
Enable platform users with visibility and performance-tuning recommendations of their applications (e.g. Spark, Tez/Hive, etc.)
Incident Management – implement health monitoring and service restoration technologies that effectively improve KPIs such as MTBF, MTTR
Problem Management – lead cross-functional groups in RCA and implementing effective STAR (Steps to Avoid Recurrence)
Change and Configuration Management
Capacity modeling and planning
Show-back/Charge-back mechanisms
Integration with Enterprise Data Warehouse and other BI and ML applications that consume data from Big Data platforms – Snowflake, PowerBI, Databricks, KNIME, for examples
Collaboration with internal and external partners on data governance, project delivery, and security hardening
Who you are:
We are looking for individuals who are:
Creative and courageous, with the ability to manage in an environment of change and ambiguity to help us take bold, strategic moves in this rapidly evolving retail environment
Action oriented, and comfortable taking calculated risks to better serve our customers and business
Outcome focused, critical thinkers with the ability to analyze and visualize, to ensure continuous improvement across our entire business
Collaborative team players with superior influencing skills, who build relationships easily across various stakeholder groups to move initiatives forward
Inclusive leaders who build and develop teams that effectively anticipate and respond to disruption, while consistently delivering strong performance
Ability to work in a fast-paced Agile environment and is transparent and professional in their communication
If you’re curious, ready to take on new challenges and open to doing things differently to help us evolve rapidly, then this is definitely the place to be.
What you bring
Collaborative team player who build relationships across various stakeholder groups to move initiatives forward
Excellent problem-solver with the ability to breakdown and resolve complex issues
Agile and innovative, and able to excel in an environment of change
Continuous improvement mindset
Clear and impactful communications
You have five or more years of hands-on experiences in
Designing and deploying and productionize hybrid (on-premise + cloud) and multi-cloud Big Data platforms
Operating and supporting Big Data platforms and clusters
Linux/Unix administration skills and experience with automation and scripting skills specifically in Python, Perl and shell
Administration skills in Hadoop with specific emphasis on Hortonworks or Cloudera distributions and Databricks
Experience with workload management & performance tuning, environment security and workload optimization
Previous experience in developing and deploying operational procedures, tuning guides and best practices documentation
Domain expertise in the following areas:
Azure Cloud Big Data Reference Architecture and Components
Cloudera CDP-DC and CDP Public Cloud
Security and Access Control
Examples:
Kerberos, SSL/TLS, IPSec and VPNs, SSSD
IAM, ACL, Active Directory,
Ranger
Encryption protocols
Key Management
Compute, Storage, and Resource Management
Examples:
Hive, Tez, Spark, LLAP
YARN
VM and Container
Kubernetes
Cloud storage solutions and HDFS
MySQL, Postgres
Use of TPU/GPU for ML
TensorFlow
Infrastructure as Code/Cluster Automation
Examples :
Ansible
Terraform
Ingestion, workflows and Data Replication
Sqoop
Kafka
NiFi
Diyotta
MLOPs/DevOps and CICD
Bachelor’s degree in Computer Science, Software or Computer Engineering, or the equivalent of formal training and work experience
#LI-VF1
Canadian Tire is an equal opportunity employer. We are committed to a diverse and inclusive workplace for all. We recognize that our future success depends on the perspectives and contributions of all our employees - their diverse backgrounds, abilities and experiences make our business stronger. If you are contacted for a job opportunity, please advise us of any accommodations needed to ensure fair and equitable access throughout the recruitment and selection process. All accommodation information provided will be treated as confidential and used only for the purpose of providing an accessible candidate experience. Information Technology
Ontario-Toronto
Permanent
Full-time
Job Posting
: Nov 4, 2020, 2:20:04 PM"
Data Engineer - Toronto Hub,"Toronto, ON",Veeva Systems,None,Organic,"Our engineering and product teams are organized around our hubs for community and collaboration. Work anywhere means you can work at home or the office on any given day. Your product hub is based on the primary location of your product. You should live within one timezone of your product hub. Our current product hubs are Pleasanton, Columbus, Boston, NYC, Raleigh, and Toronto.

Veeva is looking for a data engineer to create ETL pipelines for our Veeva Data Cloud product. We’re building a system to provide our customers with access to billions of records a day with insightful analysis along with aggregations and transformations.

For this role, we need someone who can design flexible data processes and leverage their Python and Scala skillsets to implement them in an AWS cloud environment.

You’ll be responsible for creating and owning the implementation of numerous data analysis features as well as the pipelines that process those features in a multi-tenant, highly parallel system.
What You'll Do
Design and build scripts and tools that perform data analysis, transformations, aggregations, and other augmentations on large sets of in a spark-based AWS environment (EMR, Glue, S3, Redshift, Athena)
Evaluate various pipeline models, tools, and environments and implement these to push data from our sources through your transformations and finally to our customers
Work with product management and data research teams to prototype and test new ideas then take those to production
Work in a fast-paced, test-driven environment
Requirements
BS degree in Computer Science, Engineering or related subject
3 years+ experience working on Apache Spark applications in either Python (PySpark) and/or Scala
Experience creating spark jobs that work on at least 1 billion records.
Intermediate or greater SQL knowledge
Experience creating data pipelines in a production system
Experience working on AWS environments (S3, EMR, Glue, Redshift)
Nice to Have
Experience working with Data Quality techniques
Java development experience
Experience working with Machine Learning/AI models
Experience with AWS glue
Familiarity with agile methodologies
Experience with the following tools: Jira, Git, Terraform
Perks & Benefits
Allocations for continuous learning & development
Annual budget to donate to the non-profit of your choice
Health & wellness programs
#LI-Remote

Veeva builds enterprise cloud technology that powers the biggest names in the pharmaceutical, biotech, consumer goods, chemical & cosmetics industries. Our customers make vaccines, life-saving medicines, and life-enhancing products that make a difference in everyday lives. Our technology has transformed these industries; enabling them to get critical products and services to market faster. Our core values, Do the Right Thing, Customer Success, Employee Success, and Speed, guide us as we make our customers more efficient and effective in everything they do.

Veeva’s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world."
Senior Data Engineer,"Montréal, QC",Index Exchange,None,Organic,"Data is a big deal at Index Exchange (IX). IX's advertising exchange handles billions of auctions and generates terabytes of auction-related information every day. Our team builds tools and infrastructure to manage this vast amount of data and make it accessible for our customers. We're looking for a Senior Data Engineer to join our team, and help build the next generation of big data solutions at Index Exchange with real-time streaming and batch analytical capabilities.
Working with exciting technologies, your team will experiment with new tools and engineer innovative approaches to solve interesting challenges. Things shift very quickly in our industry, and we rely on our Engineering teams to keep IX and our clients ahead of the curve and moving in the right direction. We're looking for Engineers who have experience in an Agile environment, who can drive innovation, and be a technical leader on our team.
We are a global technology company that connects the largest publishers and marketers across the globe through programmatic advertising. We ensure publishers are able to secure their bottom line so they can provide content to consumers and bring news and information to society at large. Our mission is to democratize digital advertising, all while helping marketers reach the audiences that matter most to them across digital channels. For over 20 years, IX has been known as the change agents of digital advertising – innovators who help shape how our industry reacts to new challenges and shifts in the market. As a result, we've been able to partner with some of the most prominent players in the media industry including Hearst, Disney, and Meredith Corporation among many others.
Here's what we want:
Problem Solver: You are curious and love exploring multiple approaches to find the most efficient, scalable solution and solve a problem
Collaborative: You work well with other people
Passionate: You have a passion for Big Data and an interest in the latest trends and developments constantly researching new tools and data technologies
Self-starter: You are comfortable helping your team get things done
Leadership: You can mentor the team on the design, and implementation of our solutions
Here's what you'll be doing:
Design, implement, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services
Identify, design, and implement system performance improvements
Identify, design, and implement internal process improvements
Automate manual processes and optimize data delivery
Guide and mentor team members
Identify and assess potential solutions for technical and business suitability
Useful skills/background:
You may or may not tick off every box, and that's ok. Each person brings a different background and different skills. If you think you are a good match for what we are looking for tell us why, and tell us what you are doing to improve yourself and we'll see what we can do to help!
Bachelor/Master degree in Data Science, Computer Science, Computer/Electrical Engineering or related field
7+ years of work experience in a software engineering environment
Experience working in Agile methodologies
3+ years of work experience designing and building high performance data applications with Hadoop, Spark, Kafka, Hive or other equivalent technologies
Proficiency in some of the following languages: Scala, Java, Python, Bash
Deep understanding of design principles of distributed systems and familiar with mainstream big data related technologies and distributed frameworks
Experience with SQL and NoSQL systems
Experience with automated testing systems
Mentorship, collaboration, and communication skills
Knowledge of data modelling, data warehousing, ETL processes, and business intelligence reporting tools
Experience optimizing performance with large data sets
Experience working with CI/CD, containerization, and virtualization tools such as Gitlab, Jenkins, Kubernetes, Docker
Why you'll love working here:
A bright and inviting shared office space in Downtown Montreal easily accessible by public transit or bike*
Our teams are built around cooperation, teamwork, and respect
We don't have a dress code and offer flexible work hours
Annual celebrations (Holiday Party, All Hands, Hack Days) and milestone outings
100% coverage for health and dental benefits, 12 incidental sick days per year, 5% RRSP matching, and monthly gym subsidy.
EQUAL EMPLOYMENT OPPORTUNITY Index Exchange is an equal opportunity employer who recognizes the value of every person in creating success for our customers, business partners, shareholders, employees and communities. We are committed to recruiting, hiring, developing and promoting employees without discrimination. Index Exchange believes equal opportunity and inclusion are essential to motivate, empower and recognize the best in everyone.
Index Exchange is committed to working with and providing access and reasonable accommodations to applicants with disabilities. Please let us know if you'd like to request a reasonable accommodation.
ACCESSIBILITY FOR APPLICANTS WITH DISABILITIES Index Exchange is committed to working with and providing reasonable accommodation to individuals with disabilities. In accordance with the Accessibility for Ontarians with Disabilities Act, 2005 and the Ontario Human Rights Code, Index Exchange* will provide accommodations throughout the recruitment and selection process to applicants with disabilities. If you would like to request a reasonable accommodation, please email hr.support@indexexchange.com or call 416.785.5908 ensure to provide your name, the best way to contact you, a detailed description of the nature of any accommodation that you may require (including any materials or processes that can be used to ensure your equal participation).
Considering COVID-19, until further notice, everyone is required to Work from Home. When offices re-open, you will be required to follow the mandate of your local office."
Senior Data Engineer,"Toronto, ON",Manulife,None,Organic,"Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Digital transformers needed!
Does rapid iteration, experimentation and curiosity fuel your purpose? Do you want to be part of our mission to make decisions easier and lives better for our customers and employees?
Manulife / John Hancock is on a journey to become a customer obsessed, digital leader. We’re transforming and we need you.
Manulife has a clear vision for a Global Data Strategy. By liberating and strengthening Manulife’s data capabilities we will enable deeper insights, better product and service design, and more effective business processes. The result will be exceptional experiences for our customers.
Manulife’s Group Functions Data Office (GFDO is seeking a Sr. Data Engineer / Data Engineer reporting into the Director, Advanced Analytics and AI Engineering & Enablement Lead. Located in Toronto, Canada the role will (1) champion and support strategic &/or Manulife’s Group Functions Data Office (GFDO) global data initiatives that strengthens Manulife’s global data and advanced analytic capabilities, (2) foster cross-segment collaboration and communication helping build an agile data insight driven culture, and (3) lead and nurture open data design and architecture establishing conditions for successful technical and analytic innovation.
The Sr. Data Engineer / Data Engineer will develop, maintain, and test data pipelines, application framework, infrastructure for data generation; works closely with Data Scientists to enable their work using modern data architecture and tools.
Mandate covers:
Leveraging new & existing Big Data & Cloud technologies contributing to the innovative design, development and management of data analytics labs supporting to increase knowledge and insight from enterprise data
Perform technical systems and data flow development in a variety of projects for complex front, middle and back office applications, with a focus on the reporting and analytics environment; this environment is built on a Microsoft Azure cloud and is underpinned by a Hortonworks Hadoop stack
Perform technical systems and data flow design for small-to-medium sized projects
Work with multiple project execution and deployment teams (e.g. Development, Business Analysis, Architecture, Release Management, Production Support)
Work closely with the technical leads and architecture teams, and align solutions that meet the departmental architectural vision
Assess the completeness and accuracy of data, identifying gaps in data, provide feedback to business and system owners with guidance and options to obtain missing information
Design, build and implement modern data architectures in development and production environments (data orchestration pipelines, data sourcing, cleansing, augmentation and quality control processes)
Translates business needs into data engineering and architecture solutions
Contributes to overall solution, integration and enterprise architectures
Build and support deploying machine learning models in development and production environments
Provide proactive data ingestion and analysis of large structured and unstructured datasets involving a wide range of systems across Group Functions (i.e., Finance, Treasury, Risk, Human Resources, Brand & Communications)
Evaluating existing and proposed data models and how to best access and query them as well as existing and proposed data interfaces and how to clearly document them, including specification of data flow models, data flow timing, data mapping, and data transformation rules including data validations and controls.
Required Knowledge and Skills:
Demonstrated minimum 5-8 years of professional experience in related industry experience in working in bigdata/data management & understanding big data analytic tooling and environments including a University degree and or Master’s degree in Engineering, Computer Science or equivalent quantitative program
Experience in Big Data, Analytics and Business Intelligence technologies to support design, build and implementation for advanced analytics and business intelligence reporting;
Experience working with Cloudera and/or Hortonworks Hadoop stack
Experience with big data processing frameworks and techniques such as HDFS, MapReduce, Syncsort, Sqoop, Oozie, Storage formats (Avro, Parquet), Stream processing (NiFi, Kafka), etc.
Understanding of relational and warehousing database technology working with Hadoop and other major databases platforms (e.g., Hadoop, Oracle, SQLServer, Teradata, MySQL, or Postgres)
Experience in data technologies and use of data to support software development, advanced analytics and reporting. Focus on Cloud (Azure), Hadoop-based technologies and programming or scripting languages like Java, Scala, Linux, C++, PHP, Ruby Python, R and SAS.
Knowledge regarding different databases such as Hawq/HDB, MongoDB, Cassandra or Hbase.
Working knowledge of modern data streaming using Kafka, Apache Spark and data ingestion frameworks: NiFi, Hive and Pig
Experience writing complex SQL and NoSQL jobs to analyze data in both traditional DBMS (MS-SQL, Oracle) and Big Data environments (i.e., HADOOP, SPARK, or similar open source and commercial technologies)
Knowledge of non-relational (Cassandra, MongoDB) databases preferred
Predictive analytics and machine learning experience (scikit-learn, Tensorflow, MLlib, recommendation systems) preferred
Experience with integrating to back-end/legacy environments
Experience integrating business and technology teams
Knowledge and familiarity with machine learning models application and production pipelines
Collaborative attitude, willingness to work with team members; able to coach, participate in code reviews, share skills and methods
Remains current with emerging technologies, innovations and practices within the data and analytics industry
Strong work ethic, results oriented, and accuracy / attention to detail are critical; ability to work in agile or scrum delivery environments
Exceptional oral, written and interpersonal communication skills with the ability to simplify complex technical concepts into business & value-focused language. A key requirement is to communicate clearly and consistently keeping stakeholders well-informed of progress and challenges
Excellent organizational and time management skills, strong business presence with ability to multi-task and effectively deal with competing priorities. Ability to work with minimal or no supervision while performing duties; has the ability and initiative to organize various functions and be a strong team player.
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance.
This is a full-time permanent role located in Toronto, Ontario.

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of June 30, 2020, we had $1.2 trillion (US$0.9 trillion) in assets under management and administration, and in the previous 12 months we made $30.6 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.
Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially."
Data Engineer,"Vancouver, BC","Amazon Data Services CAN, Inc.",None,Organic,"This position requires a Bachelor's Degree in Computer Science or a related technical field, and 5+ years of relevant employment experience.
At least 3 year experience developing ETL/ ELT solutions within a Data Warehouse environment
At least 3 years experience using SQL, PL/SQL or Shell scripting

We are looking for an outstanding Data/Database Engineer who is data-driven, uncompromisingly detail oriented, smart, efficient, and driven to help our business succeed. You have passion for technology. You are keen to leverage existing skills while trying new approaches. You are not tool-centric; you determine what technology works best for the problem at hand and apply it accordingly. You can explain complex concepts to your non-technical customers in simple terms.

We are moving from traditional database technologies to near real time data processing and advanced reporting services built around natural language processing and machine learning.

You will help us analyze large amounts of data, discover and solve real world problems and build metrics and business cases to help business teams to make decisions. You should be motivated self-starter that can work independently in a fast paced, ambiguous environment.

Oracle and SQL performance tuning
Strong critical thinking and attention to detail
Ability to work and communicate effectively with developers and Business users
Experience in big-data using Hadoop, Hive, and other open-source tools/technologies
Familiar with AWS tools
Ability to work independently with minimum supervision
Experience in designing and building large data warehouse systems
Strong organizational and multitasking skills with ability to balance competing priorities
Good work experience in BI Reporting tools and databases in a business environment.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us ."
GCP Data Assets Engineer,"Toronto, ON",TELUS International,$80 an hour,Organic,"GCP Data Assets Engineer
Context
Data productization is strategic to TELUS, as it enables both insights generation and the creation of AI-based algorithms by the internal TELUS analytics community. Data productization is all about the creation and light-touch governance of reusable data assets that can be democratized across our analytics community, to support real operational improvement and revenue generation use cases.
The successful candidate will build and maintain data assets such as, but is not limited to:
Advanced Queries in Big Query
Customer Journey Tables
Feature Stores for Data Science use
Symantec Layers
NLU/NLI based customer intents and triggers summary tables
*
What does a day in the life of a Data Assets Engineer look like:
Interface with users from the analytics community from across the TELUS to understand use cases and related data needs
Guide the analytics community to existing data assets, at a repository for predefined queries you have built
Expand data assets repository based on use case funnel and your understanding of where our business is growing
Envision, build and operationalize a light weight governance model to enable the analytics community to contribute to a shared ""data-as-a-product"" assets repository
Continuously learn new methods and tools to humanize a culture of being data, rather than, options driven.
*
*
What will help a candidate stick out:
Coding proficiency in SQL and at least one other modern programming language (Python, Java, etc.)
Experience and Advanced expertise in Google Cloud Platform and Big Query a strong asset
Strong desire to work in a cross-functional team environment
Ability to think and solve complex data engineering problems in a maintainable and scalable fashion
Desire to find new and better ways of doing things, generating original and imaginative ideas, products, and/or solutions
*
Job Type: Contract
Pay: From $80.00 per hour
Schedule:
Day shift
Experience:
Data Engineering: 3 years (Required)
Location:
Toronto, ON (Required)
Work remotely:
Yes, temporarily due to COVID-19"
Data Scientist/ Machine learning Engineer,"Whitchurch-Stouffville, ON",IDES CANADA INC,None,Organic,"Scentroid is looking for a Data Scientist/Machine Learning Engineer.
Scentroid is a high-tech company developing innovative technologies for air quality monitoring market. We manufacture all instruments in-house and export to over 42 countries. Our clients include Google, NASA, UC Berkeley, John Hopkins, Health Canada, Metro Vancouver, and many more. Our strong technical expertise in all aspects of instrumentation including mechanical, electrical, and software engineering provides the fuel for our obsession of research and development. For more information visit www.scentroid.com
Job Description:
As a ML Engineer you will be part of Scentroid’s advance R&D team developing intelligent air quality monitoring instruments. You will help develop new pattern recognition and machine learning tools for a variety of applications such as event detection, automatic calibration, sensor fusion, source identification, and pollution forecasting. You will be dealing with vast amount of data collected using mobile and stationary sensing platforms.
Required Skills
Ideal candidate has education background and relevant work experience demonstrates a mixture of math, statistics, and data engineering. Additional skills/experience include:
2+ years experience working with machine learning algorithms using large scale data processing tools
Knowledge about machine learning theory and algorithms: SVM, random forest, gradient boosting methods, graphical models, Bayesian methods, etc.
Familiarity with air dispersion modeling is an asset
demonstrable creativity in temporal and geospatial data representation
Proficiency in Python is a required
Proficiency in R or MATLAB is preferred
Able to extract data on your own and prepare a large data set to train a machine learning algorithm
Comfortable working with data that is sometimes incomplete, or messy, or both
Education Requirements
· Bachelor’s in Engineering, Math, Statistics, or Computer Science is required
· Master’s in related field is preferred.
What You Should Expect
· A growing fast-pace organization;
· cutting edge development and interaction with some of the world's most prestigious companies.
· Full benefit package.
· This position is full time with 3 months of probation period.
· Annual Salary range: Negotiable
Benefits:
Work from home opportunities
Extended health care
Vacation & paid time off
Company events & social hours
Casual dress
On-site parking
Education reimbursement
Job Types: Full-time, Contract, Permanent
Schedule:
Monday to Friday
Experience:
Machine Learning: 2 years (Preferred)
Education:
Bachelor's Degree (Preferred)
Work remotely:
Yes, temporarily due to COVID-19"
Azure Data Engineer,"Brampton, ON",Tech Mahindra,None,Organic,"Title: Azure Data Engineer
Location: Brampton, ON
Type: Fulltime
Must have skills
Azure Data Factory, Databricks, SCALA , SPARK , KAFKA with good understanding of OOPS concepts would be the key word for searching.
Description:
Minimum 5 years of hands on experience in Azure Data Factory, ADLS and Blob Storage (analyzing log files, IoT data, click streams, large datasets)
Minimum 5 years of experience in working on Azure Event Hub and managing real time data/event ingestion service; along with Azure Stream Analytics performing analytics on data in flight
Must be proficient with creating multiple complex Azure Data Factory pipelines and activities using both Azure and On-Prem data stores for full and incremental data loads to cloud
Hands on implementation of data solutions that use the following Azure services: Azure Data Factory, Azure Cosmos DB, Azure SQL Database, Azure Synapse Analytics, Azure Data Lake Storage, Blob, Azure Databricks, Azure Stream Analytics.
Hands on experience with Databricks, SCALA , SPARK , KAFKA with good understanding of OOPS concepts
Implement security requirements and implement data retention policies.
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Experience:
azure: 8 years (Preferred)
data engineer: 5 years (Preferred)"
Data Engineer,"Toronto, ON",Open Farm,None,Organic,"Based in Toronto, Open Farm is one of the fastest-growing CPG companies in North America. We are transforming the way people feed their pets, with a focus on producing premium, healthy food and treats, all ethically sourced from farm-to-bowl. We believe the best foods are made with consciously sourced, top-quality ingredients from farmers and fisheries who believe in doing good for all animals and the environment.

We are looking for an experienced Data Engineer to join our team in Toronto (HQ) or remote during a very exciting time as a company. We are at a point where we are scaling quickly (as is our data), and we need to build a data infrastructure that will scale with us. This is an incredible opportunity to take a leadership position in aggregating multiple data streams, providing actionable insights and recommendations, and facilitating a data-driven culture across teams. In this role, you will report directly to the VP of E-Commerce, work collaboratively with cross-functional teams, and leverage agency resources. You will play a key role in impacting our data and analytics strategy today and in the future.

The Role:
Data Aggregation: Develop an in-depth understanding of our business, our strategy, and where and how we collect data today. This will require working with cross-functional stakeholders on the e-commerce team but also Finance, Sales, Marketing, and Operations.
Data Architecture: Identify and select data warehousing, ETL, and BI solutions that meet our business needs and that can flex with added complexity and as our business scales
Data Modeling & Analytics: Implement processes for SQL-based data modeling that reflect the business rules underlying our analytics. Ensure version-control, develop documentation, and transparency.
Data Integrity: Maintain tools and systems that ingest, transform, organize and expose data across teams, ensuring data accuracy and security.
Automation & Reporting: Architect data models that are reusable and extendable, and design and optimize queries that deliver business views and reporting for insights and decision making.
You Have:
4-6 years' experience in Data Engineering, worked at a start-up, built a Data Infrastructure and Analytics solution from the ground up, and can bring a tried-and-true playbook to our team
Excellent quantitative, analytical, and problem-solving skills. Experience working with data from multiple sources and systems across multiple functions.
Built a modeling infrastructure and DWH solution, such as Redshift or Snowflake, set up ETL processes tools, such as Stitch and Fivetran, and integrated with BI tools, such as Looker or Mode, and implemented data modeling tools, such as DBT
A strong understanding and perspective on the pros and cons of using different tools, and the ability to communicate business cases to management

You Are:
Excited by the idea of working at a high-growth consumer brand, and thrive in a fast-paced, entrepreneurial environment
Curious and constantly learning about best practices pertaining to big data, data warehousing, information security, and the latest tools to make data analytics more accurate, seamless, and accessible across the organization.
Eager to roll up your sleeves and work with a passionate and growing team

What We Offer
An opportunity to grow with a dynamic company
Central Toronto location (Annex neighborhood), currently working from home due to COVID, and open to remote applicants
Great office culture (with lots of adorable pooches!)
Casual dress code
Full suite of health and dental benefits

Open Farm values diversity in its workforce and encourages applications from all qualified individuals. Applicants requiring a disability-related accommodation at any stage of Open Farm's recruitment process should contact careers@openfarmpet.com. As required by legislation, Open Farm will consult with applications requesting such an accommodation to ensure that Open Farm's recruitment process takes into account their accessibility needs."
Python Data Engineer,"Halifax, NS",AXIS Insurance,None,Organic,"This is your opportunity to join AXIS Capital – a trusted global provider of specialty lines insurance and reinsurance. We stand apart for our outstanding client service, intelligent risk taking and superior risk adjusted returns for our shareholders. We also proudly maintain an entrepreneurial, disciplined and ethical corporate culture. As a member of AXIS, you join a team that is among the best in the industry.
The primary role of the Python Data Engineer is to design, monitor, and maintain the systems populating the Axis Data Platform in the Azure Cloud.
The Python Data Engineer will work with internal customers to efficiently extract, transform, and load data fulfilling analytic, reporting, and data science requirements.
Responsibilities
Collaborate with Axis colleagues at all levels and deliver robust cloud-based data systems matching analytic needs.
Collaborate with Architecture and Database Administrators on design of data systems ensuring accuracy, correctness, and efficiency.
Collaborate with DevOps teams to ensure system reliability.
Responsible for developing and maintaining formal descriptions of data flow systems populating target databases, including data flow diagrams, metadata documentation, and lineage artifacts.
To liaise with colleagues and provide Data Transformation Expertise when needed.
Establish and document standards for DevOps relevant to the running of applications.
Skills & Qualifications
5+ years experience developing in Python data processing systems.
Sound understand of python packaging and pipenv, venv, poetry, or conda.
Cloud compute technologies such as Azure Functions, AWS Lambda, Kubernetes, or Docker.
Excels at SQL transformation and extraction; Microsoft SQL Server.
Cloud file system technologies such as Azure ADLS, Amazon S3, or HDFS.
Excellent grasp of data structures and algorithms.
Excellent verbal, written, and interpersonal skills.
Bachelor’s degree in IT / Computer Science field; advanced degree preferred.
Technical curiosity and enthusiasm.
Preferred Skills & Qualifications
Flow Based Programming and/or Functional Programming.
Additional programming languages such as R, Clojure, Go, FSharp, or Julia.
Additional Database Systems such as Azure Synapse, Teradata, Netezza, Postgres, Oracle.
Web Client/Server technologies such as Swagger, Rest, SOAP, GraphQL.
Familiarity to ETL tools such ADF, SSIS, Informatica, Data Stage, Ab Initio, or Talend.
Exposure to data science ecosystems and toolsets; NLTK, scikit-learn, tensorflow, etc.
Queue based data systems such as Azure Event Stream, Kafka, RabbitMQ, etc.
P & C Insurance industry experience preferred."
Senior Data Engineer,"Halifax, NS",Kinduct Technologies,None,Organic,"Title: Senior Data Engineer
Location: Halifax, NS, Canada
Term: Full-time
Compensation: Competitive Salary and Benefits Plan
Company Overview
Kinduct Technologies (www.kinduct.com) is a leading data and analytics software provider that is changing the way information shapes human performance. Kinduct's highly secure, cloud-based platform allows sports, health and wellness organizations to spend less time managing their data and more time using it to inform decisions, promote constant improvements and produce exceptional results. Kinduct is the software provider of choice for many world-leading professional and elite sports organizations across the NFL, NBA, MLB, NHL, MLS, and other professional and elite leagues. A number of Kinduct clients are world champions in their sport. Kinduct's investors include Intel Capital, CFFI Ventures Inc., and Elysian Park Ventures.
A year in the life of a Kinducterdactyl (what sets us apart):
Flexible working hours in an innovative and exciting industry
Health and Wellness Allowance
Parking or Transit Stipend
Extra Holiday Days
Never less than 15 days of vacation, even in your first year
Ample paid sick days
Health, Dental, Vision and complementary health - shared benefits coverage
Located at beautiful Purdy's Wharf with access to many convenient amenities
Two blow out company events per year
A games room with a pool table and ping pong gear
A collaboration and relation space with three TV's to enjoy the latest game as you relax with your view of the harbour
Many ""Fun Factory"" planned ongoing initiatives to pump up the Fun Factor
A collaborative, diverse and team focused environment that supports your growth as a professional all while ensuring we laugh the way!
Your Opportunity
We are looking for experienced individuals with deep knowledge of SQL, Stream processing, Job Orchestration, Data Modelling, and Data Warehousing. Ideal candidates are self-motivated engineers with a passion for both business and technology innovation, more importantly they quickly adapt with changing technologies. We value people who are passionate about data quality and have an eye for improving data systems. We currently work with Python, Scala, AWS Kinesis, AWS Lambda, MySQL, and Snowflake. Your primary focus will be on developing easy to scale data pipelines, robust data models, and data warehouses while following data engineering best practices. You will 1.) ensure that these pipelines robust, easy to scale, and easy to maintain. 2.) Be part of the team building the different layers of the data infrastructure. 3) Have a commitment to collaborative problem solving, sophisticated design, and code quality.
Responsibilities:
Developing new ways to store and move data
Building reusable data pipelines and transformation libraries for future use
Translating data models and their access into high quality data stores
Optimizing system components for maximum performance and scalability across a vast array of environment
Skills:
Commanding grasp of Python or Scala, SQL, and familiarity with newer cloud based data engineering tools
Strong Computer Science fundamentals
Deep understanding of data Streaming and its core principles. Experience with Kafka or Apache Beams is a plus
Experience with modern data warehousing and analysis tools and technologies. Experience with Snowflake, BigQuery, and Tableau is a plus
Demonstrated job profiling, troubleshooting and optimization experience.
Familiarity with RESTful APIs
Knowledge of modern authentication and authorization mechanisms such as OAuth, JWTs, etc.
Familiarity with modern back-end CI/CD pipelines and tools such as SBT, Pipenv, etc.
Ability to understand business requirements and translate them into technical requirements
Behavioural Competencies:
Take initiative with seeking out tasks, communication, and updating status;
Ability to prioritize, multi-task, and perform effectively under pressure;
Be willing to ask for assistance when roadblocks impede progress;
Ability to define requirements and manage tasks with multiple deadlines under minimal supervision ;
Display reliability, integrity and trustworthiness;
Demonstrate a willingness to learn new technologies during a product life cycle;
Possess strong analytical and problem solving skills;
Displays creative and innovative thinking;
Acceptance of differences in team members and a strong commitment to teamwork, cooperation and healthy behaviours.
What Sets You Apart:
Strong motivation to drive impact by making product or infrastructure improvements
Proactiveness, clear communicator of thoughts and ideas and fast learning
Interest in working in a very cross-functional team that touches many of the core systems at Kinduct
Passionate about Data Governance
Curious about new technology and tools
Seek out others' opinions to further yours

If you are looking for a new and exciting challenge, and the opportunity to work with some of the most successful professional sport franchises in the world, then look no further. We at Kinduct are looking forward to hearing from you, and to welcoming you to our ever growing family of Kinducterdactyls!"
Senior Data Engineer - Analytics,"Waterloo, ON",SkyWatch,None,Organic,"At SkyWatch, we believe that everyone on the planet should benefit from the thousands of satellites that orbit the earth. We are building a platform to put the world’s Earth observation data in one place — a simple API any developer can use. Our team of extraordinarily talented people has built a cutting-edge microservices platform on a serverless stack, and we’re growing fast.
OUR TEAM
Let’s face it, the best jobs boil down to a handful of things — getting the chance to work with amazing people in a great culture with leaders who have your back while pursuing a vision that you can get passionate about.

Our team consists of NASA award-winning developers, Earth observation scientists, big data specialists, and unicorn startup veterans from places like Facebook, IBM, and Berkeley and who come from five of the seven continents. Our leadership team includes the Chair of Canada’s first Space Accelerator for startups, an Earth observation veteran who has led multiple companies through IPOs, a planetary robotics graduate from the International Space University, and a petabyte-scale data leader with patents in location AI and contextual analytics. We are Techstars ‘16 NYC and Google for Entrepreneurs alumni. We have a unique blend of space/satellite authorities and consumer application veterans. And we are just getting started!

OUR CULTURE
Culture is the personality of a company. At SkyWatch, our culture is incredibly important to us, and we practice it every day. As a company, we value innovation, dependability, and accessibility. In our product, we value simplicity, affordability, and informability.

In engineering, we specifically value our team’s happiness over unrealistic product planning (People First), flexibility over a rigorous top-down process (Freedom & Autonomy), and improving our overall system over dwelling on unintended mistakes or bugs (Just Culture).

Our mission and values are on the walls of our office. We don’t just talk the talk, we walk the walk.

OUR PRODUCT
Every day, hundreds of trillions of pixels of our planet are captured from space. With new applications for this data such as precision farming, construction planning, disaster relief operations, and retail revenue prediction (to name a few), the demand for satellite imagery is growing. However, the current process for obtaining satellite imagery is very time-consuming and tedious.

While the number of sensors observing our planet is growing exponentially, we are seeing each of these instruments creating their own unique dataset. Getting access to many of these catalogs is expensive and time-consuming. And once access is achieved, is it difficult to integrate data from multiple sources into one system. And we all know the most impactful analytics happen when large volumes of data are normalized and can be easily incorporated into powerful models.

Using our team's experience in building NASA-award winning software, we are building the SkyWatch API, a simple way to discover and access the world’s remote sensing data. We are making it easy for organizations and developers to monitor the progress of crops, predict the markets, track ships and airplanes, measure global warming, or create other game-changing applications. We do the data work behind the scenes so developers are free to build the apps that change the world.

Sound exciting? You’re right — it is.

ABOUT THE ROLE
Where do you fit in as a Senior Data Engineer? Great question.

ABOUT YOU
You have the ability to solve problems, not just complete tasks. You are an artist and a craftsperson, not an assembly line worker. You empathize with those who use what you’ve built. You want to solve their problems.

You have strong technical chops but can also describe technical things to non-technical people. You can tell a ten-year-old how an API works and not have them glaze over with boredom. Or a CEO. Or your significant other. OK, maybe not your significant other.

You have a diverse background. Volunteer as a Big Sister? Founded a Meetup? Love to paint? Play a sport? We love diversity and value how much it adds to our team.

You have a thirst for learning. We learn something every day. We find it intoxicating. If you’d rather master one thing and do that every day, this probably isn’t the role for you.

You have thoughtful opinions that you feel comfortable sharing. We have a culture that makes sharing opinions safe. If everyone in a meeting always thinks the same way, we have too many people in that meeting.

You have the ability to give more than you take. And with an incredible team, we want you to make it even more incredible!

YOUR TECHNICAL ARSENAL
You understand the importance of code reviews and writing clean, maintainable code
You believe in automated testing and can explain why it’s so important
You’ve built software of some kind to solve a problem in your life outside of work.
You have experience with designing and building data pipelines, both instrumenting code and building out the backend infrastructure
You have a passion for uncovering what user behaviour is really going on and determining the signal through the noise
You love working with other engineers, product management, and others throughout the organization on highly impactful projects

YOUR TYPICAL DAY
9:00 am — you roll in, catch up on Slack, email, system status, and important metrics.

10:00 am — you participate in our daily stand up, sharing what you did yesterday, what you plan on doing today, and if there are any roadblocks.

10:15 am — you dig into building our newest microservice, validating the problem it will solve, and designing inputs and outputs.

12:00 pm — you hit an awesome take out place in Uptown Waterloo and chow with the team.

1:00 pm — you put on the headphones and get dialed in to build that microservice.

3:30 pm — happy with the code, you deploy to dev and if tests pass, staging.

4:00 pm — you attend a design session for a new feature, contributing thoughts, opinions, and experience to the discussion.

5:00 pm — you wrap up unit tests, prep for deploy tomorrow, and pack up for the day.

A FEW MORE THINGS TO KNOW
We offer a solid compensation package including a competitive salary, stock options, and a comprehensive benefits plan for you, and your family.
We have flexible work hours and flexible remote working options.
In normal times, our HQ is in Uptown Waterloo, at the Communitech Data Hub, where we are surrounded by other awesome startups focused on data, AI, machine learning, and IOT."
Ingénieur de données / Data Engineer,"Montréal, QC",Plusgrade,None,Organic,"Plusgrade fournit des solutions innovantes génératrices de revenus pour l’industrie mondiale du voyage. En tant que leader reconnu dans un segment clé des revenus auxiliaires et de l’espace de merchandisage, Plusgrade aide les fournisseurs de voyages du monde entier à gérer, optimiser et capturer les revenus à forte marge générés par les mises à niveau, les stocks invendus et d’autres services haut de gamme.
Basée à Montréal, au Canada, et ses bureaux à New York et à Singapour, Plusgrade a créé des milliards de dollars de nouvelles possibilités de revenus pour plus de 70 compagnies aériennes et lignes de croisière qu’elle dessert dans 50 pays. Récompensée par le prestigieux “Deloitte Technology Leadership Award” en tant que leader mondial dans son secteur, Plusgrade est également régulièrement reconnue comme l’une des entreprises technologiques les plus florissantes en Amérique du Nord dans le cadre de l’édition annuelle de “Deloitte Enterprise Fast 15™, Technology Fast 50™ and Fast 500™ awards.”.

Le rôle:
Nous sommes à la recherche d’un ingénieur de données expérimenté qui contribuera à créer avec nous d’excellents produits pour les compagnies aériennes, les croisiéristes et les voyageurs. En tant que membre essentiel de notre équipe de données analytiques, vous travaillerez sur une plateforme évolutive intégrée en temps réel, vous collaborerez avec l'équipe de Produit pour vous assurer que les bons tableaux de bord sont créés et serez responsable d’un pipeline de données bien testé et maintenu. En étroite collaboration avec les autres équipes d’ingénierie, l'ingénieur de données mettra en place des infrastructures performantes pour traiter les données provenant de sources multiples.

Nous croyons fermement que de nouveaux défis nous attendent dans l’industrie du voyage (et au-delà), si vous êtes heureux de relever ces défis avec nous, nous voulons vous rencontrer!

Nos ingénieurs logiciels sont full stack. Nous attachons une grande importance à fournir des produits de haute qualité et testés rapidement et régulièrement. Au cours d’une semaine de travail typique, vous aurez l’occasion de développer de nouvelles fonctionnalités avec le soutien d’une équipe multidisciplinaire et fonctionnelle composée de chefs de produit, de concepteurs d’automatisation de test et d’ingénieurs Frontend.

Vos Responsabilités:
Construire et maintenir un pipeline de données évolutif qui sert de source de vérité et permet à l’entreprise de prendre les meilleures décisions d’affaires pour la croissance dans tous les secteurs verticaux,
Exploitation des données dans le traitement par lots et le traitement en continu à l’aide d’outils Big Data (Redshift, AWS Kinesis, AWS Lambda, DynamoDB, base de données SQL, etc.),
Modélisation et visualisation des données - Utiliser Looker pour créer des tableaux de bord percutants et magnifiquement conçus,
Collaborer avec les équipes Produits et Commercial et les aider à comprendre leurs besoins en matière de données,
Collaborer avec plusieurs équipes d’ingénierie (Dev, QA, Platform, etc.) pour s’assurer que les données nécessaires sont saisies, utilisables et de haute qualité,
Automatiser la surveillance à l’aide de données en temps réel pour aider à identifier les problèmes potentiels dans les indicateurs clés de performance clés de l’entreprise,
Diffusion de la culture des données - Veiller à ce que les différents départements utilisent les données et y réfléchissent dans le bon état d’esprit,
Construire, entretenir, tester et déployer notre infrastructure infonuagique, et soutenir notre pipeline de données à l’aide de Terraform.

Qualifications:
BAC en informatique, en génie logiciel/informatique ou équivalent, ou expérience équivalente,
1 à 2 ans d’expérience en ingénierie des données,
Vous avez travaillé en équipe et livré un produit dont vous êtes fier,
Capacité de se présenter devant un auditoire et de contribuer à la prise de décisions techniques,
Écrire du code testable, compréhensible et durable,
Soyez un excellent communicateur: Expliquer des concepts techniques complexes aux concepteurs ou aux ingénieurs de soutien n’est pas un problème pour vous,
Connaissance des principes fondamentaux de la TI : structure des données, algorithmes, langages de programmation, systèmes distribués et recherche d’information,
Expérience de la conception et de la mise en œuvre de logiciels en Java et Python, et du déploiement dans le Cloud (de préférence AWS),
Expérience de SQL,
Connaissance de l’exploration de données, de l’apprentissage automatique ou de l’apprentissage en profondeur appréciée.

Points bonus si…
Vous connaissez les pratiques exemplaires en matière d’ingénierie, comme les CI et les CD, l’infrastructure et le code.
Vous avez de l’expérience pratique avec CI et CD.
Vous avez de l’expérience dans l’industrie du transport aérien ou vous connaissez bien la technologie de voyage.

Bénéfices et avantages:
Nous voulons que tout le monde reste en bonne santé, se perfectionne souvent et se sente appuyé!
REER de contrepartie
Plans de santé complets
Crédit de surclassement annuel
Crédit annuel pour bien-être physique
Crédit annuel pour mieux-être mental
Apprentissage et perfectionnement
Collations saines (et moins saines)

About Plusgrade
Plusgrade provides innovative revenue generating solutions for the global travel industry. As the recognized leader in a key segment of the ancillary revenue and merchandising space, Plusgrade helps travel providers worldwide manage, optimize and capture high-margin revenue generated from upgrades, unsold inventory and other premium services.

Headquartered in Montreal, Canada, with offices in New York and Singapore, Plusgrade has created billions of dollars of new revenue opportunities for the more than 70 premier airlines and cruise lines it serves across 50 countries. Honored with the prestigious Deloitte Technology Leadership award as a global leader in its sector, Plusgrade is also consistently recognized as one of North America's fastest growing technology companies in the annual Deloitte Enterprise Fast 15™, Technology Fast 50™ and Fast 500™ awards.

The Role:
We are looking for a Data Engineer who will work with us to create great products for airlines, cruise lines and travelers. As a vital member of our analytical data team, you will work on a scalable platform that’s built in real time, collaborate with Product to ensure that the right dashboards are built and be responsible for a well tested and maintained data pipeline. In close collaboration with the other engineering teams, the Data Engineer will set up high performance infrastructures to process data from multiple sources.

We firmly believe that new challenges await us in the travel industry (and beyond), if you are excited to take on these challenges with us, we want to meet you!

Our Software Engineers are full stack. We attach great importance to delivering high quality, tested products quickly and regularly. During a typical work week, you will have the opportunity to develop new features with the support of a multidisciplinary and functional team made up of Product Managers, Test Automation Designers and Frontend Engineers.

What you’ll be doing:
Building and maintaining a scalable data pipeline that serves as the source of truth and allows the company to take the best business decisions for growth in all verticals
Leveraging data in both batch processing and streaming processing using big data tools (Redshift, AWS Kinesis, AWS Lambda, DynamoDB, SQL database, and etc)
Data modeling and visualization - Using Looker to create impactful and beautifully designed dashboards
Collaborating with Product and Commercial and helping them to understand their data requirements and needs
Collaborating with multiple engineering teams (Dev, QA, Platform, etc) to ensure necessary data is captured, usable and high quality
Automating monitoring using real time data to help identify potential issues in critical business KPIs
Spreading the data culture - Ensuring that departments are using and thinking of data in the right mindset
Building, maintaining, testing, and deploying to our cloud based infrastructure, and supporting our data pipeline using Terraform

Qualifications:
Bachelor’s degree in Computer Science, Software / Computer Engineering or equivalent, or equivalent experience
1-2 years of experience in data engineering
Worked in a team and delivered a product of which you are proud of
Ability to present in front of an audience and contribute to technical decision making
Write testable, understandable and durable code
Be an excellent communicator: Explaining complex technical concepts to Support Designers or Engineers is not a problem for you
Knowledge of IT fundamentals: Data structure, algorithms, programming languages, distributed systems, and information retrieval
Experience in the design and implementation of software in Java and Python, and deployment in the Cloud (preferably AWS)
Experience with SQL
Knowledge of data mining, machine learning or deep learning is a plus

Bonus points if:
You’re familiar with engineering best practices such as CI and CD, infrastructure and code
You have practical experience with CI and CD
You have experience in the airline industry or are familiar with travel technology

Benefits and Perks:
We want everyone to stay healthy, upgrade often and feel supported!
Matching RRSP
Comprehensive health plans
Annual upgrade credit
Annual physical wellness credit
Annual mental wellness credit
Employee Referral Program
Learning and development
Healthy (and not so healthy) snacks"
Ingénieur de données /Data Engineer,"Saint-Laurent, QC",CAE Inc.,None,Organic,"Role and Responsibilities
(English will follow)
Lorsque vous prenez l’avion, peu importe la destination, il y a de fortes chances que le pilote ait été formé par CAE. Nous sommes le partenaire de choix en formation partout dans le monde. Le point focal étant les clients, l’équipe Accélérateur numérique s’engage à rehausser l’expérience de formation afin de s’assurer que les pilotes soient les meilleurs possible.
Joignez-vous au moteur de changement à CAE - notre prochain horizon de croissance passe avant tout par l’innovation numérique afin d’appuyer la réussite de nos clients.
Voici quelques raisons pour lesquelles les membres de notre personnel aiment travailler au sein de notre entreprise :
Travail significatif qui favorise le perfectionnement professionnel.
Possibilité d’entrer dans l’industrie technologique et de s’y épanouir.
Environnement de travail axé sur la collaboration.
Équipe de haut niveau.
Ce que nous avons à offrir :
Régime d’assurance collective souple
Régime de retraite à prestations déterminées
Régime d’achat d’actions du personnel
Régime enregistré d’épargne-retraite collectif
Programme pour le bien‑être physique
Programme d’aide aux employés
Prestations de maternité complémentaires
Horaire de travail variable
« Vendredis Californie » tout au long de l’année
Votre mission :
À titre d’ingénieur des données, vous serez appelé à transformer des données dans un format pouvant être exigeant sur un certain nombre de plans pour d’autres intervenants en analyse. Cela sera accompli principalement par l’élaboration, l’entretien et la mise à l’essai d’infrastructures destinés à la production de données. Cette équipe jouera également un rôle important dans la promotion de solutions d’architecture pour des projets de science des données et de modélisation avancée.
Nous sommes à la recherche d’une personne capable de réaliser les tâches suivantes :
Créer et maintenir une architecture de pipeline de données optimale et évolutive.
Assembler des ensembles de données complexes qui respectent les exigences opérationnelles fonctionnelles et non fonctionnelles.
Concevoir l’infrastructure requise pour l’extraction, la transformation et le chargement de données optimaux à partir d’une grande variété de sources de données et de technologies de « données massives ».
Définir, concevoir et mettre en œuvre des améliorations de processus internes : l’automatisation des processus manuels, l’optimisation de la transmission de données, la nouvelle conception de l’infrastructure pour une plus grande évolutivité, etc.
Mettre au point des outils d’analyse qui utilisent le pipeline de données pour fournir des perspectives applicables en matière d’acquisition de clients, d’efficacité opérationnelle et d’autres mesures clés du rendement opérationnel.
Travailler avec des intervenants, y compris les équipes de la direction, de l’expérience client et de la conception pour les assister dans la résolution de questions techniques liées aux données et le soutien de leurs besoins en infrastructure.
Maintenir les données séparées et en sécurité à travers les frontières nationales par l’entremise de plusieurs centres de données.
Travailler avec des experts en données et en analyse pour parvenir à une meilleure fonctionnalité de nos systèmes de données.
Voici les caractéristiques de notre candidat(e) idéal(e) :
Baccalauréat en informatique, en ingénierie ou un domaine connexe
Au moins trois (3) ans d’expérience dans l’industrie en matière de travail avec des données, de code, de création de scripts (Python/Java/Scala/SQL/JS/Bash), de conception, et de mise à l’essai
Au moins trois (3) ans d’expérience en matière d’élaboration et d’administration de gros systèmes de données
Solides connaissances des principes fondamentaux du soutien à la clientèle en matière d’algorithmes et de structures de données.
Expérience en soutien et en travail avec des équipes interfonctionnelles dans un environnement dynamique Expérience en utilisation d’outils de traitement de données massives : Hadoop, Spark, Kafka, etc.
Expérience en utilisation de bases de données relationnelles SQL et NoSQL, y compris Postgres et Cassandra.
Expérience en utilisation de pipelines de données et d’outils de gestion du flux de travail : Azkaban, Luigi, Airflow, etc.
Expérience en utilisation de services infonuagiques AWS : EC2, EMR, RDS, Redshift
Expérience en utilisation de services infonuagiques Microsoft : Azure, Databrick, etc.
Expérience en utilisation de systèmes de traitement de flux : Storm, Spark-Streaming, etc.
Expérience en utilisation de langages de script orientés objet et à fonction d’objet : Python, Java, C++, Scala, etc.
Volonté de participer à tous les niveaux de l’exécution des travaux liés à un projet, au besoin
Excellentes aptitudes pour la communication verbale et écrite, en français et en anglais
Pour en savoir plus, cliquez sur le lien ci-joint : https://www.youtube.com/watch?v=DuWzMIEZ_9I&list=PL20BE384270BA6C02&index=2&t=0s
****************************************************************************
If you’ve taken a plane to any destination in the world, chances are, your pilot was trained by CAE. Our company is the worldwide training partner of choice, and with good reason. With its strong customer focus, the Digital Accelerator team is dedicated to elevating the training experience to make pilots the best they can be.
Join the engine that is changing CAE, pointing towards the next horizon of growth through digital innovations to support our customers in their success.
Here are the reasons why folks love working here!
Meaningful work that drives professional development
Ability to enter and grow within the technology industry
Working in a collaborative environment
Being part of a high performance team
What we have to offer:
Flexible Group Insurance Plan
Defined Benefits Retirement Plan
Employee Stock Purchase Plan
Group Registered Retirement Savings Plan (RRSP)
Physical Wellness Plan
Employee Assistance Plan
Supplementary Maternity Plan
Flextime
California Fridays all year
Your mission:
As a Data Engineer you will be asked to transform data into a format that can be consuming for other analytics stakeholders. This should be accomplished mainly through developing, maintenance and testing infrastructure for data generation. You will also play an instrumental role enabling architecture solutions for Data Science and advance modelling projects.
We are looking for people who:
Create and maintain optimal and scalable data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources and ‘big data’ technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, CX and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across national boundaries through multiple data centers.
Work with data and analytics experts to strive for greater functionality in our data systems.
As our ideal candidate you will also have:
Bachelor's degree in Computer Science, Engineering, or related field
A minimum of 3 years industry experience working with data, coding and scripting (Python/Java/Scala/SQL/JS/Bash), design and testing
A minimum of 3 years experience developing and administering large data systems
Solid knowledge of CS fundamentals in algorithms and data structures
Experience supporting and working with cross-functional teams in a dynamic environment. Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with Microsoft cloud services: Azure, Databrick, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Willingness to participate in all levels of project work when necessary
Excellent English and French written and verbal communication skills.
To learn more, click on the attached link: https://www.youtube.com/watch?v=gQ8DRnPLH9s&list=PL783EADF99D507DC1
Position Type
Regular
CAE thanks all applicants for their interest. However, only those whose background and experience match the requirements of the role will be contacted.
Equal Employment Opportunity
At CAE, everyone is welcome to contribute to our success. With no exception.
As captured in our overarching value ""One CAE"", we’re proud to work as one passionate, boundaryless and inclusive team.
At CAE, all employees are welcome regardless of race, nationality, colour, religion, sex, gender identity or expression, sexual orientation, disability or age.
The masculine form may be used in this job description solely for ease of reading, but refers to men, women and the gender diverse."
Sr. Data Engineer,"Toronto, ON",Datanaya Inc.,None,Organic,"Overview
With 2021 approaching, we are ramping up for our projects that are in the pipeline. This is an open call for all Sr. Data Engineers! Please check out our website so you can get familiar with our services, solutions, partners and clients.
Next Steps
1. Your resume will be reviewed.
2. If we feel that you are a good fit and match the qualifications below, a phone call will be set up to discuss potential projects to gain a mutual understanding and interest.
3. We will provide you with opportunities to upskill yourself through Dremio, DataStax certifications and more!
Qualifications
Strong experience with Cloud providers ie: AWS, Azure.
Hands-on experience with at least 2 of the following: AWS Glue/EMR or Kinesis/Kafka or ELK stack or Lambdas.
Experience in designing data lakes - from structuring to managing and optimization,
Knowledge of data storage formats (Parquet, ORC, AVRO, etc.),
Experience with Python and Linux and shell scripting,
Familiarity with relational databases like Oracle, PostgreSQL, MySQL,
Advanced SQL writing skills.
Airflow / Luigi
MongoDB / ElasticSearch / Cassandra
Kunernetes
Contract length: 3-6 months
Part-time hours: 20-37.5 per week
Job Types: Full-time, Part-time, Temporary, Contract
Schedule:
Day shift
Monday to Friday
Location:
Toronto, ON (Preferred)
Work remotely:
Yes, temporarily due to COVID-19"
Senior Data Engineer - Geo-Analytics,"Montréal, QC",TELUS,None,Organic,"Create better business outcomes with your analytical acumen and creativity.

Join our team

The Data Strategy & Enablement team is on a continuous journey towards helping TELUS become a world-class leader in data solutions. We are doing so by delivering data analytics capabilities built upon unified scalable platforms, advanced AI tooling and high quality data. We have a data-product, data-platform oriented culture, and we are always keeping an eye on the horizon, preparing for the next big thing.

We are looking for a technical expert that brings world-class expertise in Artificial Intelligence and Geolocation to join our AI and Geo-Analytics Center of expertise spanning all of TELUS’ brands to provide guidance, automation and insights through collaboration with cross-functional teams to evolve the way they do business in this data insights era.

Together, we will develop and execute strategic programs that will enhance the experience for TELUS subscribers, including improving the robustness and reliability of our networks.

Always wanted to work with a team of innovators and be part of a culture that embraces creativity and collaboration? If so, we’d love to talk with you!

Here’s the impact you’ll make and what we’ll accomplish together

As our Senior Data Engineer – Geo-Analytics, you will be key in supporting the enterprise Geo-Analytics platform and delivering highly accurate and reliable geolocation information to business units all while building strong data pipelines and supporting the data science life cycle.

You will combine your passion for data and technical skills to deliver on automation, analysis and insights but also help discover how the key performance indicators we track ladder up to the customer and business outcomes we are driving.

You will be creating scripts and tools to enhance and maintain critical insights drawn from raw data, develop dashboard reporting using advanced analytics tools and will collaborate with internal business partners, continuously delivering valuable insights to help the business units achieve their goals.

You will drive process improvement through automation and learn to apply AI, and Machine Learning to help our stakeholders unlock the true potential of their data.

Here's how
Enhance and automate our Geographic Information Systems’ analysis providing new levels of Geo-Analytics insights
Provide, Robotic Process Automation solutions for the automation of repetitive tasks
Build appealing and dynamic data visualized reports
Build strong ETL data pipelines that support our overall Data Science growth
Collaborate with our Data Scientists on deploying live machine learning models
Create and maintain various containers that support the team’s needs
Qualifications
You're the missing piece of the puzzle
You are recognized for your keen eye and ability to identify and act upon potential technical improvements
You are sought out for your strong knowledge of data analysis technology ecosystems
You have solid development experience with Python and are comfortable using various data manipulation libraries such Panda, GeoPanda and Numpy
You are a database expert, especially with SQL, noSQL, Hadoop, Oracle and MongoDB
You are the go-to person for your understanding of Geographic Information Systems and ETL processes
You are a cloud, microservices and Software as a Service approaches savant
You are respected for your ability to articulate and map an end-to-end customer experiences
You are commended for your clear communication and cross-functional collaboration skills at all levels of the organization
Great-to-haves
Post graduate education in Software development, Cloud DevOps or Advanced Analytics
GCP certification"
BI Data modeler/engineer,"Bruxelles, MB",Acensi SAS,None,Organic,"Description de l'offre
In the data warehouse team, we are working in the fascinating world of data every day. In order to be fully prepared for tomorrow’s challenges, we have put in place a new platform, new tools and new detailed and aggregated data for our business clients to support them in their analysis. We work closely with the business to analyse and model data.
 Experience in data modelling and functional analysis
 Experience in star scheme modelling through projects
 Good knowledge of insurance processes and of BI principles to support these.
 Strong interest for business process analysis
 Project management experience (planning, communication)
 Thorough experience in data modelling techniques trough projects
 Understanding development life cycles
 Good organizational skills with demonstrated ability to deliver data oriented projects on time and on budget.
 Detailed understanding of SAS (Base, DI, EG) development and tabular models on SSAS through experience
 Knowledge of SQL server and SSIS
 Good computer skills, proficient with Microsoft office applications including Microsoft BI Tools such as PowerBI.
 Experience with cloud and on premise architectures and setup
The key missions of a BI data modeller/engineer are
. Understanding the functional domain of a business line
. Master the applicative BI domain of a business line
. Organization and moderating workshops with the business client in order to define their business needs.
. Gathering the business requirements and define solutions with the client
. Being able to plan and follow up projects
. Designing the different data patrimonies and keep it coherent in time
. Assessing new tools, new technologies and methodologies
. Creating and updating the functional documentation
. Explaining the functional contents of the projects to the development team (link between users and the development team)
. The monitoring/control of the functional design (logical data, functionalities and controls) of the applications of his business domain.
. Validating the deviations needed in the design and controlling they are documented.
. Ensuring the respect of the functional and technical design standards.
. Definition of best practices and patterns to be applied in his application context and documenting and maintaining them.
. Helping to define the testing approach of the projects with the clients
. Testing of new developments or changes of existing functionalities."
Data Engineer,"Toronto, ON",Gensquared,None,Organic,"Gensquared prides itself on being at the forefront of innovation in the Big Data space.
Founded in 2010, Gensquared provides thought leadership and implementation excellence within the ever-growing data and analytics world. The volume of data is expected to grow to 5x what it is today, and Gensquared helps its customers to be well-positioned for success to use this data to their advantage. Companies that use data have been proven to outperform their peers by as much as 85% (McKinsey Group 2017).
We take pride in having some of the most highly trained and experienced consultants in the industry which translates into optimal value for our clients. We were one of the first companies to provide analytics and data as a service, via the cloud, as early as 2010. We strive to make sure our customers are well-positioned with the best technologies/tools in the industry, constantly evaluating new and existing technology partnerships. Some of the more prominent companies we have partnered with include; Snowflake, DataRobot, MicroStrategy, Informatica, Amazon AWS & Microsoft. We continue to invest in our most valuable resource, our people. We do this through extensive training both on the job and through various educational programs.
Our consultants are family and valuing each other is one of our most important core values. We are looking to grow our family as we search for a Data Integration Consultant.
This individual will be responsible for attaining the following goals:
Attaining a minimum of 1 new accreditation/certification per year
Spending 80% or more of their time on billable work
Completing 90% or more of their agile delivery tasks on time
Demonstrating competency in 1 new relevant technology every year

Day to day activities for this role would include:
Conduct relevant customer interviews to determine key business requirements and objectives
Build appropriate analytical data models based on outcomes of user interviews
Analyze and profile data systems to build source to target data mappings
Build required ETL to populate target designed data warehouse and/or data lake
Review ETL performance and conducts performance tuning as required on mappings/workflows or SQL
Administration and support of data integration infrastructure
2nd level on-call support of ETL services as required
Desired Skills and Experience
University/College degree in Computer Science, Mathematics, Data Science and/or Relevant Degree
5+ years hands-on development, configuration, scripting and administration experience with Data Integration platforms. (i.e. Informatica, Talend, DataStage, SSIS)
BI Experience (MicroStrategy, Looker, Tableau, PowerBI) considered a nice to have
Extensive theoretical and practical knowledge of data warehousing principles/concepts and practical development experience in all areas of the data warehousing life cycle
Experience with Data Management, ETL, Cloud Data (AWS), Data Integration
Knowledge of OLAP-related principles and concepts
Strong grasp of data modeling techniques and concepts (Normalized/Denormalized, Conceptual/Logical/Physical, Star, Snowflake, Data Vault)
Strong knowledge and experience with relational databases such as Snowflake, SQL Server, Oracle (Advanced knowledge of reading and writing SQL, Performance analysis and tuning)
Knowledge and experience with key Big Data technologies (Hive, Presto, Spark, Kafka, NoSQL databases, Semi-structured data access patterns (Json, Parquet, XML, etc.))
Strong Python scripting skills
Excellent communication skills
Great problem-solving skills
Leadership and good client management skills"
Sr Data Engineer,"Etobicoke, ON",Moneris Solutions Corporation,None,Organic,"Creates data collection, extraction, and transformation frameworks for structured and unstructured data. Develops and maintains data infrastructure systems (e.g. data warehouses, data lakes) including data access points. Prepares and manipulates data using MS SQL, Azure Synapse , Databricks and other data pipeline tools.
Organizes data into formats and structures that optimize reuse and efficient delivery to businesses, analytics teams and system applications.
Integrates data across data lake, data warehouse and systems applications to ensure the consistent delivery of information across the enterprise.
Implements backend APIs to enable real time access to datasets.
Accountable for efficient data architecture and systems design.

You will be accountable to:
Builds and evolves the data service layer and engages the team to bring together components for a best-in-class customer offering. Highly skilled in assessing overall data architecture and integrations and making ongoing improvements to the solution offering.
Engage critically with business stakeholders to establish clear needs and link to solutions, including setting up prototypes, and involving multiple parties in design sessions.
Lead in the architecture, design and implementation of complex data architecture and integrations including best practices for the full development life cycle, coding standards, code reviews, source control management, build processes, testing, and operations.
Design, develop and support back-end applications and programs (API) while ensuring all components adhere to a consistent, extensible, evolving architecture which meets business requirements
Perform database monitoring, collaborate with database administrators to optimize database performance
Lead the analyses of database entities, relationships, and attributes to determine efficient design solutions according to business needs
Collaborate with internal stakeholders to ensure adherence to standards for code, design, documentation, testing and deployment
Collaborates with data governance and strategy to ensure data lineage is well understood and constructed in a way to highlight data re-use and simplicity.
Actively works to assess new opportunities to simplify the data operation with new tools, technologies, file storage, management, and process. Uses team context and experience to valuate these opportunities and bring them forward to team members for assessment and implementation.

Your experience includes:
Bachelor's degree required, Masters an asset, in Software Engineering, Computer Science ; or equivalent work experience in a Technology or business environment
Minimum of 7 years of experience working in developing and following structured work processes in data engineering using Microsoft SQL Server or Oracle.
Minimum of 1 year of backend development experience Java, C#, Go, Node.js, API/Microservices
Minimum of 1 year of experience working with Azure/AWS or other cloud environments
Highly proficient in multiple programming languages and coding. Excellent ability to design and engineer moderately complex enterprise solutions.
Highly proficient in data management, governance, data design and database architecture. Proven track record of manipulating, processing and extracting value from large disconnected datasets.
Highly proficient in data modeling, data integrations, data orchestration, and supporting methodologies.
Highly proficient in leading large scale projects or significant project steps and communicating progress/approach with technical/non-technical peers/clients and leaders."
"Staff Software Engineer, Data Insights",Ontario,CircleCI,None,Organic,"CircleCI is looking for an experienced, staff full-stack engineer to help us continue to build the exciting next generation of our Insights product. With Insights, we're leveraging the wealth of data CircleCI has to offer - through data aggregation services, APIs, and UIs - to help customers make better engineering decisions. As a technical leader within our Insights Engineering team, you will work closely with product, design, and your engineering teammates to architect, implement, test, and rapidly iterate on a product that will make a huge impact in how CircleCI's customers build the next generation of software.
About the team
The Insights team is working on a customer facing product that provides teams an avenue to track success/failure rates, throughput, and mean time to recovery, as well as duration metrics, credit burn and more for all of their jobs, workflows, and pipelines. The Insights service truly spans the full stack from a visualization-heavy UI through to our event-driven, data storage and aggregation back-end service.
What you'll do:
Bring an API-first approach to the development of new features in our Insights product and simplify and scale our systems while we rapidly grow and evolve across backend services, APIs, and UIs.
Collaborate with product management and designers, helping to brainstorm on new features and working closely with your engineering teammates on building those features
Write plenty of sustainable, testable, high-quality code.
Help drive a culture of observability and monitoring: using operational data to help the team improve our systems' stability and performance and participate in our on-call rotation
Coach and mentor your teammates to help expand their skills and support their career growth.
What we're looking for:
We're looking for someone who enjoys collaboration, is curious and interested in learning, brings strong communication and teamwork skills, and helps set the standard for a highly collaborative culture by sharing their expertise and encouraging best practices. If this sounds like you, here's the additional experience we're looking for:
Experience designing distributed services that span the technology stack, particularly in our backend services (we use Clojure, PostgresQL, and RabbitMQ, but it's ok if you only have experience in similar tech stacks).
You write code that's easily readable, testable, and maintainable.
Demonstrable experience building applications and services - primarily distributed, event-driven services - using well-accepted design patterns to allow for iterative development.
You're excited by working within a rapidly-iterating team: adjusting to changing priorities, quickly learning new skills, and growing through collaboration.
Experience in the day-to-day practices of continuous delivery and agile development.
Why you'll love working here
We value transparency and collaboration across distributed teams.
We favor regular, incremental delivery of value over perfection.
We encourage continuous learning and improvement for teams and team members
CircleCI engineering competency matrix
This role equals level E4 - Staff Software Engineer - on our Engineering Competency Matrix, our internal career growth system for engineers. These are the minimum expectation for this position, but we are always willing to discuss bringing people on at more senior positions when appropriate. Find more about the matrix in this blog post.
We know there's no such thing as an ""ideal"" candidate - we're all a work in progress and are growing new skills and capabilities all the time. CircleCI welcomes those who are enthusiastic about learning and evolving, so however you identify and whatever your background, if this looks like a role where you could do work that excites you, we hope you'll apply.
Work remotely with our globally distributed team!
We're a distributed company with teammates across the world. For this role, we are hiring engineers to work remotely in The United States and through our affiliate, Continuous Labs, in the following Canadian provinces: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island and Saskatchewan.
How to apply
Submit your application online via the Apply Now button. Please include a cover letter that describes why you're interested in working for CircleCI and summarize how your experience and career goals fit the qualifications for the position.
About CircleCI
CircleCI is the world's largest shared continuous integration and continuous delivery (CI/CD) platform, and the central hub where code moves from idea to delivery. As one of the most-used DevOps tools that processes more than 1 million builds a day, CircleCI has unique access to data on how engineering teams work, and how their code runs. Companies like Spotify, Coinbase, Stitch Fix, and BuzzFeed use us to improve engineering team productivity, release better products, and get to market faster.
CircleCI is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law."
Data engineer - Ingénieur de données Bilingue,"Montréal, QC",ASTEK Canada,"$75,538 - $133,240 a year",Organic,"Rôle:
Implanter les composantes de notre data estate sur AWS
Data Lake
Sandbox Analytique
Data Warehouse
Visualisation
Mise en place des meilleures pratiques en gouvernance de données AWS
Profil :
· Détenir un baccalauréat ou maîtrise en informatique, en génie logiciel (requis)
· 2-5 d'expérience comme ingénieur de données (requis)
· Min de Deux (2) années d’expérience avec les environnements AWS (requis)
· Expérience pratique en création de solutions infonuagiques de données
· Connaissance des processus ETL
· La certification AWS (un atout)
· Expérience avec les outils AWS suivants: Athena, Glue, S3, RedShift
· Connaissance approfondie des méthodes de développement agile/SCRUM
· Bilinguisme français et anglais, parlé et écrit (requis)
Reference ID: 3476
Date de début prévue : 2020-11-22
Type d'emploi : Temps Plein, Pigiste (Freelance), Permanent
Salaire : 75 538,00$ à 133 240,00$ par an
Avantages :
Assurance Maladie Complémentaire
Congés de Vacances et Compensatoires
REER Collectif
Horaire :
8 Heures
Du Lundi au Vendredi
Repos la Fin de Semaine
Langue:
Français (Souhaité)
Télétravail:
Temporairement en raison de la COVID-19"
Senior Software Engineer-Data Analytics,"Vancouver, BC",MacroHealth,None,Organic,"Are you interested in being part of a rapidly growing team tasked to build the next-generation industry-leading data platform for healthcare markets? We are looking for smart, passionate, and highly skilled Senior Software Engineers - Data Analytics to join the effort to define and build the future of MacroHealth. You will be the founding member of our data team and will own and implement our data collection and reporting systems across all levels of the stack.
*About us:  ** *
The U.S. health sector is complex and dynamic - a vast and diverse network of entities that impacts the population and economy like no other industry. At USD$3.3 Trillion, healthcare spending represents nearly 18% of U.S. GDP. It is complex and inefficient. Health service buying, selling, and settlements have seen little change in the past decade despite clear changes in available information, technology, regulation, and market consolidation.
Enter MacroHealth. MacroHealth is a mission-driven company. We are building the most advanced and trusted market platform for health service payers and providers. The company’s vision is to create Intelligent Health Markets by building technology, knowledge and relationships that enable payers and providers to optimize the buying and selling of health services.
We are expanding our team of passionate thought leaders as we pursue our goal of disrupting the U.S. health sector with our vision and technology. We are committed to developing leading edge solutions in a supportive environment focused on customer success and employee fulfillment. We provide our engineers an environment in which they can contribute from day one while also providing opportunities for learning and growth.
*Responsibilities:  ** *
You will be the founding member of our data team and will own and implement our data collection and reporting systems across all levels of the stack.
You will work with cutting edge technologies, taking advantage of open-source initiatives and third-party services which provides the best developer experience.
This position requires you to be a self-starter, innovative and with the ability to take ownership, work with tight timelines, and handle various tasks simultaneously while continuing to develop a positive work culture. You will join a world-renowned leadership team with a track record of leading the development of multiple successful companies and products.
Help architect, build, manage and scale our data platform.
Design and implement features and data solutions in UI, RESTful API and backend
Collaborate with data scientists and engineers to solve complex data problems at scale
Identify and evaluate new technologies that improve performance, maintainability and elegance of our infrastructure
Contribute to a team culture that values the people-first culture, inclusiveness and quality while fostering innovation
Advocate for technical quality, effective team processes, and engineering best practices
*Your background includes**: ** *
7+ years of professional experience as a full-stack software developer, in increasingly senior roles
Solid software engineering skills and proficiency with object-oriented and/or functional programming languages, such as Python, Scala, Java, etc.
Strong experience with data processing frameworks and tools, such as Spark, Hadoop, Hive, Kafka, etc.
Experience designing and building solutions within a cloud-based microservice architecture, using docker containers and/or Kubernetes
Strong experience with cloud platforms and their services for functions, secrets management, deployment and data storage. We currently use Azure but equal experience in AWS or GPC is great too
Strong dedication to code quality, automation and operational excellence: unit/integration tests, scripts, workflows.
Demonstrated ability to learn new technologies and work across technologies
Strong communication skills, both verbal and written
Thriving in a people-first culture of teamwork and respect and comfort working within an Agile methodology
Bachelor’s degree in Computer Science or related field, or equivalent work experience
*Your background may additionally include**: ** *
Experience building pipelines using data analytics platform such as Azure Databricks
Experience with modern JavaScript frameworks such as ReactJS
Knowledge of the US Healthcare space including standards such as HIPAA
Job Type: Full-time"
Senior Data Engineer - Toronto Hub,"Toronto, ON",Veeva Systems,None,Organic,"Our engineering and product teams are organized around our hubs for community and collaboration. Work anywhere means you can work at home or the office on any given day. Your product hub is based on the primary location of your product. You should live within one timezone of your product hub. Our current product hubs are Pleasanton, Columbus, Boston, NYC, Raleigh, and Toronto.

We are looking for experienced data engineers to build a cloud-based data analytics solution for the life science industry. If you are passionate about data and are eager to design and build data platforms from the ground up this is the role for you. The data analytics platform will provide data ingestion, data storage, and rich data analytics capabilities with elegant visualization dashboards.
What You'll Do
Design and implement AWS based ETL processes to onboard data into our data lake from a variety of internal and external sources for our new data analytics platform
Design data models and data services for optimal storage and retrieval
Implement scalable data lake interfaces, microservices, and rest based API for querying and storing structured data
Integrate new technologies to support advanced analytic use cases
Requirements
5+ years’ experience in Python or Java, preferably at an enterprise cloud software company
Proven ability to write clean, testable, readable code in a team environment
Hands-on experience with building data pipelines in a programming language like Java or Python
3+ years of experience in relational databases with a mastery of SQL
Experience in data modeling, ETL development (pref. Apache Spark), and Data warehousing
Nice to Have
AWS Services (S3, Redshift, Elastic Search)
Experience with large scale big data pipeline – ETL / Kafka / Spark / MapReduce / Hadoop
Familiarity with Open API Specifications and Swagger
Experience working in an agile environment
Learn More
Engineer Perspective: 3 Reasons to Consider Veeva
Engineering at Veeva
Perks & Benefits
Allocations for continuous learning & development
Annual budget to donate to the non-profit of your choice
Health & wellness programs
#LI-Remote

Veeva builds enterprise cloud technology that powers the biggest names in the pharmaceutical, biotech, consumer goods, chemical & cosmetics industries. Our customers make vaccines, life-saving medicines, and life-enhancing products that make a difference in everyday lives. Our technology has transformed these industries; enabling them to get critical products and services to market faster. Our core values, Do the Right Thing, Customer Success, Employee Success, and Speed, guide us as we make our customers more efficient and effective in everything they do.

Veeva’s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world."
"Senior Software Engineer, Data Platform","Halifax, NS",MobSquad,None,Organic,"ABOUT MOBSQUAD
We are a well-funded, hyper-growth, scale-up looking for an experienced Data Systems Engineer with strong experience in the security industry. If you've ever dreamed of working with a top tier technology company scale-up, on leading edge technologies, backed by the very best venture capitalists in the world, then this is your chance.
Some details about MobSquad:
MobSquad solves the significant and growing technology talent shortage faced by US-based start-ups and scale-ups by enabling our clients to quickly have a turnkey ""virtual"" Canadian subsidiary, where Canadian-based technology professionals work with our clients on an exclusive basis
We've been featured in The Globe and Mail, Calgary Herald, The Financial Times (UK), BetaKit, CBC, Global News, and many other places. We're most proud of our front-page article in The Washington Post, as well as our coverage on NPR
We're a Certified B Corporation, were recognized as one of Canada's Best Places to Work in 2019, and have made a financial commitment to the Upside Foundation. We believe we are playing a key role in enhancing Canada's innovation economy, and have received financial support from the Government of Canada, Province of Alberta, Province of Nova Scotia, and City of Calgary, to support this ambition
You can learn more about us on our website
ABOUT THE ROLE
In this role as a Senior Software Engineer, you will use Python to gather, process, and analyze large datasets and build data services that are used for software development. You'll spend 80% of your time building highly scalable back-end services, and 20% of your time solving complex data processing problems.
ABOUT YOU
You have a Bachelor's degree in Computer Science, Engineering, or a comparable field from an accredited institution
You have over five years of experience with Python, Node.js, and Java in a cloud environment
You have over three years of data engineering experience, and over five years of overall software engineering industry experience
You have experience with webservice standards and protocols including REST, SOAP, GraphQL, Swagger/OpenAPI
You have experience with large-scale SQL and NoSQL databases and datasets
You have experience processing large datasets with Hive and Hadoop
You have experience with big data formats and technologies including Spark, Parquet, Avro, ORC, HDFS, CephFS, HBase, Cassandra, Elasticsearch, MongoDB, Lucene
You have experience with caching and load balancing to maintain highly scalable webservices
You have experience designing, building, and maintaining large-scale data infrastructures
You have experience with software development in a Linux and/or Unix environment
You have experience creating and delivering microservice architectures
You have experience providing zero downtime and continuous availability for large-scale distributed systems and technologies
You have experience with Continuous Integration and Continuous Delivery (CI/CD) processes
WHAT YOU'LL GET @MOBSQUAD
A full-time position that offers competitive compensation
A benefits program delivered through our bespoke digital platform, giving you control, choice, and flexibility. We give you the ability to build your package of benefits covering health (think medical, dental, vision), wellness (think gym, workout gear, massage, transit) and RRSP (think retirement)
A downtown office location with first-rate amenities, surrounded by great restaurants and easily-accessible transit
For international candidates, sponsorship for an immediate work permit, expedited permanent residency, and Canadian citizenship within four years.
At MobSquad, we support and encourage building a work environment that is diverse, inclusive, and safe for all. We invite and welcome applicants of all backgrounds, regardless of race, religion, sexual orientation, gender identity, national origin, or disability."
Data Acquisition Engineer - Ingénieur en acquisition de donn...,"Montréal, QC",Data Acquisition Engineer - Ingénieur en acquisiti...,"$50,000 - $65,000 a year",Organic,"Note : English below
Qui nous sommes
On est une start-up techno qui développe une infrastructure de recherche pour le système de santé digital de demain. Les entreprises de toutes tailles utilisent notre suite d’outils pour faciliter la navigation de leurs utilisateurs au sein du système de santé au Canada. Sache que pour y arriver, on travaille avec ce qu’il y a de plus cutting-edge et on n’a pas l'intention d'arrêter. Le défi tech devenant de plus en plus important, on a rapidement besoin de quelqu’un pour prêter main-forte à l’équipe de devs.
Ce qu’on recherche
Un FIT.
On cherche quelqu’un qui se lève le matin en ayant envie d’être meilleur que la veille et pour qui un 7/10 n’est pas « ok ». On cherche aussi quelqu’un qui a envie de faire partie d’une équipe et qui a la passion et la collaboration comme un must pour atteindre un objectif.
Sinon, on cherche un collègue capable de nous mettre au défi, de définir de nouvelles solutions et de les mettre en œuvre, de comprendre comment les solutions techniques contribuent à la réalisation d'une vision plus large, et qui a la passion d'interagir avec de nombreuses personnes différentes pour atteindre un objectif.
Si tu te reconnais dans ça, t’es ce qu’il faut à Clinia !
Côté technique, tu devrais avoir :
Excellentes capacités d’analyse et de résolution de problèmes;
Une bonne compréhension des différents protocols de communication et du web;
De l’expérience en traitement parallèle et en manipulation de données;
Une bonne base en HTML, XML et avec l'utilisation de XPath;
De l’expérience avec Python, C# et Java;
De solides compétences en matière de gestion de projet et d'organisation;
Un diplôme d’études universitaires en informatique ou dans un domaine connexe de l'ingénierie;
Excellentes compétences en communication, à l’écrit et à l’oral, y compris la capacité de communiquer des informations techniques à des professionnels non spécialisé en informatiques;
Être capable d’échanger fluidement en Anglais et Français.
Ton rôle
En tant qu'ingénieur en acquisition de données chez Clinia, tu dois :
Créer et maintenir une architecture optimale de pipeline de données
Mettre en place des processus soutenant l’acquisition, la transformation et la mise à jour des données
Travailler avec les parties prenantes pour aider à résoudre les problèmes techniques liés aux données et répondre à leurs besoins en matière d'infrastructure de données
Si tu as d’autre skills que tu crois seraient un plus à l’équipe, on est très curieux de t’entendre.
Ce que nous avons à t’offrir
Des équipements performants (nouveau macbook pro) et un poste adapté à tes besoins (tous les membres de l’équipe ont leur propre stand-up desk) ;
Des horaires flexibles : libre de travailler aux heures que tu es le plus productif (on fait la grasse matinée nous aussi des fois) ;
Un bureau à 30 secondes (littéralement) de la station Sherbrooke  ;
Du télétravail ;
Du café de spécialité, torréfié à Montréal et spécialement livré au bureau chaque semaine ;
Un accès direct à un médecin en ligne 24/7 pour toi et ton/ta conjoint(e) grâce à notre partenaire (et client) Dialogue ;
Des teams buildings, 5@7, et activités d’équipe ;
3 semaines de vacances.
On offre aussi l’opportunité :
De jouer un rôle essentiel dans le développement d'une entreprise d’impact ;
De contribuer au développement d’un produit utilisé par des millions de patients au Canada ;
De travailler avec une équipe de personnes persévérantes et ambitieuses avec un véritable esprit d'équipe.
Notre approche est simple :
Nous sommes une jeune équipe dynamique qui prône l’implication et l’égalité de chacun dans la prise de décision - on ne dit pas ça pour être cool, on y croit vraiment. Nous cherchons donc une personne qui saura mettre son expertise à profit pour nous aider à bâtir un solide futur pour demain.
Tu as la motivation, le focus et l’esprit entrepreneurial pour relever ce défi ? Nous cherchons quelqu'un comme toi !
-----
Who we are
We're a technology start-up developing a search infrastructure for the digital health system of tomorrow. Companies of all sizes use our suite of tools to make it easier for their users to navigate Canada's health system. To achieve this, we are working with the most cutting-edge technologies and have no intention of stopping anytime soon. As the technical challenges become more and more important, we quickly need someone to lend a hand to the dev team.
What we're looking for
A FIT.
We are looking for someone who gets up in the morning wanting to be better than the day before and for whom a 7/10 is not ""ok"",who wants to be part of a team and for whom collaboration is a must. We are also looking for a colleague that can challenge us, define new solutions and see them through to implementation, understand how technical solutions contribute to realizing a broader vision, and has a passion for interacting with many different people to achieve a goal.
If you recognize yourself in these previous lines, you're just what Clinia is looking for!
On the technical side, you should have :
Excellent analysis and problem-solving skills
A good understanding of the different communication protocols and the web;
Experience in parallel processing and data manipulation;
A good knowledge of HTML, XML and the use of XPath;
Experience with Python, C# and Java;
Strong project management and organizational skills
A university degree in computer science or a related field of engineering;
Excellent communication skills, both written and spoken, including the ability to communicate technical information to non-IT professionals
Be able to communicate fluently in English and French.
Your role
As an Data acquisition engineer at Clinia you will :
Create and maintain optimal data pipeline architecture
Build processes supporting data acquisition, transformation and update
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs
What we have to offer you
High-performance equipment (new MacBook pro) and a workstation adapted to your needs (all team members have their own stand-up desk) ;
Flexible hours: free to work the hours you are most productive (we sleep in sometimes too);
An office 30 seconds (literally) away from the Sherbrooke station - of course, you'll still have to use your living room as a #covid19 office;
Work-from-home schedules;
Montreal roasted coffee specially delivered to the office every week;
24/7 direct access to an online doctor for you and your spouse through our partner (and client) Dialogue;
Team buildings, 5@7, and team activities (virtual for now).
We also offer the opportunity to :
Play an essential role in the development of a scaling company;
Contribute to the development of a product used by millions of patients in Canada;
Work with a team of persevering and ambitious people with a true team spirit.
Our approach is simple:
We are a young and dynamic team that advocates the involvement and equality of everyone in decision-making - we don't say that to be cool, we really believe in it. So we're looking for someone who can use their expertise to help us build a solid future for tomorrow.
Do you have the motivation, focus and entrepreneurial spirit to meet this challenge? We're looking for someone like you!
Type d'emploi : Temps Plein, Permanent
Salaire : 50 000,00$ à 65 000,00$ par an
Avantages :
Congés de Vacances et Compensatoires
Événements d'Entreprise
Horaires flexibles
Options d'Achats d'Actions
Stationnement sur place
Tenue Décontractée
Travail à Distance
Horaire :
8 Heures
Lieu:
Montréal, QC (Souhaité)
Télétravail:
Oui, toujours"
Senior Data Engineer (VP),"Mississauga, ON",Citi,None,Organic,"Key Responsibilities:
Working closely with a global team building large distributed data-centric applications in the trade surveillance and compliance space.
Designing and building message and data processing/streaming services to enable seamless integration with multiple business systems.
On-boarding of new data streams from external systems.
Working closely with data scientists to ensure data is of the highest quality and is available when needed.
Act as data pipeline subject matter expert to the global application team as well as other internal and external stakeholders.
Building close relationships with clients and stakeholders to understand the use cases for the platform and prioritising work accordingly.
Working well in a multidisciplinary DevOps-focused team and building close relationships with engineers, data scientists, business analysts, and production support teams.
Holds themselves accountable for ensuring high quality results and acts as a mentor and coach to other team members.
Skills & Qualifications:
You have experience driving the technical direction on data-intensive projects.
You will have specific examples of times that you have:
Delivered value to business by getting applications into production.
Designed systems from scratch that can scale with large volumes of data.
You have expertise in multiple programming languages and building data pipelines, ideally using Spark and Java.
You have expertise working with message/event streaming services, ideally using Kafka or Solace.
You have experience working with both relational and non-relational databases, ideally Oracle, Mongo and Elastic Search
You understand the full spectrum of the data processing and integration ecosystem, including testing strategies.
You have experience working in a DevOps culture and are comfortable working with CI/CD tools (ideally IBM UrbanCode Deploy, TeamCity, and/or Jenkins.)
Ideally, you have experience working in virtualized environment, as well as container orchestration services such as Kubernetes/OpenShift.
You have experience in systems observability including monitoring and health patterns and the tools to ensure the highest production stability.
You have high development standards, especially for code quality, code reviews, unit testing, continuous integration and deployment.
You have proven capability to interact with clients and deliver results – from ideation to production.
You have experience working in fast paced development environments.
You agree that verbal and written communication skills are vital.
Citi Canada is an equal opportunity employer. Accordingly, we will make accommodations to respond to the needs of people with disabilities (including, without limitation, physical and mental health disabilities) during the recruitment process and otherwise in accordance with law. Individuals who view themselves as Aboriginals, members of visible minority or racialized communities, and people with disabilities are encouraged to apply.
-
Job Family Group:
Technology
-
Job Family:
Applications Development
-
Time Type:
-
Citi is an equal opportunity and affirmative action employer.
Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Citigroup Inc. and its subsidiaries (""Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi.
View the ""EEO is the Law"" poster. View the EEO is the Law Supplement.
View the EEO Policy Statement.
View the Pay Transparency Posting"
Data Engineer – Hadoop w Python or SQL Shell - 297741,"Toronto, ON",Procom,None,Organic,"Data Engineer - Hadoop w Python or SQL Shell
On behalf of our client in the Banking Sector, PROCOM is looking for a Data Engineer - Hadoop w Python or SQL Shell.
Data Engineer - Hadoop w Python or SQL Shell – Job Description
The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhances data feeds.
The group is currently working on a new modernization of the current in-house system that will require a strong data engineer who has a strong foundation to submit into the enterprise data lake experience.
The Digital Banking & Operations Analytics team consolidates, customizes and supports internal platforms for customer and product onboarding channels
Update and ingest data into the Enterprise Data Lake with Hadoop
Conduct ETL, SQL and DB performance tuning, troubleshooting, support, and capacity estimation to ensure highest data quality standards with the business and technical SME’s.
Helps with the process of profiling data and ETL logic for documenting end to end data flow and lineage from capture at source, to storage, to delivery and its business intent at each stage (i.e. capture, transformation, fragmentation, editing)
Profile and analyze source data to determine the best reporting structures to build.
Design and develop ETL pipelines using multiple sources of data in various formats according to business requirements.
Conduct dimensional modelling, metadata management, data wrangling, data cleaning and conforming, and warehouse querying.
Provide day-to-day support and technical expertise to both technical and non-technical teams
Work with other engineers to brainstorm solutions to problems and support others in their goals.
Exhibit sound judgement, keen eye for details and tenacity for solving difficult problems.
Use strong analytical skills and support use of data for sound decision making.
Help us build data expertise and data focused mindset throughout the enterprise
Translate business needs into technical requirements
Understand how the Bank’s risk appetite and risk culture should be considered in day-to-day activities and decisions.
Data Engineer - Hadoop w Python or SQL Shell – Mandatory Skills
4-5 years’ experience ingesting data with Hadoop (Hive/Beelin, HDFS, Sqoop, Spark)
4-5 years’ Python (PyHive, PySpark) OR SQL-Shell (Bash, Korn) & general linux/unix familiarity, R, J
3+ years of hands on experience with relational databases (Oracle, DB2, Redshift, etc.)
Good communication skills to work with multiple business lines
Build relationships, communicate well with teams outside of Canada
Strong written and documentation
University education in Business, Computer Sciences, Electrical/Computer/Software Engineering, Mathematics, or equivalent experience.
Data Engineer - Hadoop w Python or SQL Shell – Nice to Have Skills
Java programming is a nice to have
Other (DB2, SQL Server, etc.)
Experience on the following programing Languages:
CI/CD tech stack:
Git/BitBucket, Jenkins, Artifactory
Engineering Change Management:
Change Requests (Service Now RFCs), CAB Review process, error checking and logging
ETL techniques:
SFTP/SSH, Ingestion (across platforms via direct DB connection or flat file), Data Modelling
Talend
Knowledge of data engineering and part of the technical interviews – has flexibility
Data Engineer - Hadoop w Python or SQL Shell - Assignment Start Date
ASAP – 12 months to start
Data Engineer - Hadoop w Python or SQL Shell - Assignment Location
Toronto, ON – Work Remotely"
Associate Data Engineer,"Montréal, QC",Moka,None,Organic,"The Role
You are passionate about data, performance, accessibility, and you are a result-oriented professional. You are a product builder, who loves solving challenging problems and delivering powerful analysis that will help Moka reach new levels. We are looking for specialists who want to work on cutting-edge technologies, be part of the data team behind one of the most popular Fintech app in Canada, bring fresh ideas and want to see them come to life. As an Associate Data Engineer, you will serve all departments and project critical to Moka's needs and have opportunities to develop your skills.
You will develop a deep understanding of all the data sources used by the team, you will be connecting and streaming data from company's databases to enable stakeholders to efficiently mine data and enable them to optimizations in company's product development and business strategies. You will be responsible to maintain data integrity of the data warehouse by implementing quality control measures, defining and document best practices to reduce and resolve data anomalies.
Interested? We are looking for a bright individual who loves data and analytics and who demonstrates a willingness to go the extra mile, takes on the responsibility of doing what needs to be done, has a positive attitude and rolls with changes. This person will have a great opportunity to join a committed and talented team of data enthusiasts.
What You'll Be Doing
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data' technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Skills
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data' data stores.
We are looking for a candidate starting out in the field of Data Engineering, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and MongoDB.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Javascript.


Meet Moka
Moka is on a mission to help Canadians achieve their financial goals. The Moka app makes it easy for everyone to save by automatically rounding up their purchases and investing the spare change, but roundups are just the beginning. To help our users across all aspects of their financial lives, we use data to develop personalized insights, recommendations and services. Now, we are looking for more great people who want to be part of a high-energy startup reshaping the financial services industry. If you want to work with talented people, solve big problems and make life better for hundreds of thousands of people, in Canada and beyond, come join us at Moka.
Why Moka?
Moka is Canada's fastest growing fintech, and we're just getting started. Since launching in July 2017, Moka has been downloaded over 750,000 times and has helped Canadians save and invest, many for the very first time.
We've been featured in major media (https://mylo.ai/press/) across the country, won a deal on Dragons' Den, and acquired an asset management firm with over $120M AUM. Our CEO Phil Barrar was nominated for EY Entrepreneur of the Year, made the Bay St Bull's 30<30 List, and has raised $14M in funding to date, most notably from NAVentures, the venture capital division of National Bank of Canada and Desjardins Capital, the venture capital arm of North America's largest association of credit unions.
What We Offer
In a short time, our team has grown from just a few to more than 50 people. At Moka, we value making an impact, asking the tough questions and being inclusive. Our people and their passion are what truly set us apart.
Open-concept offices in St-Henri (dogs are welcome!) with snacks and coffee provided
Comprehensive health benefits package including remote care and extras like subsidized metro/bus pass
Generous vacation policy as well as flexible work hours and location
Support for continuing your skills development, developing your industry knowledge and achieving your career and personal development goals
Team activities and get-togethers like game night and our annual holiday chalet getaway
A casual environment where you will work with smart and passionate people
We believe in being an equal opportunity employer and we celebrate diversity and differences of perspectives at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Vancouver, BC",App Annie,None,Organic,"Data Engineer
App Annie is growing and we're seeking a Data Engineer to design, build App Annie's ETLs and data systems.
In this role, you will partner with technology providers, product managers, analysts, and other engineers to develop and maintain cloud-based data warehouse infrastructure, assuming overall responsibility for the delivery of projects. App Annie is in the business of data insights for the App economy.

Our team is deeply data-centric, global, SaaS-based, and cloud-based. Check us out at www.appannie.com.
Qualifications
Bachelor's degree in computer science, information technology or another computer-based discipline, or a comparable combination of education and experience.
Minimum 3 years hands-on experience developing ETL/data pipelines (Python preferred).
Minimum 3 years of SQL/NoSQL experience designing and building database objects to meet business requirements.
Experience with at least two database platforms (Cloud-based preferred).
Preferred Experience: Working on global engineering teams.
Preferred Skills: AWS, Snowflake, Redshift/Google BigQuery, PostgreSQL, S3.
#LI-21_KS1
What do we offer?
Lots of responsibility + room for you to experiment and grow with the company.
An international team of super-talented people from different cultural backgrounds (Beijing, San Francisco, Utrecht and more)
Paid leave, so long as you promise to come back!
Competitive salary, performance bonus, and stock options
Extended health and dental benefits
Gym membership subsidy
Covid-19
During the Covid-19 Pandemic we've adapted how we support our employees in a number of ways:
Donut Dates - connecting you with someone else in your region once every 2 weeks so you can get to know each other
WFH Equipment Allowance - To ensure you have the best set up in your home office
Productivity Allowance - To keep your home office running
Wellness Sessions - To keep everyone happy and healthy
Virtual Happy Hours and Trivia to keep you connected with your colleagues
Generous PTO that we encourage you to use for your wellbeing to go offline, recharge, and reflect
You will also be taken through a fully digital onboarding process!
Why choose App Annie?
App Annie is the industry's most trusted mobile data and analytics platform. App Annie's mission to help customers create winning mobile experiences and achieve excellence. The company created the mobile app data market and is committed to delivering the industry's most complete mobile performance offering. More than 1,100 enterprise clients and 1 million registered users across the globe and spanning all industries rely on App Annie as the standard to revolutionize their mobile business. The company is headquartered in San Francisco with 12 offices worldwide."
Senior Data Engineer,"Montréal, QC",Guavus,None,Organic,"Guavus is looking for a highly motivated and talented Senior Data Engineer to participate in the development of the most advanced solutions in the Big Data space by using agile methodologies. The developer will actively participate and collaborate with data team to design and implement data pipelines integrating advanced AI/ML models.
Responsibilities
Develop and maintain batch and streaming data pipelines with big data technologies such as Spark, Kafka, Hive, HDFS, HBase, Phoenix, Impala etc.
Analyze and implement proof of concepts related to big data technologies.
Analyze new technologies (DB, Storage, Compute Engines).
Produce quality code that is well documented.
Participate in code reviews and mentoring.
Required Skills
Degree in Computer Science or Engineering
Five years of experience in a data engineer position
Experience in Cloud and non-Cloud based Hadoop ecosystem
Experience in data warehousing and ETL development
Fluent in Java & with some Scala knowledge
Fluent in SQL
Experience in performant and highly scalable applications
Experience in distributed framework and technologies e.g. Columnar Database, NoSQL and Hadoop
Experience in Linux and shell scripting
Basic knowledge or interest in Python
Fluent in English, both written and spoken
Preferred Skills
Speaking French is an asset
About Guavus (a Thales company)
Guavus is at the forefront of AI-based big data analytics and machine learning innovation, driving digital transformation at 6 of the 7 world’s largest telecommunications providers. Using the Guavus-IQ analytics solutions, customers are able to analyze big data in real time and take decisive actions to lower costs, increase efficiencies, and dramatically improve the end-to-end customer experience – all with the scale and security required by next-gen 5G and IoT networks.
Guavus enables service providers to leverage applications for advanced network planning and operations, mobile traffic analytics, marketing, customer care, security and IoT. Discover more at www.guavus.com and follow us on Twitter and LinkedIn."
Data Engineer,"Toronto, ON",Game Hive Corporation,None,Organic,"The Hero Profile
You're the type of person who can handle a fair amount of work freedom while understanding how to manage yourself. Knowing that, ""with great power comes great responsibility,"" you move fast, and you thrive off of new solutions to unique problems. You know when to build something on your own or when to leverage existing solutions, and have confidence in your skills to make key decisions.
About Us
We make games. Popular games that have generated over 120 million downloads. You’ll be working closely with the game teams to provide the infrastructure for important game features, in part or in full.
We are our own system admins. We like to keep things in-house to manage the performance and reliability of our servers.
We are secure. The idea of storing passwords in plaintext makes us (and you!) cringe, while the idea of catching cheaters makes us giddy.
We do a full stack. Server code, database schemas, caching, cloud infrastructure. We do it all.
We like a fun workplace. We have challenging projects and hackathons where our talented people can work on ideas that they're passionate about.
We like munchies. Hungry? Don't worry, our kitchen is fully stocked with snacks, fruits, drinks, and beer.
Oh, and we play games. Often.
Responsibilities
Responsibilities include building web APIs for our mobile game features, collecting metrics from game usage to server health, and ensuring the performance, scalability, and reliability of our servers.
You’ll be doing Python development (including frameworks such as Flask and SqlAlchemy).
You’ll be responsible for the PostgreSQL database schema design of any feature you build.
You’ll be working with a series of production Redis deployments and should know when data belongs in Redis vs PostgreSQL.
You'll be using/developing in a Linux environment.
You're able to manage multiple Linux machines.
You'll feel comfortable managing the performance and reliability of our servers.
Markup, server code, database schema, database schema - You're comfortable with managing it all.
You’ll grow incredibly familiar with Amazon Web Services and Google Cloud Platform if you aren’t already.
Max Level Skills (Requirements)
You have a degree in Computer Science, Software Engineering or equivalent education and/or experience
Familiar with technologies and languages used: Python, Flask, SqlAlchemy, Memcached, Redis, Celery, PostgreSQL, MySQL, Docker, Amazon Web Services (AWS) and Google Cloud Platform (GCP)
Familiar with Big Data ETL/machine learning pipeline development
Experience with workflow automation tools (e.g. Airflow)
You have advanced SQL skills
Familiar with microservices and data modelling concepts
You have worked with Spark in the past and understand when to use it

How to Apply
Submit your resume and cover letter in pdf format. In your cover letter, please outline why you're the star we need."
Data Engineer,"Toronto, ON",RBC,None,Organic,"What is the opportunity?
The Data Engineer researches, develops and implements new and existing data and visualization technologies to provide Technology and Business Lines with customer experience, performance, productivity and sales insights from self-service and Advisor-assisted Contact Centre interactions (e.g. voice, email, chat, video, text). You will ensure new initiatives meet Technology and Business Lines data and reporting needs, in alignment with Contact Centre Technology strategy and Technology capabilities defined in the future state (e.g. Omni-Channel reporting, Gamification/Visualization).

What will you do?
Execute development and integration activities (planning, execution, testing, deployment and post implementation support) of existing and new data and visualization technologies (e.g. Omni-Channel reporting, Gamification/Visualization)
Prepare data to be used for analytics, operational reports and data science work
Provide third level support for data and visualization technologies incident and problem resolution; act as primary technical lead for problem resolution and root cause determinations in close collaboration with third party vendors, if appropriate
Research emerging data and visualization technologies trends/best practices and propose solutions for technology and business partners
Provide advice to Technology & Operations, Contact Centre Technology and business lines as Subject Matter Expert in enterprise wide data and visualization technologies in collaboration with Technology & Operations partners (e.g. Enterprise Information Management); provides technical leadership and guidance

What do you need to succeed?
Must-have
Experience in requirement analysis, design, implementation, and testing of software solutions, especially data related using Python and other programming languages
Advanced working SQL knowledge and experience working with relational databases
Strong analytic skills related to working with datasets
Experience with big data tools (Hive, NiFi, Kafka and Spark)
Undergraduate degree in Computer Science, Engineering or equivalent experience
Experience participating of multiple complex cross functional projects

Nice-to-have
Advanced knowledge of Contact Centre technologies

What’s in it for you?
We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual.
A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable
Leaders who support your development through coaching and managing opportunities
Ability to make a difference and lasting impact
Work in a dynamic, collaborative, progressive, and high-performing team
Opportunities to do challenging work
Opportunities to take on progressively greater accountabilities
Opportunities to building close relationships with clients
Access to a variety of job opportunities across business and geographies

Learn more about RBC Tech Jobs

Join our Talent Community
Stay in-the-know about great career opportunities at RBC. Sign up and get customized info on our latest jobs, career tips and Recruitment events that matter to you.

Expand your limits and create a new future together at RBC. Find out how we use our passion and drive to enhance the well-being of our clients and communities at rbc.com/careers.

JOB SUMMARY
City: Toronto
Address: 88 Queens Quay W
Work Hours/Week: 37.5
Work Environment: Office
Employment Type: Permanent
Career Level: Experienced Hire/Professional
Pay Type: Salary + Variable Bonus
Required Travel(%): 0
Exempt/Non-Exempt: N/A
People Manager: No
Application Deadline: 11/15/2020
Platform: Technology and Operations
Req ID: 288622
Ad Code(s):"
Data Engineer / Data Developer,"Montréal, QC",Callière Group,None,Organic,"Stealth start-up backed by titans in the tech sector who've each enjoyed globally renowned success, and have applied their collective expertise to this massive endeavour. The 'beta' for this venture is a 2-3 year plan. The actual plan, is a very long term project with enormous financial backing. This isn't a bootstrapped start-up that might go away in 6-12months.


The mandate of the Data Science and Engineering team is to acquire, pre-process, and distribute multiple data sources for investment research. As part of the team you will work to obtain new datasets, define and synthesize features of interest, and productize the full pipeline from dataset acquisition to deployment of predictive modelling processes.


You will:


Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.).
Process unstructured data into a form suitable for analysis.
Monitoring performance and advising any necessary infrastructure changes.
Implement data microservices.
Collaborate closely with teammates to ensure consistency and maximize re-use of software components.
Candidate Requirements




Requirements

Bachelor’s Degree in Computer Science, Engineering or related subject.
Experience with data mining applications, preferably for finance-related applications
Python experience strongly preferred with extensive experience in core data science libraries (NumPy, SciPy, Pandas, Scikit-Learn, Tensorflow, etc)
Experience with SQL and NoSQL technologies.
Experience developing, scripting, debugging and performance tools on Linux based development environment.
Critical thinking: ability to track down complex data and engineering issues, evaluate different algorithmic approaches, and analyze data to solve problems.


Self-starter : ability to rapidly learn and apply new technologies as warranted by projects.


Preferred Qualifications:


Team player with excellent communication skills.
Experience with highly available distributed systems.
Experience with web application development
Experience with real-time high performance software applications (asset)."
Senior Software Engineer (Big Data),"Toronto, ON",NLogic,None,Organic,"Backend/Data Engineer
Permanent
Toronto, Ontario
About NLogic
We turn insights into action.
As Canada's leading provider of audience analysis tools for the TV and radio broadcast industry, media agencies, martech firms and more, we help our clients bring data to life. We’ve developed industry first APIs, media trading platforms, and data integration systems that drive innovation and growth across the industry.
We could not achieve these accomplishments without our amazing team. We are passionate about hiring individuals who push us to be our best and to do more. We encourage them to speak up and share their ideas. We also support their thirst for training and development to ensure our employees build a strong future along with ours. But we’re not all work; we like to have fun too. An important part of building a cohesive and collaborative team is creating work-life balance. From flexible work hours to Friday social sessions, we know how to have fun. Even in a virtual world, we are constantly finding unique ways to bring us all together including virtual BINGO games, photo and cake decorating contests, online scavenger hunts and more.
www.nlogic.ca
About the role
NLogic is looking for a hybrid Backend/Data Engineer to join a talented team of engineers that share common interests around building distributed backend systems, managing data at scale, and driving agile development across the organization. This is a role for engineers that are familiar with standard web backend architecture, and capable in database design and interaction. Your ability to visualize the flow of data through a complex application is critical to your success, and to the team’s.
In this role, you would also display the following characteristics:
Strong attention to quality and detail
Energetic and eager to collaborate across departments, supporting corporate objectives
Curious and passionate in your approach to development
Qualifications and Experience
Knowledge and experience in:
Software development on Microsoft development stack including .Net Framework, C#, TFS
Building distributed, well-designed services
Data platforms and Big data systems like Databricks, Hadoop, Spark, Python, SQL, DAX etc.
Cloud platforms such as Microsoft Azure
Agile software development process
Parallel/multithreaded algorithms and processing methods, systems design, algorithms and data structures
Testing frameworks
Responsibilities
Design, develop and operate business critical systems with focus on high availability, low latency and scalability.
Work on custom projects involving data and systems integration
Collaborate with other engineers, product managers and designers to solve challenging problems
Building new systems to securely store and retrieve large volumes of data for computational purposes
Building new tools for our operational teams
What's in it for you
Projects - Opportunity to work on exciting projects and make an impact on the Canadian broadcast media industry
Work-life balance - In addition to a flexible summer hours program, we offer paid days off during the holiday closure between Christmas and New Year’s Day
Competitive benefits package and group savings and retirement program
Education reimbursement - For those wishing to pursue additional professional development, funding of up to $1000 per year is available through our education reimbursement program
Currently working in a remote environment
Values
Stronger together, Strive for better, Always learn, Be passionate
NLogic is an equal opportunity employer
We are committed to inclusive, barrier-free recruitment and selection processes and work environments. If you are contacted for a job opportunity, please advise the People and Culture department if any accommodations are needed to ensure you have access to a fair and equitable process. Any information received relating to accommodation will be addressed confidentially.
Job Types: Full-time, Permanent
Additional pay:
Bonus pay
Benefits:
Casual dress
Company events
Company pension
Dental care
Employee assistance program
Extended health care
Flexible schedule
On-site parking
Paid time off
RRSP match
Tuition reimbursement
Vision care
Wellness program
Work from home
Schedule:
8 hour shift
Monday to Friday
Experience:
C#: 5 years (Required)
Azure: 1 year (Preferred)
Work remotely:
Yes, temporarily due to COVID-19"
Data Engineer,"Toronto, ON",theScore Inc.,None,Organic,"Score Media and Gaming Inc. empowers millions of sports fans through its digital media and sports betting products. Its media app ‘theScore’ is one of the most popular in North America, delivering fans highly-personalized live scores, news, stats, and betting information from their favorite teams, leagues, and players. The Company’s sports betting app ‘theScore Bet’ delivers an immersive and holistic mobile sports betting experience and is currently available to place wagers in New Jersey, Colorado, and Indiana. Publicly traded on the Toronto Stock Exchange (SCR), theScore also creates and distributes innovative digital content through its web, social and esports platforms.
We are looking for a talented and passionate Data Engineer to help build and measure the performance and popularity of our apps. You will be working alongside smart, friendly, and dedicated developers along with a first rate analytics team in one of Toronto’s top development shops.
Requirements:
A solid foundation in computer science, with strong competencies in data structures, distributed systems, algorithms and software design
Experience building out a scalable infrastructure to fit the needs of a growing company
Experience in data ingestion and processing
Experience with testing frameworks such as RSpec, Jest, pytest or equivalent.
Experience in SQL and other Relational Databases
Experience with Python
Excellent communication skills
Other duties as required
Desired Skills:
Been part of a data driven team
Good knowledge of Math (e.g. data modelling/applied statistics)
Experience with Docker and Kubernetes
Experience with data ingestion APIs (e.g. Facebook, Google)
Experience with Airflow
Experience with RedShift, Snowflake or other SQL-based data warehouses
What We Offer:
Competitive salary with Employee Share Purchase Plan
Comprehensive Benefits package
Fun, relaxed work environment
Located downtown Toronto; easily accessible by public transit
A/V club, Friday presentations, book library, and more
Snacks and drinks provided
Awesome patio with BBQ
Top of the line office hardware.
Parental leave top up.
Opportunity for career progression and mentoring others.
Games room (bring your A game for our FIFA 20, arcade, ping pong and foosball tournaments!)
theScore is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability or age."
Data Engineer,"Toronto, ON",Upchain,None,Organic,"Are you the next Data Engineer we’re looking for?

Who we are
Founded in 2017, Upchain (www.upchain.com) provides Product Lifecycle Management (PLM) solutions on the Cloud. We fuel innovation and simplify product development by connecting sales, engineering, manufacturing, supply chain, service, and others at all points in the product lifecycle. Empowering an impressive list of customers to make better products faster, our easy-to-use platform enables real-time collaborations, establishes the digital thread, manages changes, facilitates viewing of CAD files in 3D, and captures feedback and business processes from any stakeholder throughout the product value chain.

Headquartered in Toronto with three additional offices in Europe, our 85+ employee company was named one of Deloitte Technology’s Fast 50™ Companies-to-Watch last year for exhibiting strong growth as well as recognized by the Lazaridis ScaleUp Program as one of Canada’s Top 10 growth-stage start-ups.

You will enjoy working with us if:
You are suited to work in a young and evolving environment where there is significant wisdom and years of industry knowledge at the helm.
You are excited to play a part in helping the company realize its commitment to strengthening culture, communication, procedures, and leadership.
You are smart, creative, love taking ownership, and can work efficiently and autonomously.
You want to gain lots of experience, learn about the industry, and advance your career through personal and professional development supported by experienced mentors.
You are self-motivated, independent, confident, accountable, and comfortable exercising initiative and solid judgement.
You are keen to learn new technologies in an inclusive and diverse work environment.
You like challenges, being a trailblazer, shaping the future, and working with some of the world’s most innovative and iconic organizations.

Position profile
Reporting to the head of the implementation team, you will work in a modern office conveniently located in one of Toronto’s most popular downtown areas and support data analysis, architecture, and migration for PLM/PDM projects with large enterprise clients, many of them household names. In this full-time role, you would enjoy competitive pay, performance bonus, stock options, vacation, flexible/work from home options, and team building activities.

Key responsibilities include:
Working both independently and at times in collaboration with client staff to perform configuration, setup, migration, training, troubleshooting, or other hands-on technical tasks.
Analyzing customer's software architecture with a focus on data containers (database, file system, etc.) and their associated business and IT processes.
Creating detailed documents outlining the logical data mapping between source systems and the Upchain PLM platform.
Devising a logical data migration flow that takes into account business requirements, business rules, and system technical constraints.
Assisting in developing interfaces with downstream systems for data migration.
Creating data migration protocols and/or tools.
Running data migration proofs of concept (POCs) in an iterative fashion, refining procedures and tools as needed.
Assisting in executing production data migration steps in conjuction with other team members.
Interfacing with product managers and software development team members through established processes and procedures to get help with technical issues that arise on your project, and/or communicate status, information, and updates.
Assisting with data manipulation and transfer to ensure appropriate client data is available in the Upchain PLM.
Identifying and addressing required changes, scope confusion, risks, and/or misalignments in expectations/understandings, and working with client and internal staff to resolve them.
Proactively following up on internal and client-side tasks and dependencies and escalating as needed.
Using internal tools (i.e. ticketing system, internal messaging platforms, etc.) and established procedures to coordinate with other Upchain staff on project tasks.
Practicing rigor and discipline in following Upchain procedures while proactively contributing to quality and the improvement of procedures and processes to benefit Upchain and its teams and clients.
Occasionally collaborating in the pre-sales process to contribute to the understanding of requirements, scope and/or project plans.
Applying your continually improving understanding of knowledge of Upchain’s software, PLM/PDM practices, and the industry in general.

Qualifications
You may be our ideal team member if you are:

An active listener and tactful communicator with the strong interpersonal skills needed to work with various stakeholders, eliminate ambiguity, manage expectations, and tactfully push when needed.
A multitasker capable of prioritizing and managing your time and multiple priorities simultaneously in a fast-paced, time-sensitive, results-oriented environment.
Empathetic and able to understand issues from different perspectives and accurately articulate client goals, needs, difficulties, and points of view.
Someone who is thoughtful and curious with a tendency to ask ""why"" questions and the drive to apply a broad understanding of your work towards the creation of better solutions to challenges.
A lifelong learner who is naturally drawn to acquiring new skills, learning about new subjects, and sharing knowledge with others who are interested.
Experienced in business analysis/ business systems analysis and data modeling with complex enterprise software.
Comfortable speaking at a solution architecture level about data structures and data migration.

Requirements
Bachelor's degree in Computer Science, Data Engineering, or other related field (or equivalent experience).
Fluency in English (verbal and written), and good communication skills both spoken and in writing.
Minimum of 2 years of experience working with business analysis, business systems analysis, business intelligence, or data architecture.
Familiarity with one or more popular relational database platforms, such as Microsoft SQL Server, Oracle DBMS, MySQL, etc.
Familiarity with ETL tools.
Working knowledge of SQL.
Preferably, working knowledge of another programming language commonly used for data analysis, such as R or Python.
Proficiency in Microsoft Excel or other spreadsheet applications.
Experience with data visualization/ business intelligence tools such as Tableau, PowerBI, Qlik, SAP BI.
Familiarity with Product Lifecycle Management concepts is an asset but not required.
Do you fit with an innovative, fast-growing software engineering company with a very bright future? Let’s find out. Submit your resume and cover letter today.

Upchain embraces diversity in the workplace and welcomes applications from women, members of visible minorities, Aboriginal peoples, persons with disabilities, and those requiring employment accommodation in accordance with the Ontario Human Rights Code and the Accessibility for Ontarians with Disabilities Act."
Data Engineer,"Mississauga, ON",Dun & Bradstreet,None,Organic,"Why We Work at Dun & Bradstreet
We are at a transformational moment in our company journey - and we’re so excited about it. Each day, we are finding new ways to strengthen our award-winning culture, and to accelerate creativity, innovation and growth. Our purpose is to help customers improve business performance with Dun & Bradstreet’s Data Cloud and Live Business Identity, and we’re wildly passionate and committed to this purpose. So, if you’re looking to make an immediate impact at a company that welcomes bold and diverse thinking, come join us!
Responsibilities of the role
Work with our data and operations team to improve the overall quality of data delivered to customers
Develop data ingestion and mastering processes on new structured and unstructured data sources to synthesize new insight and enhance production database
Identification, design and implementation of process improvements including redesigning current infrastructure/process for greater scalability
Automation of manual processes
Building required infrastructure to support development and production environments using AWS and SQL technologies
Building new analytical processes providing insight to data quality issues and implementation of data improvement solutions
Build processes to fetch data from open data sources, eg web sites
Skills and qualifications
Bachelor’s degree (or better) in information technology, computer science or related field or equivalent work experience
3+ years of experience on large data projects - defining and implementing data analysis and improvement processes in a MS-SQL Server environment
Ability to perform root cause analysis on external and internal processes and data to identify data quality improvement opportunities
Analytical skills associated with working on unstructured datasets
Required technical proficiency - Microsoft SSIS and Sql Server, Python, C#
Additional beneficial experience/skills – Graph database technology, AWS, Sql Server administration, R, machine learning, web technologies
Strong organization skills with attention to detail
Able to work independently and willing to learn new skills through independent learning
#LI-PD1
Dun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law.

We are committed to Equal Employment Opportunity and providing reasonable accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with Dun & Bradstreet and need special assistance or an accommodation to use our website or to apply for a position, please send an e-mail with your request to TalentAcquisitionTeam@dnb.com. Determination on requests for reasonable accommodation are made on a case-by-case basis.
Please note that all Dun & Bradstreet job postings can be found at https://dnb.wd1.myworkdayjobs.com/Careers and all communication from Dun & Bradstreet will come from an email address ending in @dnb.com."
Data Engineer,"Toronto, ON",Score Media Ventures Inc.,None,Organic,"Score Media and Gaming Inc. empowers millions of sports fans through its digital media and sports betting products. Its media app ‘theScore’ is one of the most popular in North America, delivering fans highly-personalized live scores, news, stats, and betting information from their favorite teams, leagues, and players. The Company’s sports betting app ‘theScore Bet’ delivers an immersive and holistic mobile sports betting experience and is currently available to place wagers in New Jersey, Colorado, and Indiana. Publicly traded on the Toronto Stock Exchange (SCR), theScore also creates and distributes innovative digital content through its web, social and esports platforms.
We are looking for a talented and passionate Data Engineer to help build and measure the performance and popularity of our apps. You will be working alongside smart, friendly, and dedicated developers along with a first rate analytics team in one of Toronto’s top development shops.
Requirements:
A solid foundation in computer science, with strong competencies in data structures, distributed systems, algorithms and software design
Experience building out a scalable infrastructure to fit the needs of a growing company
Experience in data ingestion and processing
Experience with testing frameworks such as RSpec, Jest, pytest or equivalent.
Experience in SQL and other Relational Databases
Experience with Python
Excellent communication skills
Other duties as required
Desired Skills:
Been part of a data driven team
Good knowledge of Math (e.g. data modelling/applied statistics)
Experience with Docker and Kubernetes
Experience with data ingestion APIs (e.g. Facebook, Google)
Experience with Airflow
Experience with RedShift, Snowflake or other SQL-based data warehouses
What We Offer:
Competitive salary with Employee Share Purchase Plan
Comprehensive Benefits package
Fun, relaxed work environment
Located downtown Toronto; easily accessible by public transit
A/V club, Friday presentations, book library, and more
Snacks and drinks provided
Awesome patio with BBQ
Top of the line office hardware.
Parental leave top up.
Opportunity for career progression and mentoring others.
Games room (bring your A game for our FIFA 20, arcade, ping pong and foosball tournaments!)
theScore is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability or age."
Cloud Data Engineer,"Vancouver, BC",Appnovation Technologies,None,Organic,"As a member of the Systems Integration team, the Cloud Data Engineer creates data pipelines, big data platforms and data integrations in databases, data warehouses and data lakes, working with various cloud and on premises technologies.
The Systems Integration team at Appnovation provides a variety of services to our teams around the world to help them run efficiently and effectively. We are a small, agile, team that has an impact on all areas of the business and therefore always has new challenges to solve. Some areas we focus on are: reporting, global governance, system integration, process definition, and special projects.
Appnovation prides itself on having an open environment focused on teamwork, innovation and growth giving you the opportunity to guide your own workload and grow your career.
YOU WILL HAVE AN OPPORTUNITY TO:
Ensure data accuracy and availability are maintained
Design, architect, develop, and maintain our next generation data management and analytics platform
Provide ongoing updates as systems are modified, added or removed from the corporate ecosystem
Research and propose the best technology solutions
Gain a broad understanding of business systems and processes to ensure accurate reporting
Scale the data management platform as volumes and requirements evolve
Participate in related areas such as system integration, visualization, system admin, and process improvement
WHO YOU ARE:
Proactive, organized, a problem-solver, collaborative, big picture thinker, meets deadlines
Excellent attention to detail
Balance of being detail-oriented with long-term projects, and also being flexible to resolve ad hoc issues as they arise
Familiar and comfortable with working in a global company
An expert in RDBMS, SQL, ETL/ELT, REST APIs, Big Data, Data Integration, Data Warehousing, Data Governance, Data Lakes and Batch and Streaming Data Pipelines
An expert working with AWS, RedShift, Linux, Windows, Mac, Java, Python
Familiarity with Azure, GCP, Spark, Kafka, Hadoop, Airflow preferred
Strong understanding of data visualization
Strong understanding of business processes, from project management to financial reporting
Strong collaborative and communication skills (written and verbal)
Degree in a related field and 5+ years experience working in a cloud data analytics environment

Thank you for your interest in a career with Appnovation Technologies! Please note that only those selected for an interview will be contacted.
Appnovation is an equal opportunity employer and committed to diversity and inclusion. We encourage applications from all qualified candidates and accommodations are available upon request throughout the recruitment process."
Data Engineer - Machine Learning,"Vancouver, BC",Prenuvo Inc,None,Organic,"Prenuvo Inc. (dba Voxelwise Imaging Technologies Inc.)

Overview
We are looking for a high-calibre, driven, technically-skilled software engineer to join what we expect to be the most rewarding role in your career. Prenuvo is literally transforming and reshaping preventative health by rolling out a data/ML heavy advanced MRI based screen that catches cancer at stage 1 and 400+ other diseases.
The data engineer establishes and maintains scalable, reliable, and efficient data pipelines for data inquiry, model implementation & monitoring. The ideal candidate will have experience with big data pipeline and will be able to detect our pipeline bottlenecks and offer solutions.
Required Skills
ETL development and data warehousing
Optimization of data structure, data pipeline, pipeline architecture in AWS/Azure/Google Cloud and tooling Spark/Airflow or similar
Strong knowledge of event-based and near real-time analytics using tools such as Kafka, Kinesis, SQS/SNS and Elasticsearch.
Strong command of back-end tech stack, strong proficiency in databases for relational (SQL) and non-relational (NoSQL), and frameworks such as Flask/FastAPI
Familiarity with model development process using Machine Learning algorithms
Qualifications
BS / MS in Computer Science, fields of science, or an Engineering discipline
2+ years experience as a software or data engineer at an enterprise software/analytics company
2+ years experience with relational databases (PostgreSQL preferred) and SQL, including strong understanding of concepts
Strong understanding of algorithms with 3+ years of experience in Python and/or Java
Good communication skills - both written and verbal
Knowledge of medical data/images is a plus
Knowledge of statistics or a background in life-science is a plus
Quick learner, and a strong team player
What’s in it for you
You’ll be part of a close-knit team that’s purpose is to solve complex problems in creative ways.
Work in an environment where it is safe to experiment, challenge yourself and others, fail, and learn.
Build systems and products that make a meaningful impact on the lives of people and their health.
Get a competitive salary with an equity stake
Extended health benefits including comprehensive body scans.

The position is based out of Vancouver, BC. Most of the team is working remotely these days. Local Candidates are strongly preferred.
Powered by JazzHR
9zctr5LfBx"
Senior BI Data Engineer,"Richmond Hill, ON",Paymentus,None,Organic,"Summary/Objective
The BI Data Engineer will assist in growing our monitoring and BI infrastructure. The individual will be responsible for supporting the stack 24/7 and working with the OPS team. An ideal candidate has a strong desire to learn new things, and the capability to “learn-as-you-go”. A passion for applying innovative technology in combination with excellent problem solving and communication skills will make a successful BI Data Engineer.
Essential Functions/ Responsibilities * Build and maintain batch and real-time data pipelines to power our BI and operational reports and dashboards
Generate graphs and reports in Grafana for business and operational needs.
Create Kapacitor alarms for business and operational events
Support the stack 24/7
Identify, design, and implement internal process improvements
Assist stakeholders with data-related technical issues and support data infrastructure needs
Work with data and analytics experts to strive for greater functionality in our data systems
Supervisory Responsibility
This position does not have any supervisory responsibility or direct reports.
Education and Experience
1. A bachelor's degree in Software Engineering, Computer Science, or related technical degree
2. 6+ years of experience in software development or DevOps engineering
3. Basic understanding of time series and relational databases (Oracle, InfluxDB)
4. Proficiency in bash scripting or Python; SQL; Unix/Linux
5. Experience as a successful problem solver and communicator
6. Preferred but not required:
a. Prior knowledge with InfluxDB stack is plus
b. Knowledge of at least one programming language like Scala, Java or Python is plus.
c. Experienced in Apache Spark, or other big data processing frameworks
d. Software development and source code management (Git etc)
Work Environment
This job operates in a professional office environment. This role routinely uses standard office equipment such as laptop computers, photocopiers and smartphones.
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
Specific vision abilities required by this job include close vision and ability to adjust focus. Prolonged periods sitting at a desk and working on a computer. Must be able to lift up to 15 pounds at times.
Position Type/Expected Hours of Work
This is a full-time position. Days and hours of work are Monday through Friday, during normal business hours. Occasional evening and weekend work may be required as job duties demand.
Travel
Little to no travel is expected for this position.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.
EEO Statement
Paymentus is an equal opportunity employer. We enthusiastically accept our responsibility to make employment decisions without regard to actual or perceived race, creed, color, age, sex or gender (including pregnancy, childbirth and related medical conditions), gender identity or gender expression (including transgender status), sexual orientation, national origin, ancestry, citizenship status, religion, marital status, physical or mental disability, military service or veteran status, genetic information, protected medical condition as defined by applicable state or local law, genetic information, or any other classification protected by applicable federal, state, and local laws and ordinances. Our management is dedicated to ensuring the fulfillment of this policy with respect to hiring, placement, promotion, transfer, demotion, layoff, termination, recruitment advertising, pay, and other forms of compensation, training, and general treatment during employment.
Reasonable Accommodation
Paymentus recognizes and supports its obligation to endeavor to accommodate job applicants and employees with known physical or mental disabilities who are able to perform the essential functions of the position, with or without reasonable accommodation. Paymentus will endeavor to provide reasonable accommodations to otherwise qualified job applicants and employees with known physical or mental disabilities, unless doing so would impose an undue hardship on the Company or pose a direct threat of substantial harm to the employee or others.
An applicant or employee who believes he or she needs a reasonable accommodation of a disability should discuss the need for possible accommodation with the Human Resources Department, or his or her direct supervisor.
Job Types: Full-time, Permanent
Pay: $0.00 per year
Benefits:
Casual dress
Company events
Dental care
Disability insurance
Employee assistance program
Extended health care
Life insurance
Paid time off
Vision care
Wellness program
Schedule:
Monday to Friday
Experience:
software development or DevOps engineering: 6 years (Preferred)
Work remotely:
No"
Sr. Big Data Engineer,"Toronto, ON",Northbay,None,Organic,"NorthBay Solutions, an 8+ Year Award Winning AWS Premier Consulting Partner, is an Global Analytics, DB/App Migration, App Dev and DevOps professional services company. NorthBay's intense focus is unparalleled in the areas of Big Data (Data Lakes, Data Platforms, Enterprise Data Warehouse), Database & Application Migrations and Machine Learning/AI. NorthBay has earned the Analytics, DevOps, Mobile; Education Competency and is also a Public Sector Partner and we are also certified for AWS Well Architected Reviews and Database Freedom Migrations.

NorthBay Solutions is looking for Talented Big Data Engineers to join their team in Halifax. We are seeking technically savvy Senior Big Data Engineer to implement solutions for our customers working with our offshore engineering team. In this role, you will collaborate with NorthBay customers, some working onsite, understand requirements and needs, translate into specifications to develop solutions, drive work with offshore engineering teams, and deliver solutions and results to the customer. This includes assessing customer needs, re-engineering business intelligence processes, designing and developing data models, and sharing your expertise throughout the deployment process.
Responsibilities:
Possess In depth knowledge and hands on development experience in building Distributed Big Data Solutions including ingestion, caching, processing, consumption, logging & monitoring) (Must Have)
Strong Development Experience in either one of the Distributed Big Data processing (bulk) engines preferably using Spark on EMR or related (Must Have)
Strong Development Experience on at least one or more event driven streaming platforms prefer Kinesis, Firehose, Kafka or related (Must Have)
Strong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Airflow or related (Must Have)
Assess use cases for various teams within the client company and evaluate pros and cons and justify recommended tooling and component solution options using AWS native services, 3rd party and open source solutions (Must Have)
Strong experience on either one or more MPP Data Warehouse Platforms prefer AWS RedShift, PostgreSQL, Teradata or similar (Must Have)
Strong understanding and experience with Cloud Storage infrastructure and operationalizing AWS based storage services & solutions prefer S3 or related (Must Have)
Strong technical communication skills and ability to engage a variety of business and technical audiences explaining features, metrics of Big Data technologies based on experience with previous solutions (Must Have)
Strong Data Cataloging experience preferably using AWS Glue (Nice to Have)
Strong Development Experience on at least one NoSQL OR Document databases (Nice to Have)
Experience on at least one or More Ingestion Integration tools Like Apache NIFI or Streamset or related (Nice to Have)
Strong Development Experience on at least one Caching Tools like Redis, Lucene, Memcached (Nice to Have)
Strong Understanding and experience in Big Data Audit Logging and Monitoring solutions (Nice to Have)
Strong Understanding of at least one or more Cluster Managers (Yarn, Hive, Pig, etc) (Nice to Have) Interface with client project sponsors to gather, assess and interpret client needs and requirements
Advising on database performance, altering the ETL process, providing SQL transformations, discussing API integration, and deriving business and technical KPIs
Develop a data model around stated use cases to capture client’s KPIs and data transformations
Assess, document and translate goals, objectives, problem statements, etc. to our offshore team and onshore management
Document and communicate product feedback in order to improve user experience
Qualifications:
5+ years of AWS Solutions implementation, professional services experience, prefer Data Analytics space.
A passion for exploring data and extracting valuable insights.
Proven analytical, problem solving, and troubleshooting expertise.
Proficiency in SQL, preferably across a number of dialects (we commonly write MySQL, PostgreSQL, Redshift, SQL Server, and Oracle).
Exposure to developer tools/workflow (e.g., git/github, *nix, SSH)
Experience optimizing database/query performance.
Experience with AWS ecosystem (EC2, S3, RDS, Redshift).
Experience with business intelligence tools with a physical model (e.g., MicroStrategy, Business Objects, Cognos).
Experience with data warehousing.
Exposure to NoSQL-based, SQL-like technologies (e.g., Hive, Pig, Spark SQL/Shark, Impala, BigQuery)
Excellent verbal and written communication skills
Ability to travel up to 25% post-Covid"
"Azure Data Engineer, Omnia AI, Toronto","Toronto, ON",Deloitte,None,Organic,"Job Type: Permanent
Primary Location: Toronto, Ontario, Canada
All Available Locations: Calgary; Montreal; Toronto; Vancouver

Be encouraged to deepen your technical skills…whatever those may be.
Be expected to share your ideas and to make them a reality.
Be part of a firm where you are valued for your unique strengths and where everyone feels a sense of belonging.

Do you dream about data? Do you speak SQL as well as your mother language? Are you motivated by solving complex problems by leveraging best-in-class data pipelines? We want to hear from you!
What will your typical day look like?

Everyday you will work with our clients to solve their toughest problems in data engineering, data acquisition, and data standardization. You will do so by leveraging leading technology to accomplish large-scale implementations. You will facilitate design sessions with business stakeholders to define key data definitions, consolidate findings, and work with technology teams to develop appropriate data models. Everyday, you will design and implement optimal data pipeline architecture that is auditable, redundant, scalable and high-performing. Since we know you are a great self-starter, you willm also build the cloud infrastructure and pipeline required for optimal extraction, transformation, and loading of data from a wide variety of data sources (structured and un-structured, streaming and batch).
About the team

Omnia AI, Deloitte's Artificial Intelligence practice is comprised of specialized experts with hands-on experience, and cutting-edge information assets that facilitate successful Artificial Intelligence (AI) transformations. We develop AI-enabled solutions to address all aspects of a client’s transformative journey with disciplined focus on business outcomes.
As a member of our Data & Analytics Modernization team within our Omnia AI practice, you will help our clients design and implement the data platform architectures – be it in the cloud or on-premises – required to enable cutting-edge AI solutions. You will be part of a practice to deliver a breadth of solutions to solve our clients most challenging business problems, with a focus on Big Data, BI/DW, Data Integration, Data Governance, Master Data and Analytics applications. Each of these applications leverages a different mix of traditional and innovative technologies to achieve business outcomes.
Enough about us, let’s talk about you

You are someone who has completed:
At least 2 projects (minimum of 3 month each) hands-on experience with Microsoft Azure data ingestion (demonstrated via hands on project experience)
Describe in detail how you leveraged at least 3 out of these 2 components: Azure Data Factory, Azure Event Hub, Databricks
1+ years of hands-on experience with data ingestion on Azure
3+ years of relevant technology consulting or industry experience in data & analytics delivery
2+ years designing and developing data pipelines, data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
Azure Data Engineer Certification
Why Deloitte?
Launch your career with The One Firm where you can make an impact that matters in a way that you never thought possible. With endless opportunities at every turn, and a culture built to support and develop our people to be the very best they can be, Deloitte is The One Firm for you to learn, grow, create, connect, and lead. We do this by making three commitments to you:
You will lead at every level: We grow the world’s best leaders so you can achieve the impact you seek, faster.
You can work your way: We give you the means to be flexible in how you need and want to work, and we have innovative spaces, arrangements and the mindset to help you be wildly successful.
You will feel included and inspired: We create a deep sense of belonging where you can bring your whole self to work.

The next step is yours
Sound like The One Firm. For You?
At Deloitte we are all about doing business inclusively – that starts with having diverse colleagues of all abilities! We encourage you to connect with us at accessiblecareers@deloitte.ca if you require an accommodation in the recruitment process, or need this job posting in an alternative format. We’d love to hear from you!
By applying to this job you will be assessed against the Deloitte Global Talent Standards. We’ve designed these standards to provide our clients with a consistent and exceptional Deloitte experience globally."
Business Intelligence Data Engineer,"Toronto, ON",Loblaw Digital,None,Organic,"At Loblaw Digital, we know that our customers expect the best from us. Whether that means building the best, most innovative online shopping experience, or designing an app that will impact the lives of people across the country, we’re up for the challenge. From our office in Downtown Toronto, we’ve created leading eCommerce experiences in the online grocery shopping, beauty, pharmacy, and apparel spaces, and we’re only just getting started.

The impact you’ll make
As a Business Intelligence Data Engineer, you will impact our business by fueling the best in class data analytic and reporting platform at Loblaw Digital that will empower our business users in making data driven decisions and strategies in their everyday work.
We’re growing our BI team to better serve the enterprise data strategy and want you to be a part of it. You’ll work with users across the organization to understand how data helps them and translate that into solutions that will bring data from various systems into our data warehouse. We’re looking to turn our enviable wealth of data into actionable insights and meaningful recommendations that drive growth and improved engagement across all of our lines of business, and a customer-focused mindset is a must-have.
What you'll do
Develop and scale quality and performant data pipelines using GCP (Big Query, Compute Engine, Kubernetes, Cloud Composer, Dataflow) and other open source technologies
Work with stakeholders to develop solutions that support various data consumers
Work with DevOps on setting up environment for new deployment
Build and implement robust data models and semantic layer
Be a subject matter expert on the entire BI framework stack
Support reporting and analytics needs from all areas of the business. For example: Developers, Product Managers, Product Designers, Merchandizers, Marketers, and Operations Analysts
Help us in our mission to be a hypotheses-driven product development organization which means data is at the center of everything we do
Does this Sound like you?
BA/BS in computer science, engineering or related technical field
Hands-on experience with relational databases (e.g. Oracle, MySQL, DB2, etc.)
Python experience coding ETLs using various packages/connectors to databases and endpoints (or Java)
Working experience with Git and CI/CD pipelines
Experience with data pipeline automation using a code focused scheduling tool (e.g. Airflow)
Exposure to Kubernetes, docker, and streaming tools (Kafka)
Experience working with sourcing and feeding data from REST APIs is nice to have
Understanding of data warehouse concept and BI architecture is a plus
Strong oral and written communication skills
Solid stakeholder management skills
Proven team player that will work well in a dynamic and fast-paced environment
Experience working in an agile environment
Experience in retail or ecommerce is a plus
Strong desire to learn and to push the boundaries of what can be done to better empower our business with data
How you’ll succeed
At Loblaw Digital, we seek great people to continually strengthen our culture. We believe great people model our values, are authentic, build trust and make connections. We’re able to keep innovating because our colleagues are passionate about their work and excited about the future of eCommerce. You will get to work with some of the best digital minds and will have the support of world class technologies to craft products our customers will love!

Loblaw Digital recognizes Canada's diversity as a source of national pride and strength. We have made it a priority to reflect our nation’s evolving diversity in the products we sell, the people we hire, and the culture we create in our organization. Accommodation is available upon request for applicants with disabilities in the recruitment and assessment process and when hired. In addition, we believe that compliance with laws is about doing the right thing. Upholding the law is part of our Code of Conduct – it reinforces what our customers and stakeholders expect of us."
"Software Engineer, Data","Toronto, ON",Fathom,None,Organic,"We're on a mission to understand and structure the world's medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the world's largest health systems.
We're seeking exceptional Data Engineers to work on data products that drive the core of our business-a backend expert able to unify data, and build systems that scale from both an operational and an organizational perspective.
As a Data Engineer you will:
Develop data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs.
Build performant and expressive interfaces to the data
Build infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning
We're looking for teammates who bring:
2+ years of development experience in a company/production setting
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
Excitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark. You've not only worked with a variety of technologies, but know how to pick the right tool for the job.
A unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.
Bonus points if you have experience with:
Developing systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python's NLTK.
Expertise with wrangling healthcare data and/or HIPAA.
Experience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive."
Data Engineer Analyst (Cloud) / Analyste ingénieur(e) de don...,"Montréal, QC",Slalom Consulting,None,Organic,"*English will follow*
Slalom est une société de conseil moderne axée sur la transformation de la stratégie, de la technologie et des activités. Avec notre mentalité axée sur les objectifs, nous travaillons en collaboration avec des entreprises pour repousser ensemble les limites de ce qui est possible.
Chez Slalom, la connexion personnelle rencontre l’échelle mondiale. Nous établissons des relations étroites avec les clients au sein de nos marchés et à l’échelle mondiale, en faisant circuler nos connaissances dans tous les marchés afin que chaque engagement puisse bénéficier de toute l’étendue de l’expertise de Slalom. Nos sept centres régionaux Build agissent comme points centraux de l’innovation pour attirer des talents de haut niveau qui collaboreront rapidement à la création des produits technologiques de demain. Nous entretenons également de solides partenariats avec plus de 200 fournisseurs technologiques de premier plan, notamment Amazon Web Services, Google Cloud, Microsoft et Salesforce.
Fondée en 2001, Slalom a établi son siège social à Seattle et, selon un mode de développement par croissance interne, compte maintenant plus de 8 500 employés. Nous figurons en 2020 sur la liste des 100 meilleurs employeurs établie par le magazine Fortune pour la cinquième année consécutive et nous sommes régulièrement mentionnés par nos employés comme l’un des meilleurs environnements de travail. En savoir plus : https://www.slalom.com/fr-ca/?lp=1.
Au Canada depuis 2015, Slalom compte maintenant plus de 500 employés répartis dans trois marchés : Vancouver, Toronto et Montréal.

L’équipe Slalom de Montréal recherche un(e) analyste ingénieur(e) de données en technologie infonuagique pour notre pratique d’analytiques des données. À titre d’analyste, vous aller développer vos compétences pour analyser, concevoir et construire des solutions infonuagiques pour répondre aux besoins de nos clients en matière d’infrastructure sous forme de service, de plateforme sous forme de service et de logiciel sous forme de service. Nous cherchons une personne récemment diplômé(e) avec une base technique solide pour apprendre et grandir dans notre équipe. Dans le cadre de ce poste, vous aller apprendre comment utiliser des outils d’architecture de données modernes, notamment en nuage (AWS/Azure/GCP), Hadoop, Spark, Kafka et d’autres technologies liées aux mégadonnées. En plus de bâtir la prochaine génération de plateformes de données, vous travaillerez avec certaines des organisations les plus innovatrices en analytiques des données. Nous sommes à la recherche de personnes vives d’esprit, disciplinées et motivées qui sont passionnées par l’utilisation de solutions infonuagiques pour résoudre des problèmes d’entreprise réels. Cette personne sera soutenue par Slalom pour la formation, les certifications, et l’apprentissage des technologies modernes infonuagiques.

Vous allez :
Travailler au sein d’une équipe sur la conception et le développement de solutions infonuagiques de données
Établir les exigences techniques, évaluer les capacités des clients et analyser les résultats afin de fournir des recommandations appropriées en matière de solutions infonuagiques et de stratégies d’adoption
Apprendre à définir les stratégies infonuagiques de données, y compris la conception de feuilles de route de mise en œuvre à plusieurs phases
Soutenir l’analyse, la construction, la conception et le développement d’entrepôts de données et de solutions d’intelligence d’affaires
Se familiariser avec les solutions infonuagiques, l’architecture et les technologies Azure, ainsi que leurs interdépendances
Se familiariser avec ETL les entrepôts de données, l’intégration de données, le profilage de données et la visualisation de données (PowerBI, Tableau…)
SQL et de débogage de problèmes complexes
Rechercher, analyser, recommander et sélectionner des approches techniques pour résoudre des problèmes de développement et d’intégration difficiles et stimulants
Découvrir et adopter de nouveaux outils et de nouvelles techniques afin d’accroître la performance, l’automatisation et l’évolutivité
Aider les équipes de développement des affaires à exécuter les activités de prévente et les demandes de proposition
Comprendre les objectifs et déclencheurs d’affaires, et les traduire en une solution technique appropriée
Compétences requises :
Solides capacités analytiques de résolution de problèmes
Personne autonome ayant la capacité de travailler de façon indépendante ou au sein d’une équipe de projet
Maitrise (MSc.) en sciences informatiques, mathématiques, ingénierie, ou en un domaine connexe ou une expérience professionnelle équivalente
Un état d’esprit axé sur la croissance démontré, et un enthousiasme à l’idée de découvrir de nouvelles technologies rapidement et de mettre en pratique les connaissances acquises pour résoudre les problèmes opérationnels
Une capacité de présenter des solutions avec assurance et de communiquer clairement avec les clients sur le plan commercial et de transformer leurs besoins en solution technique
Bilingue, expert en communication en français et en anglais (oral et écrit)
Compréhension des écosystèmes infonuagiques et des technologies infonuagiques émergentes et dernier cri
Slalom est un employeur qui offre l’égalité des chances, et toutes les candidatures qualifiées seront étudiées pour la dotation du poste, sans égard à la race, à la couleur, à la religion, au sexe, aux origines nationales, au statut d’ancien combattant ou toute autre caractéristique visée par la loi.

Slalom is a modern consulting firm focused on strategy, technology, and business transformation. With our purpose-driven mindset, partner with companies to push the boundaries of what’s possible—together.
At Slalom, personal connection meets global scale. We build deep relationships with our clients within our markets and across the globe, while sharing insights across markets to bring the full breadth of Slalom's expertise to every engagement. Our seven regional Build Centers are hubs for innovation, attracting top talent to rapidly co-create the technology products of tomorrow. We also nurture strong partnerships with over 200 leading technology providers, including Amazon Web Services, Google Cloud, Microsoft, and Salesforce.
Founded in 2001 and headquartered in Seattle, Slalom has organically grown to over 8,500 employees. We were named one of Fortune's 100 Best Companies to Work For in 2020 for the 5th year in a row and are regularly recognized by our employees as a best place to work. Learn more at slalom.com.
Slalom in Canada began in 2015 and has grown to over 500 employees across 3 markets – Vancouver, Toronto, and Montréal.

Slalom in Montreal is hiring a Data Engineering Analyst for Cloud Technology to join our Data and Analytics practice. As an Analyst on our team, you will build your skills to analyze, design and architect cloud-based solutions to address our clients’ needs for infrastructure-as-a-service, platform-as-a-service, and software-as-a-service. We are looking for a recent graduate with a solid technical foundation to learn and grow with our team. In this role, you will learn to use modern data architecture tools, including cloud (AWS/Azure/GCP), Hadoop, Spark, Kafka and other Big Data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics. We are looking for sharp, disciplined, and self-motivated individuals who have a passion for utilizing the cloud solutions to solve real business problems for our customers. They will be supported by the Slalom Montreal team for trainings, certifications, and learning for modern technology in the cloud.

Responsibilities:
Work as part of a team, to design and develop cloud data solutions
Gather technical requirements, assess client capabilities and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Learn to define Cloud Data strategies, including designing multi-phased implementation roadmaps
Support analysis, architecture, design, and development of data warehouse and business intelligence solutions
Become versed in cloud solutions, architecture, related technologies and their inter-dependencies
Become versed with ETL, data warehousing, data ingestion, data profiling and data visualization (PowerBI, Tableau…)
SQL and debugging complex queries
Research, analyze, recommend and select technical approaches for solving difficult and challenging development and integration problems
Learn and adopt new tools and techniques to increase performance, automation, and scalability
Understand business goals and drivers and translate those into an appropriate technical solution
Qualifications:
Strong analytical problem-solving ability
Self-starter with the ability to work independently or as part of a project team
Experience in languages such as Python, Java, or Go
Masters Degree in Computer Science, Engineering, Mathematics, or related fields or commensurate work experience
A demonstrated growth mindset, enthusiastic about learning new technologies quickly and applying the gained knowledge to address business problems
Ability to confidently present and communicate clearly with clients on a business level and translate their needs into a technical solution
Bilingual expert communication in English & French (verbal and written)
Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
Design Specialist II – DevOps Data Engineer,"Scarborough, ON",TELUS,None,Organic,"Make your mark on the leading edge of next generation wireless networks and technologies!

Join our team

The IMS Services and Optimization DevOps team drives strategy, development and operations of TELUS Mobility current and next-generation IP Multimedia Subsystem Core Networks, and emerging 5G Voice Network, providing Voice and multimedia services nationally on TELUS’ state-of-the-art access networks (including 4G/5G, GPON and others). The team designs, integrates and operates multiple mass-scale mission-critical service platforms, including VoLTE (Voice over LTE) and VoWiFi (Voice over WiFi), serving millions of TELUS subscribers across the country. Our team members are empowered to initiate and innovate new services and technologies, performing Proof of Concept builds, translating ideas into business cases and into real-world products. We leverage a broad range of technologies across virtualization, automation, AI, Data Analytics and Engineering, as we continue to optimize service quality, reliability and network resiliency.

Here’s the impact you’ll make and what we’ll accomplish together

As a DevOps Data Engineer, you will be an integral part of the IMS DevOps team as we complete the virtualization of the core network and leverage the all-IP build to drive innovation: developing new services, improving existing flows and services. Your creativity and core technical expertise in Data Science and Engineering, coupled with your passion for application development, leveraging and embracing open frameworks, Data tools, data mining, building and implementing models, using and creating algorithms and simulations, will allow us to drive our business and services portfolio for the Consumer and SMB markets. You will exercise your entrepreneurial spirit and innovative vision to change the way TELUS leverages data technology and to enrich our 5G network strategy and evolution. The successful candidate will have a passion for investigating and discovering solutions hidden in large data sets, working with wide range of stakeholders to improve outcomes.
Transforming the network landscape is our passion, and a leading factor in why customers continue to recognize TELUS as the fastest and most reliable network in Canada.

Here’s how
Assess the accuracy and effectiveness of data sources and ingestion processes
Mine and analyze data sets from network component KPIs, design and build data visualization and trending report
Develop custom algorithm and traffic models to apply to KPI data sets
Use predictive modeling (AI) to assist team decision and strategy decisions
Analytics experience and expertise in SPLUNK admin and development
Proven experience in converting different data sources (e.g. Nodal performance counters, billing records, Network data collection tools output) into automated KPI visualization dashboard creation and implement Performance based alerts on KPI deviation using Splunk and other tools
Hadoop integration and Tableau dashboard creation using multiple data sources
Analyze new service categories and products for technical feasibility and operational practicality, from end to end system view
Collaborate with Product and Business Development teams to find the right balance between product and network evolution
Translate Business requirements into deployable and solid technical solutions and vice-versa: leverage state of the art technology to build lab and production demos and trials
Ensure the balance between cost, delivery timelines and business/architecture compliance is achieved across all program phases and across all delivery organizations
Lead and participate in the creation of detailed documentation; design, planning and implementation
Work with other teams, vendors as applicable, to understand and resolve technical issues impacting IMS-based services and drive the issues all the way to resolution
Support Operations, Performance and Capacity Planning for the platforms in the team scope
Research and share the state of the art concepts in 5G data analytics and workflow improvement tools – DevOps toolchains, Continuous Integration, Continuous Delivery (CI/CD)
Participate in Service and Operational optimization initiatives, working together with other teams within the company
Explore leading trends and developments in technology, service and customer expectations, identifying evolving opportunities for TELUS, by participating in industry standards bodies, technology forums and relationships with telecom providers
Qualifications
You’re the missing piece of the puzzle
Bachelor's degree in Telecommunications, Computer Science, Electrical Engineering or related discipline
Master’s degree an asset
Seasoned technical leader with 5+ years of experience in leading complex solutions development and implementations in a Data Engineering/Scientist capacity, coupled with wireless communication network knowledge in IMS and Enhanced Packet Core
Data Analytics and modeling coupled with Splunk and Tableau expertise in visualization and administration
Strong Linux operating systems and open cloud computing platforms background: RedHat, CentOS, Kubernetes, OpenShift, OpenStack
Recognized for your knowledge of 3GPP, ETSI, IEEE and other industry standards associations
Good knowledge VoLTE/VoWiFI/VoNR Call Flow and 3GPP Protocols including GTP, SIP, Diameter
Skilled in programming/scripting languages such as Python, C/C++, Java, SQL Rust, NodeJS; solid understanding of DevOps tools and application/function deployment platforms and tools
Knowledge of cloud computing concepts and NFV
Must have technical and operating knowledge, practice and insight to identify and assess technical conflicts, generate feasibility analysis and optimize solution choices
Ability to learn new technologies, applications and solutions
Strong communication skills to understand and be understood and able to articulate feasibility, choices and risks
Adaptability to change and leading change adoption
Great-to-haves
Development (coding) background in a DevOps environment, server and/or client side
Mobile application development frameworks: experience on the basics of Android/IOS app dev workflows
Virtualization with QEMU/KVM, Kubernetes, Ansible, Jenkins/Travis, Git; ELK Stack
Professional certifications: Red Hat (RHCE), OpenShift/OpenStack, Voice/VoIP, Routing and Switching"
Sr. Big Data Engineer,"Halifax, NS",Northbay,None,Organic,"NorthBay Solutions, an 8+ Year Award Winning AWS Premier Consulting Partner, is an Global Analytics, DB/App Migration, App Dev and DevOps professional services company. NorthBay's intense focus is unparalleled in the areas of Big Data (Data Lakes, Data Platforms, Enterprise Data Warehouse), Database & Application Migrations and Machine Learning/AI. NorthBay has earned the Analytics, DevOps, Mobile; Education Competency and is also a Public Sector Partner and we are also certified for AWS Well Architected Reviews and Database Freedom Migrations.

NorthBay Solutions is looking for Talented Big Data Engineers to join their team in Halifax. We are seeking technically savvy Senior Big Data Engineer to implement solutions for our customers working with our offshore engineering team. In this role, you will collaborate with NorthBay customers, some working onsite, understand requirements and needs, translate into specifications to develop solutions, drive work with offshore engineering teams, and deliver solutions and results to the customer. This includes assessing customer needs, re-engineering business intelligence processes, designing and developing data models, and sharing your expertise throughout the deployment process.
Responsibilities:
Possess In depth knowledge and hands on development experience in building Distributed Big Data Solutions including ingestion, caching, processing, consumption, logging & monitoring) (Must Have)
Strong Development Experience in either one of the Distributed Big Data processing (bulk) engines preferably using Spark on EMR or related (Must Have)
Strong Development Experience on at least one or more event driven streaming platforms prefer Kinesis, Firehose, Kafka or related (Must Have)
Strong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Airflow or related (Must Have)
Assess use cases for various teams within the client company and evaluate pros and cons and justify recommended tooling and component solution options using AWS native services, 3rd party and open source solutions (Must Have)
Strong experience on either one or more MPP Data Warehouse Platforms prefer AWS RedShift, PostgreSQL, Teradata or similar (Must Have)
Strong understanding and experience with Cloud Storage infrastructure and operationalizing AWS based storage services & solutions prefer S3 or related (Must Have)
Strong technical communication skills and ability to engage a variety of business and technical audiences explaining features, metrics of Big Data technologies based on experience with previous solutions (Must Have)
Strong Data Cataloging experience preferably using AWS Glue (Nice to Have)
Strong Development Experience on at least one NoSQL OR Document databases (Nice to Have)
Experience on at least one or More Ingestion Integration tools Like Apache NIFI or Streamset or related (Nice to Have)
Strong Development Experience on at least one Caching Tools like Redis, Lucene, Memcached (Nice to Have)
Strong Understanding and experience in Big Data Audit Logging and Monitoring solutions (Nice to Have)
Strong Understanding of at least one or more Cluster Managers (Yarn, Hive, Pig, etc) (Nice to Have) Interface with client project sponsors to gather, assess and interpret client needs and requirements
Advising on database performance, altering the ETL process, providing SQL transformations, discussing API integration, and deriving business and technical KPIs
Develop a data model around stated use cases to capture client’s KPIs and data transformations
Assess, document and translate goals, objectives, problem statements, etc. to our offshore team and onshore management
Document and communicate product feedback in order to improve user experience
Qualifications:
5+ years of AWS Solutions implementation, professional services experience, prefer Data Analytics space.
A passion for exploring data and extracting valuable insights.
Proven analytical, problem solving, and troubleshooting expertise.
Proficiency in SQL, preferably across a number of dialects (we commonly write MySQL, PostgreSQL, Redshift, SQL Server, and Oracle).
Exposure to developer tools/workflow (e.g., git/github, *nix, SSH)
Experience optimizing database/query performance.
Experience with AWS ecosystem (EC2, S3, RDS, Redshift).
Experience with business intelligence tools with a physical model (e.g., MicroStrategy, Business Objects, Cognos).
Experience with data warehousing.
Exposure to NoSQL-based, SQL-like technologies (e.g., Hive, Pig, Spark SQL/Shark, Impala, BigQuery)
Excellent verbal and written communication skills
Ability to travel up to 25% post-Covid"
Ingénieur·e de données /Data Engineer,"Montréal, QC",BusPatrol,None,Organic,"BusPatrouille :
Notre mission est de responsabiliser et de sensibiliser les usagers de la route. Nous nous consacrons à rendre le trajet entre la maison et l'école plus sûr. Nous développons des partenariats, nous déployons une technologie de sécurité et nous gérons tous les aspects du programme. Nous avons déjà équipé des milliers d'autobus scolaires en Amérique du Nord avec notre technologie innovante et nous continuons d'éduquer chaque mois des dizaines de milliers de conducteurs quant à la sécurité routière. La sécurité des élèves est au cœur des préoccupations de BusPatrouille. Nous éduquons les automobilistes jour après jour en aidant à faire respecter la loi et nous travaillons avec les responsables des écoles pour améliorer la sécurité.

Offre d'emploi :
L'ingénieur·e de données de BusPatrouille travaillera à structurer, modéliser et automatiser tous les flux de données dans notre architecture sur AWS, en mettant l'accent sur l'optimisation, l'intégrité et la sécurité. Dans le cadre de ses fonctions, l'ingénieur·e de données travaillera énormément avec les différentes parties prenantes et sera un pivot en ce qui a trait à tous les besoins techniques du service de veille stratégique afin de développer des algorithmes permettant de rendre les données brutes plus utiles à l'entreprise.

Votre rôle :
Créer et tenir à jour une architecture optimale du pipeline de données.
Assembler de grands ensembles de données complexes qui répondent aux exigences tant fonctionnelles que non fonctionnelles de l'entreprise.
Définir, concevoir et mettre en œuvre les améliorations possibles des processus internes : automatisation des processus manuels, optimisation de la transmission des données, reconception de l'infrastructure en vue d'une plus grande évolutivité.
Construire l'infrastructure requise pour optimiser l'extraction, la transformation et le chargement de données provenant d'une grande variété de sources de données en utilisant les technologies de données SQL et AWS.
Assurer la séparation et la sécurité transfrontalières de nos données grâce aux régions AWS.
Créer des outils de données qui aident les membres des équipes scientifiques d'analyse et de données à concevoir et à optimiser nos produits.Travailler avec des spécialistes en matière de données et d'analyse pour obtenir une plus grande fonctionnalité de nos systèmes de données.
Profil recherché :
Baccalauréat ou maîtrise ès sciences en informatique ou dans un domaine technique connexe.
Plus de quatre années d'expérience dans la conception de schémas et la modélisation de données dimensionnelles.
Plus de cinq années de connaissances de SQL pour divers besoins en matière de rapports et de transformation (MySQL, PostgreSQL, Redshift)
Expérience dans la conception et l'optimisation d'architectures de pipelines de données et d'ensembles de données structurées/non structurées.
Expérience pratique d'AWS Glue.Compréhension et connaissances techniques des services d'AWS (EC2, S3).
Aptitude à la communication verbale et non verbale : La communication est cruciale pour travailler avec une équipe et la diriger. Ce poste nécessite de communiquer avec son équipe, ses supérieur·e·s, les parties prenantes et les clients à l'externe. Un·e chef d'équipe communique de manière claire et efficace tout en donnant la direction à suivre.
Compétences organisationnelles : L'organisation est importante lorsque plusieurs membres d'une équipe travaillent sur un même projet. De solides compétences organisationnelles vous aideront à suivre la progression des projets et à entretenir la motivation des membres de l'équipe.
Capacité à déléguer : Les chefs d'équipe doivent déléguer des tâches à chaque membre de l'équipe. Pour cela, les chefs d'équipe doivent avoir confiance dans les capacités de l'équipe.
Intégrité : Les chefs d'équipe doivent donner l'exemple. Un·e chef d'équipe intègre a non seulement plus de chances de gagner la confiance des membres de son équipe, mais aussi d'être respecté·e et apprécié·e par l'équipe.
Éthique de travail basée sur la confiance : Les membres d'une équipe imitent souvent l'éthique de travail de leur chef d'équipe. Faire preuve de confiance envers l'équipe et sa tâche contribue à inspirer confiance aux membres de l'équipe.
Parler couramment l'anglais et le français.
Ce que nous offrons
L'occasion de contribuer à la création d'une entreprise vouée à la sécurité des enfants.
La chance d'intégrer une équipe innovante et dévouée, spécialisée dans les technologies de pointe.
Un salaire et un ensemble d'avantages sociaux compétitifs.
Nous sommes à la recherche de membres essentiels de l'équipe de BusPatrouille qui nous aideront dans notre quête pour accroître la sécurité des enfants. Ce poste joue un rôle important dans notre entreprise et constitue une formidable opportunité pour les candidat·e·s qui seront choisi·e·s. Nous offrons un milieu de travail inclusif, diversifié, enthousiaste, intègre et profondément engagé. Venez nous aider à assurer la sécurité des enfants.
Who we are:
Our mission is to create a culture of responsibility and awareness on the road. We are devoted to making the journey to and from school safer. We develop partnerships, deploy Safety Tech and manage the entire program. We have equipped thousands of buses across North America with our innovative technology and we continue to educate tens of thousands of drivers a month on safety. BusPatrol America cares about student safety. We educate motorists every day by helping to enforce the law and work with school officials to improve safety.
The Opportunity:
The data engineer at BusPatrol will work to structure, model, and automate all data flows into our AWS architecture, with a focus on optimization, integrity, and security. As part of the role, the Data Engineer will work immensely with stakeholders, and be a focal point of all technical needs of the BI department to develop algorithms to help make raw data more useful to the enterprise.
What we need you to do:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional/non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS data technologies.
Keep our data separated and secure across national boundaries through AWS regions
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
Work with data and analytics experts to strive for greater functionality in our data systems.
Bilingual in English and French

What you bring:
BS or MS degree in Computer Science or a related technical field
4+ years of experience with schema design and dimensional data modeling
5+ years of SQL knowledge for various reporting and transformation needs (MySQL, PostgreSQL, Redshift)
Experience building and optimizing data pipelines, architecture, and structured/unstructured data sets
Hands on experience in AWS GlueUnderstanding and technical knowledge on AWS services (EC2, S3)
Verbal and nonverbal communication skills: Communication is crucial when working with and leading a team because you will have to communicate with your team, supervisors, stakeholders, and external customers.. A team leader communicates in a way that is clear, effective and directive.
Organizational skills: Organization is important when you have multiple team members working on one project. Strong organizational skills will help you monitor progress and keep team members motivated.
Ability to delegate: Team leaders must delegate tasks to individual team members. This requires the team leader to trust in the abilities of the team.
Integrity: Team leaders should lead by example. A team leader who has integrity is not only more likely to be trusted by their team members, but also will often be respected and appreciated by the team.
Confident work ethic: Team members often mimic the work ethic of the team leader. Displaying confidence in the task and the team itself can help to instill confidence in team members.
What we offer:
An opportunity to help build a company dedicated to children's safety
The chance to join an innovative and dedicated team, focused on leading edge technology
Competitive salary and benefits package
We are looking for critical members of the BusPatrol team to assist us in our quest to improve children's safety. This is an important role for us and a great opportunity for the right candidates. Our environment is inclusive, diverse, ignited, built on integrity and deeply committed. Come and help us keep our children safe."
"ELT Data Engineer (Talend, Datastage), Banking and Financial...","Toronto, ON",CGI,None,Organic,"We are a global IT and business consulting services leader, and after 40 years, we're still growing! Join Canada's largest IT Company in our Global Wealth and Banking Services Division in Toronto.?

CGI supports our members’ career aspirations, offering learning initiatives and provides access to our global health and wellness program. We also offer competitive compensation and benefits such as our share purchase plan, a profit sharing program, flexible schedules that guarantee a good work-life balance!

Innovation, technology and service delivery are our focus. Our goal is to ensure our clients remain ahead of the competition. We provide a full spectrum of services from Business Consulting and Systems Integration to Managed Services and IP Solutions that are transforming our clients’ operations and helping them to succeed.?
Build your career with us.

It is an extraordinary time to be in business. As digital transformation continues to accelerate, CGI is at the center of this change—supporting our clients’ digital journeys and offering our professionals exciting career opportunities.

At CGI, our success comes from the talent and commitment of our professionals. As one team, we share the challenges and rewards that come from growing our company, which reinforces our culture of ownership. All of our professionals benefit from the value we collectively create.

Be part of building one of the largest independent technology and business services firms in the world.

Learn more about CGI at www.cgi.com.

No unsolicited agency referrals please.

CGI is an equal opportunity employer. In addition, CGI is committed to providing accommodations for people with disabilities in accordance with provincial legislation. Please let us know if you require a reasonable accommodation due to a disability during any aspect of the recruitment process and we will work with you to address your needs.
Your future duties and responsibilities
As a Data Engineer, you will design, develops, tests, implements, and maintains complex ELT functions, user defined functions and complex queries for custom solutions with limited direction.

Leads and coordinates code/peer review of focused development work to ensure it aligns to the business and technical requirements.

Collaborate with Product Owners and Analysts to understand business requirements and define technical solutions.
Design, develop, maintain and take ownership of code.
Design reusable components, user defined functions.
Required qualifications to be successful in this role
Minimum 5 years+ of related experience in Data Engineering.
Education: College/Bachelor in Computer Science and or related discipline. Recent relevant technical certification an asset.
Experience and knowledge of data architecture and concepts of relational and dimensional databases
Experience with enterprise application architecture and enterprise integration patterns.
Ability to implement re-usable data-integration/ETL code in an enterprise data-warehouse environment.
Perform complex applications programming activities. Code, test, debug, document, maintain, and modify complex applications program as required.
Examine and solve the performance bottlenecks in the ETL processes.
Demonstrates good understanding of the Software Development Life Cycle.
Proficiency in ETL tools - specifically Talend or DataStage.
Strong SQL skills.
Must have an experience working in Data Lake environment with Hive, Beeline expertise preferred.
Previous experience of working in an Agile Development environment.
Ability to work well in a challenging environment.
Strong troubleshooting skills.
Excellent writing skills, oral communication skills, strong process skills, and leadership ability.
Ability to multi-task and prioritize under minimal supervision."
Ingénieur en Science de Données / Data Science Engineer,"Montréal, QC",Square Enix Montréal,None,Organic,"// ENGLISH VERSION WILL FOLLOW …//
Poste: Ingénieur en Science de Données
Relève de: Directeur BI

Fonctions:
Concevoir et mettre en place des services Data Driven pour les opérations des jeux;
Concevoir, monitorer et améliorer des solutions Big Data en batch ou traitement de flux afin de maintenir notre stack analytique à la pointe de la technologie;
Optimiser la performance des pipelines de données en termes de temps et de coût;
Rédiger et maintenir la documentation technique appropriée;
Participer au processus de qualité des données en collaboration avec l’équipe QA;
Participer à l’amélioration continue des processus BI au sein de l’équipe centrale en collaboration étroite avec l’équipe DevOps du back-end;
Effectuer toutes autres taches connexes.

Expérience et qualifications:
Au moins 5 ans d’expérience pertinente ou équivalente dans le domaine du jeu mobile ou domaine connexe.
Maitrise de plusieurs systèmes de base de données dans le cloud tel que Google Big Query, Snowflake, Redshift ou équivalent;
Expérience en conception/architecture et mise en production de modèles data science;
Expérience avec l’environnement cloud Big Data de Google (AI Platform, Dataflow, Dataproc, PubSub etc.);
Expérience importante sur le traitement de données en batch;
Maitrise du Java, SQL et Python (Scala, Spark ou NodeJs sont des plus);
Expérience en développement CI/CD est un plus (Team City, Terraform, Ansible, Spinnaker, Kubernetes, itsio etc.);
Expérience avec un outil de reporting est un plus (Looker, Tableau etc.);
Expérience sur le traitement de données en flux est un plus;
Expérience sur l’application de la gouvernance et la sécurité des données (GDPR etc.) est un atout;
Application concrète des concepts de DevOps dans un contexte de service Data Driven est un plus;
Connaissances des outils de version control Gitlab;

Qualités interpersonnelles:
Capable de présenter les concepts clés reliés à la data science à des audiences variées;
Capacité à résoudre des problèmes complexes;
Rigoureux et précis;
Axé sur l’impact opérationnel, débrouillard et capable de prendre de l’initiative;
Être capable de s’adapter dans un environnement où les priorités peuvent changer régulièrement;
Bonne capacité d’apprentissage;
Être capable de faire preuve d’humilité et d’aider à faire progresser l’équipe.
Motivation et intérêts:
De l’ambition et de la passion pour les jeux vidéo sont essentiels!

// ENGLISH VERSION //
Title: Data Science Engineer
Reports to: BI Director

Duties:
Design and implement Data Driven services for game operations;
Design, monitor and improve Big Data solutions in batch or flow processing in order to keep our analytical stack at the cutting edge of technology;
Optimize the performance of data pipelines in terms of time and cost;
Write and maintain the appropriate technical documentation;
Participate in the data quality process in collaboration with the QA team;
Participate in the continuous improvement of BI processes within the central team in close collaboration with the DevOps team of the back-end;
Perform other related duties.

Experience and qualifications:
At least 5 years of relevant or equivalent experience in the field of mobile gaming or related field.
Mastery of several database systems in the cloud such as Google Big Query, Snowflake, Redshift or equivalent;
Experience in design / architecture and production of data science models;
Experience with Google's Big Data cloud environment (AI Platform, Dataflow, Dataproc, PubSub etc.);
Significant experience in batch data processing;
Mastery of Java, SQL and Python (Scala, Spark or NodeJs are a plus);
Experience in CI / CD development is a plus (Team City, Terraform, Ansible, Spinnaker, Kubernetes, itsio etc.);
Experience with a reporting tool is a plus (Looker, Tableau etc.);
Experience in data flow processing is a plus;
Experience in the application of governance and data security (GDPR etc.) is an asset;
Concrete application of DevOps concepts in a Data Driven service context is a plus;
Knowledge of Gitlab version control tools;

Interpersonal qualities:
Able to present key concepts related to data science to a variety of audiences;
Ability to solve complex problems;
Rigorous and precise;
Operational impact oriented, resourceful and able to take initiative;
Be able to adapt in an environment where priorities may change regularly;
Good learning ability;
Being able to show humility and help move the team forward.

Motivation and interests:
Ambition and passion for video games are essential!"
"Senior Software Engineer, Data Instruments","Vancouver, BC",Tableau,None,Organic,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
Einstein Analytics, recently rebranded as Tableau CRM, is the best-in-breed analytics for the CRM market. Within the Data Platform, our team is responsible for creating predictive ELT transformations on our next generation dataflow service that can handle phenomenal volumes of data resiliently to support our customer's business critical processes backed by Tableau CRM. The Tableau CRM Data Platform is at the heart of enabling our customers to curate and shape their data in preparation for analysis, join us and help build the future of our backend!
What you’ll be doing...
Building out new full stack data transformations with a UI which uses modern web frameworks and techniques which are backed by machine learning and predictive algorithms at multi billion row scale
Iteratively improving the reliability and performance
Collaborating with other teams building new features on our service
Keeping a customer success focused mindset in addressing production issues
Who you are...
3+ years of software development experience
Solid knowledge of Object-Oriented Programming and design
Experience developing in a modern programming language such as Java, C++, Golang
Experience with web applications and programming languages: Javascript (React or similar framework), HTML5, CSS3, ES6, Web components
In depth knowledge and experience of data structures, algorithms and design patterns
Experience with Agile development methodology and testing
Excellent written and verbal communication skills
You may have…
Bachelor’s degree in Computer Sciences or equivalent field, Masters degree is a plus
Experience designing, creating, performance tuning, and maintaining full stack features
Experience with machine learning and predictive algorithms
Knowledge of Big Data technologies such as Spark, Hadoop.
Experience with building data-processing or ETL products at scale is a plus
Experience with container technologies like docker and Kubernetes is a plus
Accommodations - If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesfore.com or Salesforce.org.
Salesforce welcomes all."
Senior Product Security Engineer - Data Security,Ontario,CircleCI,None,Organic,"Reporting to the head of security, the Senior Product Security Engineer - Data Security overseeing product privacy use will consult with data engineering teams, conduct privacy-focused code reviews, privacy impact assessments at the design phase, promote privacy by design concepts, and work on enablement tools that increase privacy.
You'll join a highly-distributed team that's building a paved security path so our team of more than 130 engineers to ensure our infrastructure provides value to legitimate customers. You'll write sustainable, resilient code as part of an engineering organization that values collaboration, trust, and learning. You'll be part of a team at the heart of CircleCI's business responsible for build environments used by thousands of development teams every day.
What You'll Do:
Establish a refined culture of privacy and security.
Partner with Security Operations, Product, Legal and Platform Security Engineering.
Write and maintain sustainable, high-quality, high-performance code for infrastructure and security automation.
Lead data anonymization and encryption projects.
Be a subject matter expert on not just privacy compliance, but best practices that go above and beyond what's required.
Report up to security management about product engineering's focus on data insights.
Participate in the Security Team's on-call incident rotation.
Respond to bug emails submitted by security researchers and work on remediation.
What We're Looking For:
Process makes you feel good. Learning something new every day is essential to your happiness. Mentoring is a primary reason why you love your profession. You are compassionate and genuinely like people. Privacy is not an afterthought, it's a way of life.
Does that sound like you? If so, here's the experience we're looking for:
Security mindset.
Strong analytical skills.
Passionate advocate for user privacy.
Comfortable designing and anonymizing large data sets.
Five years experience with privacy best practices and an ability to work with key business stakeholders in a non-adversarial way.
Excellent communication skills.
Calm under high-pressure situations.
Comfortable working in a modern cloud environment that uses Docker, Kubernetes, Terraform, AWS and GCP.
A willingness to learn new languages. We use Clojure, Go and TypeScript.
A focus on delivering high-quality code through strong testing practices.
Ability to manage customer demands and work with internal stakeholders to solve them.
Demonstrated ability to lead multiple, complex projects simultaneously.
CircleCI Engineering Competency Matrix:
The Engineering Competency Matrix is our internal career growth system for engineers. This position is level E3. If you're not sure this is you, we encourage you to apply. Find more about the matrix in this blog post.
We know there's no such thing as a ""perfect"" candidate - we're all a work in progress and are growing new skills and capabilities all the time. CircleCI welcomes those who are enthusiastic about learning and evolving, so however you identify and whatever your background, if this looks like a role where you could do work that excites you, we hope you'll apply.
Work remotely with our globally distributed team!
We're a distributed company with teammates across the world. For this role, we are hiring engineers to work remotely in The United States, Ireland, The United Kingdom, Germany and through our affiliate, Continuous Labs, in the following Canadian provinces: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island and Saskatchewan.
About CircleCI
CircleCI is the world's largest shared continuous integration and continuous delivery (CI/CD) platform, and the central hub where code moves from idea to delivery. As one of the most-used DevOps tools that processes more than 1 million builds a day, CircleCI has unique access to data on how engineering teams work, and how their code runs. Companies like Spotify, Coinbase, Stitch Fix, and BuzzFeed use us to improve engineering team productivity, release better products, and get to market faster.
CircleCI is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law."
"Senior Software Engineer, Data Instruments, Tableau","Vancouver, BC",Salesforce,None,Organic,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
Einstein Analytics, recently rebranded as Tableau CRM, is the best-in-breed analytics for the CRM market. Within the Data Platform, our team is responsible for creating predictive ELT transformations on our next generation dataflow service that can handle phenomenal volumes of data resiliently to support our customer's business critical processes backed by Tableau CRM. The Tableau CRM Data Platform is at the heart of enabling our customers to curate and shape their data in preparation for analysis, join us and help build the future of our backend!
What you’ll be doing...
Building out new full stack data transformations with a UI which uses modern web frameworks and techniques which are backed by machine learning and predictive algorithms at multi billion row scale
Iteratively improving the reliability and performance
Collaborating with other teams building new features on our service
Keeping a customer success focused mindset in addressing production issues
Who you are...
3+ years of software development experience
Solid knowledge of Object-Oriented Programming and design
Experience developing in a modern programming language such as Java, C++, Golang
Experience with web applications and programming languages: Javascript (React or similar framework), HTML5, CSS3, ES6, Web components
In depth knowledge and experience of data structures, algorithms and design patterns
Experience with Agile development methodology and testing
Excellent written and verbal communication skills
You may have…
Bachelor’s degree in Computer Sciences or equivalent field, Masters degree is a plus
Experience designing, creating, performance tuning, and maintaining full stack features
Experience with machine learning and predictive algorithms
Knowledge of Big Data technologies such as Spark, Hadoop.
Experience with building data-processing or ETL products at scale is a plus
Experience with container technologies like docker and Kubernetes is a plus
Accommodations - If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesfore.com or Salesforce.org.
Salesforce welcomes all."
Senior Data Engineer,"Mississauga, ON",Dimensional Strategies Inc. (DSI),"$85,000 - $100,000 a year",Organic,"The Senior Data Engineer embodies their passion for the business intelligence through building pragmatic data warehousing and business intelligence applications. At this level, the incumbent demonstrates detailed knowledge of data warehousing and business intelligence application development through simple, yet powerful user experiences supported by clear, maintainable software engineering efforts. They continuously refine their skills seeking a higher degree of mastery of the data warehousing and business intelligence applications while advocating for quality software development efforts.
KEY DUTIES & RESPONSIBILITIES
§ Uses detailed knowledge of programming techniques, data warehouse methodologies and hardware/software interfaces to develop data warehousing and business intelligence applications.
§ Performs work with some supervision, and some guidance will be provided.
§ Works with team members to deliver high quality software that meets the business needs in a timely fashion.
§ Designs, tests and integrates data warehouse and BI modules and resolves programming errors using various debugging tools and techniques.
§ Provides support and production assurance for common problems.
§ Conducts impact analysis for proposed changes to or problems within an application.
§ Prepares technical documentation (e.g., user guides, technical specifications).
§ Undertakes routine analysis and works with designers and analysts to clarify and improve specifications or to identify alternative programming solutions.
§ Makes recommendations in data warehouse and BI design, standards and process improvements.
§ Enforces team and organizational standards and practices (e.g. at walkthroughs and peer code reviews).
§ Engages in continuous learning by developing and executing on a learning plan.
§ Takes responsibility for individual and the team's results.
§ Estimates and prioritizes work to maximize value while considering risk, effort and dependencies.
§ Raises impediments, risks, and issues as early as possible and work with the team to mitigate as needed.
§ Advocates for quality in all aspects of development efforts based on the team's definition of quality.
§ Assists in the design of business solutions.
KNOWLEDGE & SKILLS
§ Demonstrates detailed knowledge of data warehouse development methodologies, Microsoft BI Suite, metadata ETL automation and data visualization tools.
§ Extensive experience in one of these cloud data warehouses (Snowflake, bigtable, Redshift), Data Vault 2.0 methodology, steaming data processing, BI components in SQL Server 2016, TSQL, and DAX, Power BI.
§ A good working knowledge of application security, C#, python, PowerShell, metadata management, NoSQL, and data security.
§ Experience in programming and debugging complex data warehouse and BI applications as part of a multi-disciplinary team environment (following an agile framework such as SCRUM preferred) based on Microsoft Team Foundation Server and git.
§ University graduation on Computer science, Mathematics or engineering majors
§ Minimum of 8 years of relevant job experience
§ Experience in writing unit tests to support production code.
§ Experience in leveraging modeling techniques such as Logical and physical ER to communicate ideas.
§ Takes ownership and initiative and collaborates well with a team of peers.
§ Demonstrates a commitment to continuous learning (e.g. user groups, blogs, conferences, community awareness, and next generation tooling).
§ Able to clearly communicate in both a verbal and written form within a predominantly English working environment.
§ Have a positive, passionate, idea generating attitude when faced with challenges.
Licenses and/or Professional Accreditation
§ Certification in Microsoft technologies preferred
Job Types: Full-time, Permanent
Salary: $85,000.00-$100,000.00 per year
COVID-19 considerations:
This role will be remote until September 1st at a minimum.
Work remotely:
Yes"
"Project Engineer, Solution Architecture & Services – Data Ce...","Calgary, AB",CoolIT Systems Inc.,None,Organic,"An exciting and challenging position exists as a Project Engineer at CoolIT’s head office in Calgary, Alberta.
As a member of the Solution Architecture & Services (SA&S) team, you will be accountable for the end-to-end activities required to deliver a successful data center deployment with global tier I customers.
As part of the broader Program Management group, your role will be to work closely with customers to ensure a seamless execution of site deployment.
Job Type: Full-time, permanent.
Job Location: Calgary, AB.
Required education: B.Sc. Mechanical Engineering.
Required experience: Minimum of 5+ years background in Mechanical Engineering field.
Reports to: Program Manager, SA&S, Data Center Solutions.
Travel Requirements: up to 50%--must hold a valid passport as travel will be international.
ABOUT COOLIT SYSTEMS, INC.
CoolIT Systems, headquartered in Calgary, is an industry-leading engineering company that has been dedicated to the invention and design of state-of-the-art liquid cooling technology since 2001. With over 60 patents and 2 million Direct Contact Liquid Cooling (DCLC™) units deployed in desktop computers, servers and data centers around the world, our vision is to become the global leader in energy efficient liquid cooling solutions. We strive to be an employer of choice and as such we believe in rewarding our employees with career and development opportunities that will maintain and strengthen our culture while aligning to our vision and values. We’re a company that is full of vibrant, innovative people who love what we do.
As part of our team at COOLIT, we offer the following benefits:
3 Weeks’ Vacation.
Participation in the Benefits program including; Health & Dental, Flex Spending Account, Flex time, all other corporate benefit programs that are or become available.
RESPONSIBILITIES:
Accountable for all aspects of commercial and technical requirements for CoolIT’s liquid cooling technology deployments and managing schedules for each designated project.
Establish standardization of software, processes, tools, document templates, and contracts to assist in running each project from initial engagement at the opportunity qualification stage to the final deployment stage.
Work closely with the other members of the Solution Architecture & Services team, namely, to develop productized services offerings and engage with Service Technicians on-site at the initial site survey and deployment stages of the project.
Collaborate with server OEM sales representatives and solution architects to design and deploy industry leading liquid-cooled High-Performance Compute systems, often listed on the Top 500 list of world’s most powerful supercomputers.
Introduce CoolIT’s technology on joint sales calls with CoolIT’s server OEM partners and their High-Performance Compute customers, identifying the products and services required to achieve a successful outcome.
Perform engineering pre-fabrication builds for our liquid cooling fluid networks alongside Service Technicians.
Design, validate, and provide a solution for liquid cooling deployments worldwide in a variety of End User data centers, utilizing various thermal and flow engineering software tools.
Oversee the creation of detailed project schedules, negotiate contracts, engage on product pricing decisions, and create tools which contribute to resource planning, as well as authorized service provider (ASP) engagement & training platforms.
Manage customers, market risks and issues responding with the appropriate level of planning and action to ensure all product commitments are met for each project.
Measure and report project status using appropriate tools, techniques, and metrics.
Report and escalate to management potential issues and risks as needed.
Hold regular meetings to ensure cross-functional team members communicate effectively and ensure that all Product risks are identified in a timely fashion.
OTHER REQUIREMENTS
Bachelor’s degree in engineering with strong account management and project management skills.
5+ years experience in solution design and deployment.
1-year minimum experience in a project management role.
Field engineering experience seen as an asset.
Data center experience considered a strong asset.
Experience in deploying products in partnership with customers, end-users and other stakeholders seen as an asset.
Team player with strong interpersonal skills, above average communication skills, and a high level of customer service.
Acts with a sense of urgency, illustrates ambition and drives for completion of tasks/projects.
Solid organizational skills including time management, attention to detail, and multitasking.
Motivated, creative, collaborative, and results oriented.
Overflowing with exuberance, energy, and enthusiasm.
HOW TO APPLY
Interested in this position? We would be pleased to review both your cover letter briefly telling us why you would be a great fit for this position and organization, and your resume detailing your related experience and education.
We thank every candidate in advance, however, only those selected for an interview will be contacted.
We kindly advise that we will not be recruiting or taking calls/emails from Talent, Recruitment, Staffing, Placement firms.
Job Types: Full-time, Permanent
Salary: From $1.00 per year
Benefits:
Casual dress
Dental care
Disability insurance
Extended health care
Life insurance
On-site parking
Stock options
Work from home
Schedule:
8 hour shift
Monday to Friday
Work remotely:
Yes, temporarily due to COVID-19"
Data Engineer (New Grad),"Toronto, ON",Jarvis,None,Organic,"Jarvis Consulting Group identifies high potential individuals and develops them into professional technology consultants working with the hottest technologies in some of Canada’s top companies.
The Opportunity
We’re looking for individuals with the right attitude and aptitude to become Data Engineers and work with some of Canada’s top companies to unlock the value of their data. If you want to become a data engineer working with top companies on high profile projects Jarvis just might be what you're looking for.

You’ll be involved in projects of high strategic importance to design, build and integrate data pipelines that provide a consistent flow of high quality data. Also, you’ll have the opportunity to explore the use of new and emerging big data related technologies and pioneer new patterns and practices.
What we offer
Comprehensive training on core BigData technologies provided by industry experts:
Cloud
DevOps
Java
Hadoop/Spark
Access to the most exciting and innovative projects within some of Canada’s top companies
Dedicated support to help you develop your career through coaching, professional networking opportunities and attaining industry recognized certifications
The opportunity to dramatically improve your skills by writing a lot of code
Who we're looking for
Curious - You’re interested in experimenting, learning, innovating and trying new things
Customer Focused – You strive to create value for your customers and always deliver with quality
Adaptable - You maintain a focus on results even as plans and priorities change and consistently deliver value
Humble - You recognize your strengths as well as your opportunities and are always willing to learn from others
Collaborative - You value the success of the group and freely share your knowledge, experience and insight
What we’re looking for
Experience and education
Diploma or degree in computer science, technology, engineering or a mathematical discipline
Aptitude for logical reasoning and quantitative problem solving
Good oral and written communication skills
Good collaboration and teamwork skills
Willing and able to commit to working with Jarvis for at least two years following a training period
Legally permitted to work in Canada (citizen/permanent resident/work permit)
Core Technical Skills
Familiar with Java or another object oriented programming language ( C++, Python, Ruby, etc)
Have completed at least one course related to designing algorithms and data structures
Why you should work with us
The ability to have your potential recognized and rewarded
Practical and relevant training utilizing the newest technologies
Support to help you establish your career and achieve your goals
The opportunity to work on innovative projects within a variety of Canada’s top companies
A competitive compensation package with support for continuous learning"
Data Engineer (Women in Technology Program),"Toronto, ON",Jarvis,None,Organic,"Jarvis Consulting Group identifies high potential individuals and develops them into professional technology consultants working with the hottest technologies in some of Canada’s top companies.
We're committed to diversity and creating opportunity for all, so we're looking for women candidates we can develop into Data Engineers. If you think you have what it takes to be a Data Engineer and just need the opportunity to prove yourself, then please apply!
The Opportunity
We’re looking for individuals with the right attitude and aptitude to become Data Engineers and work with some of Canada’s top companies to unlock the value of their data. If you want to become a Data Engineer working with top companies on high profile projects, Jarvis just might be what you're looking for.

You’ll be involved in projects of high strategic importance to design, build and integrate data pipelines that provide a consistent flow of high quality data. Also, you’ll have the opportunity to explore the use of new and emerging big data related technologies and pioneer new patterns and practices.
What we offer
Comprehensive training on core BigData technologies provided by industry experts:
Cloud
DevOps
Java
Hadoop/Spark
Access to the most exciting and innovative projects within some of Canada’s top companies
Dedicated support to help you develop your career through coaching, professional networking opportunities and attaining industry recognized certifications
The opportunity to dramatically improve your skills by writing a lot of code
Who we're looking for
Curious - You’re interested in experimenting, learning, innovating and trying new things
Customer Focused – You strive to create value for your customers and always deliver with quality
Adaptable - You maintain a focus on results even as plans and priorities change and consistently deliver value
Humble - You recognize your strengths as well as your opportunities and are always willing to learn from others
Collaborative - You value the success of the group and freely share your knowledge, experience and insight
What we’re looking for
Experience and education
You should have experience working with in Java or Python to develop applications but even if you just have a strong grasp of Java fundamentals please apply, we want you!
Diploma or degree in computer science, technology, engineering or a mathematical discipline
Aptitude for logical reasoning and quantitative problem solving
Good oral and written communication skills
Good collaboration and teamwork skills
Willing and able to commit to working with Jarvis for at least two years following a comprehensive training program
Legally permitted to work in Canada (citizen/permanent resident/work permit)
Core Technical Skills
Familiar with Java or another object oriented programming language ( C++, Python, Scala, etc)
Have completed at least one course related to designing algorithms and data structures
Why you should work with us
Practical and relevant training utilizing the newest technologies
Support to help you establish your career and achieve your goals
The opportunity to work on innovative projects within a variety of Canada’s top companies
A competitive compensation package with support for continuous learning"
Data Engineer Trainee,"Toronto, ON",Jarvis,None,Organic,"Jarvis Consulting Group identifies high potential individuals and develops them into professional technology consultants working with the hottest technologies in some of Canada’s top companies.
The Opportunity
We’re looking for individuals with the right attitude and aptitude to become Data Engineers and work with some of Canada’s top companies to unlock the value of their data. If you want to become a data engineer working with top companies on high profile projects Jarvis just might be what you're looking for.

You’ll be involved in projects of high strategic importance to design, build and integrate data pipelines that provide a consistent flow of high quality data. Also, you’ll have the opportunity to explore the use of new and emerging data related technologies and pioneer new patterns and practices.
What we offer
Comprehensive training on core Big Data technologies provided by industry experts:
Cloud
DevOps
Java
Hadoop/Spark
Access to the most exciting and innovative projects within some of Canada’s top companies
Dedicated support to help you develop your career through coaching, professional networking opportunities and attaining industry recognized certifications
The opportunity to dramatically improve your skills by writing a lot of code
Who we're looking for
Curious - You’re interested in experimenting, learning, innovating and trying new things
Customer Focused – You strive to create value for your customers and always deliver with quality
Adaptable - You maintain a focus on results even as plans and priorities change and consistently deliver value
Humble - You recognize your strengths as well as your opportunities and are always willing to learn from others
Collaborative - You value the success of the group and freely share your knowledge, experience and insight
What we’re looking for
Experience and education
Diploma or degree in computer science, technology, engineering or a mathematical discipline
Aptitude for logical reasoning and quantitative problem solving
Good oral and written communication skills
Good collaboration and teamwork skills
Willing and able to commit to working with Jarvis for at least two years following a training period
Legally permitted to work in Canada (citizen/permanent resident/work permit)
Core Technical Skills
Familiar with one or more programming languages (C++, Java, Python, Scala, etc)
Have completed at least one course related to designing algorithms and data structures
Why you should work with us
The ability to have your potential recognized and rewarded
Practical and relevant training utilizing the newest technologies
Support to help you establish your career and achieve your goals
The opportunity to work on innovative projects within a variety of Canada’s top companies
A competitive compensation package with support for continuous learning"
Data Engineer,"Montréal, QC",AdGear,None,Organic,"AdGear (belonging to the Samsung Ads business), is an Advertising Technology Company located in the heart of Old Montreal. AdGear focuses on enabling brands to connect with Samsung TV audiences as they are exposed to digital media across all devices. Being part of an international company such as Samsung and doing business around the world means that we get to work on big complex projects with stakeholders and teams located around the globe.

Samsung has developed a proprietary ad platform that leverages unique first-party TV data to help brands connect to their audience as they explore content across desktop, mobile, tablets and our Smart TVs. The Samsung Ad Platform delivers high-quality audience targeting powered by three key components: first-party audience data at scale, world-class data science, and brand-safe cross-device ad inventory.

About Reporting and Insights

Our group is responsible for developing a cohesive set of powerful reporting & insights features to empower our service and analytics teams, as well as inform our customers. Our playground includes reporting facilities and dashboards to serve both internal and external users as well as various ETL pipelines ingesting 1 terabyte of data per hour, around the clock.

Our tech stack includes a mixture of Java/Scala/Akka, Spark, Hive, AWS Athena, EMR, Vertica, Kafka, Airflow, Concourse, Docker and Kubernetes.

About your Role

As a Data Engineer, you will be designing and developing ETLs and data processing pipelines at scale. As a consequence, there will be opportunities to contribute to open source, conduct research and development, review code, participate in shaping our engineering practices and share knowledge. You will work with some incredibly talented and passionate developers within an engineering team with a strong technological background. You will also interact on a day-to-day basis with software engineers, scrum masters and product owners.

In this position, the chosen candidate is expected to have a hands-on, problem-solving approach and a friendly human-facing side to communicate and manage expectations.

RESPONSIBILITIES
Develop and maintain ETL pipelines
Develop and maintain streaming jobs
Develop and maintain data extraction tools (REST API clients)
Occasionally provide ad hoc data
NICE TO HAVE SKILLS AND/OR EXPERIENCE
Prior experience in a similar role
Experience in Java, Scala, Python, Bash, C, etc.
Solid understanding of Unix/Linux systems
Prior experience in streaming technologies like Akka, Kafka or Spark Streaming
Prior experience in Airflow
Prior experience in working in a Agile environment
GENERAL SKILLS
Good communication skills and capacity/willingness to work in a multi-teams environment.
Be resourceful, inventive and passionate
You are eager to challenge the status quo and willing to learn new programming languages
Demonstrated ability to prioritize tasks and resolve problems in a timely manner;
Ability to work autonomously, multi-task and work in a fast-paced and stressful environment;
Be proactive, addressing potential problems before they occur;
Attention to detail;
Problem-solving outlook, can-do attitude is a must

Location and working conditions
While our employees are currently working remotely, and before any return to our beautiful office located downtown Montreal, we are taking extra cared steps to ensure a smooth experience for any new employee joining our wonderful organization:

Tailored virtual onboarding plan sent prior to your start date
IT equipment & Samsung Ads Welcome Swag kit delivered to your doorstep

We have adapted our benefits & perks as currently the following:

Benefits

100% Company-paid comprehensive health & dental coverage
Personal spending account & Healthcare spending account with Sun Life
Competitive compensation package, including performance incentive bonus plan based on company, team and personal objectives
Minimum start of 16 vacation days with additional flex days
Access to a virtual care platform (wellness, medical & nutrition)
Access to employee assistance program
Travel, Life, and Short/Long term disability Insurance
Group RRSP Matching Program up to 5%

Perks

Monthly virtual social committee-lead activities (games night, happy hour, health challenges, etc.)
Virtual companies get togethers (demo days, Town Halls, and more!)
Virtual yoga once per week
Open source days for software engineers
Employee Referral Program
Perkopolis website with great rebates, coupons and promotions
Amazing discounts through Samsung Employee Discounts website

What can you expect for your interview process?
Below is what it looks like from start to finish. Our promise is to keep communication open the entire way so you always know where you stand.
A 30 min Pre Screen with our fabulous TA team.
A 1 hour Culture Interview via Webex with our Engineering Manager and a team member.
We’ll use this time to simply get to know you as a person and hear more about your work experience. Some more technical questions may be asked to make sure you possess the minimum skills we think are required to accomplish the key objectives.
And finally, the Offer presentation with HR and the Hiring Manager. Welcome to the team!

About AdGear culture…
We promote autonomy, ownership, and accountability. Everyone is a leader at Samsung Ads, taking initiative with confidence.
Our differences make a difference, and we are part of a collaborative team that values different opinions, thoughts, and ideas.
We conduct ourselves with mutual respect, authenticity, and empathy. We act with thoughtful intention to make the right decisions
We are dynamic individuals who care about results. We celebrate the hard work and recognize achievements. We love what we do.
We embrace the open source movement and actively support the open source community
We foster a culture of teamwork and ideation, and embrace challenges through a creative and innovative approach.
Powered by JazzHR
XfqTEeGIlj"
Software Engineer - Data Pipeline,"Toronto, ON",BenchSci,None,Organic,"BenchSci's vision is to bring medicine to patients 50% faster by 2025. We're doing this by empowering scientists with the world’s most advanced biomedical artificial intelligence to run more successful experiments. Backed by F-Prime, Gradient Ventures (Google’s AI fund), and Inovia Capital, our platform accelerates science at 15 top 20 pharmaceutical companies and over 4,300 leading research centers worldwide. We're a CIX Top 10 Growth company, certified Great Place to Work®, and top-ranked company on Glassdoor.

We are currently seeking a Software Engineer to join our growing Data Pipeline Team! Reporting to the Engineering Manager, you will work on evolving our data models in several styles of datastores, improve internal tooling to allow data self-service, and operationalize production-grade data pipelines. As part of this role, you’ll have an opportunity to collaborate with a world-class team, experience growth and mentorship, and apply state-of-the-art data engineering solutions that will shape the future of scientific discovery.
You Will:
Scale data pipelines to allow data to go from research to platform as fast as possible
Develop data access mechanisms for downstream applications consumption
Manage sources which contain both semi-structured as well as unstructured data
Develop and apply suitable frameworks to detect data drift, and then calibrate and redeploy them to production seamlessly
Collaborate closely with other engineers to solve interesting and challenging data problems
Provide troubleshooting analysis and resolution in a timely manner
Have opportunities to work both independently and in pair-programming settings
Work with a highly collaborative team
Be given an unmatched opportunity for growth and development, and to learn from a team of outstanding engineers
Manage your code by troubleshooting, debugging, and fixing issues
Design testable, scalable solutions to complex problems using the latest frameworks and tools
Feel challenged and engaged as you’re exposed to new opportunities that will require you to push yourself
You Have:
3+ years working as a professional developer
Experience in Python
Experience with SQL
Experience with cloud reference architectures and developing specialized stacks on cloud services
Experience with Spark 2.x or Pandas
Eagerness to share your own ideas and openness to those of others
An interest in working with the latest tools and technologies available
The desire to see constant improvement in both yourself and your team
Nice to haves, but not mandatory qualifications:
A background in Life Science
Experience with Kubernetes in production
Our benefits and perks:
A compensation package that includes equity options in the company
An annual Executive Health Assessment at Medcan: All employees get the “executive treatment”
Effectiveness coaching for managers: Onsite, personalized coaching from a trained clinical psychologist
Mental health tools and support: Optional mindfulness sessions and a free Headspace account
Complimentary genome sequencing from 23andMe: Find out what your DNA says about your health, traits, and ancestry
Three weeks of vacation, plus another week: Get 15 days to use anytime, and we’re closed Dec 25-Jan 1
Additional days off: Company summer day, your birthday, and earn +1 vacation day annually
Work from anywhere flexibility: Every day right now, and up to 4 days per week once we return to the office
An onsite gym: Keep fit, conveniently, with a Peloton and other great equipment
A great benefits package: Including health and dental

Here at BenchSci, these are our core values:
Focused: We focus on what will drive the greatest impact at all times.
Advancement: We believe in continuous growth, and discovering new ways to do things better. This applies to our product and business, but also to ourselves.
Speed: We recognize that without a sense of urgency, our team, our product and our mission lose their value.
Tenacity: What we’re trying to do isn’t easy, but we hire the best people, and give them the autonomy, tools, and resources to succeed. The hard work is up to them.
Transparency: We believe that sharing diverse ideas and information creates strong teams. Our success stems from research, collaboration, feedback, and trust.
BenchSci is an equal opportunity employer. We value diversity and are committed to fostering an inclusive environment. All four of our cofounders are immigrants to Canada, as are many of our employees. We welcome your fresh perspectives and ideas."
Data Engineer (Azure Cloud) / Ingénieur(e) infonuagique de d...,"Montréal, QC",Slalom Consulting,None,Organic,"*English will follow*
Slalom est une société de conseil moderne axée sur la transformation de la stratégie, de la technologie et des activités. Avec notre mentalité axée sur les objectifs, nous travaillons en collaboration avec des entreprises pour repousser ensemble les limites de ce qui est possible.
Chez Slalom, la connexion personnelle rencontre l’échelle mondiale. Nous établissons des relations étroites avec les clients au sein de nos marchés et à l’échelle mondiale, en faisant circuler nos connaissances dans tous les marchés afin que chaque engagement puisse bénéficier de toute l’étendue de l’expertise de Slalom. Nos sept centres régionaux Build agissent comme points centraux de l’innovation pour attirer des talents de haut niveau qui collaboreront rapidement à la création des produits technologiques de demain. Nous entretenons également de solides partenariats avec plus de 200 fournisseurs technologiques de premier plan, notamment Amazon Web Services, Google Cloud, Microsoft et Salesforce.
Fondée en 2001, Slalom a établi son siège social à Seattle et, selon un mode de développement par croissance interne, compte maintenant plus de 8 500 employés. Nous figurons en 2020 sur la liste des 100 meilleurs employeurs établie par le magazine Fortune pour la cinquième année consécutive et nous sommes régulièrement mentionnés par nos employés comme l’un des meilleurs environnements de travail. En savoir plus : https://www.slalom.com/fr-ca/?lp=1.
Au Canada depuis 2015, Slalom compte maintenant plus de 500 employés répartis dans trois marchés : Vancouver, Toronto et Montréal.

L’équipe Slalom de Montréal recherche un(e) ingénieur(e) infonuagique de données pour notre pratique d’analytiques des données. À titre d’ingénieur(e) infonuagique de données, vous analyserez, concevrez et construirez des solutions infonuagiques pour répondre aux besoins de nos clients en matière d’infrastructure sous forme de service, de plateforme sous forme de service et de logiciel sous forme de service. Dans le cadre de ce poste, vous utiliserez des outils d’architecture de données modernes, notamment en nuage (AWS/Azure/GCP), Hadoop, Spark, Kafka et d’autres technologies liées aux mégadonnées. En plus de bâtir la prochaine génération de plateformes de données, vous travaillerez avec certaines des organisations les plus innovatrices en analytiques des données. Nous sommes à la recherche de personnes vives d’esprit, disciplinées et motivées qui sont passionnées par l’utilisation de solutions infonuagiques pour résoudre des problèmes d’entreprise réels.

Responsabilités :
Travailler au sein d’une équipe sur la conception et le développement de solutions infonuagiques de données
Établir les exigences techniques, évaluer les capacités des clients et analyser les résultats afin de fournir des recommandations appropriées en matière de solutions infonuagiques et de stratégies d’adoption
Définir les stratégies infonuagiques de données, y compris la conception de feuilles de route de mise en œuvre à plusieurs phases
Diriger l’analyse, la construction, la conception et le développement d’entrepôts de données et de solutions d’intelligence d’affaires
En savoir beaucoup sur les solutions infonuagiques, l’architecture et les technologies Azure, ainsi que leurs interdépendances
Expérience démontrée avec ETL, Azure Data Factory, les entrepôts de données, l’intégration de données, le profilage de données et la visualisation de données (PowerBI…)
Connaissance approfondie de SQL et de débogage de problèmes complexes
Rechercher, analyser, recommander et sélectionner des approches techniques pour résoudre des problèmes de développement et d’intégration difficiles et stimulants
Découvrir et adopter de nouveaux outils et de nouvelles techniques afin d’accroître la performance, l’automatisation et l’évolutivité
Aider les équipes de développement des affaires à exécuter les activités de prévente et les demandes de proposition
Comprendre les objectifs et déclencheurs d’affaires, et les traduire en une solution technique appropriée
Compétences requises :
Plus de trois ans de construction et de mise en œuvre d’infrastructures MS Azure
Comprendre la mise en œuvre de conceptions fondées sur l’architecture Lambda
Expérience de configuration et d’ajustement de nuages virtuels privés
Expérience concrète d’évaluation des besoins en matériel et en stockage
Solides capacités analytiques de résolution de problèmes
Personne autonome ayant la capacité de travailler de façon indépendante ou au sein d’une équipe de projet
Expérience du codage en Python ou .Net est un atout
B. Sc. en sciences informatiques, en un domaine connexe ou une expérience professionnelle équivalente
De l’expérience en Azure Databricks et en Azure Dev/Ops est un atout
Compréhension des écosystèmes infonuagiques et des technologies infonuagiques émergentes et dernier cri
Slalom est un employeur qui offre l’égalité des chances, et toutes les candidatures qualifiées seront étudiées pour la dotation du poste, sans égard à la race, à la couleur, à la religion, au sexe, aux origines nationales, au statut d’ancien combattant ou toute autre caractéristique visée par la loi.

Slalom is a modern consulting firm focused on strategy, technology, and business transformation. With our purpose-driven mindset, partner with companies to push the boundaries of what’s possible—together.
At Slalom, personal connection meets global scale. We build deep relationships with our clients within our markets and across the globe, while sharing insights across markets to bring the full breadth of Slalom's expertise to every engagement. Our seven regional Build Centers are hubs for innovation, attracting top talent to rapidly co-create the technology products of tomorrow. We also nurture strong partnerships with over 200 leading technology providers, including Amazon Web Services, Google Cloud, Microsoft, and Salesforce.
Founded in 2001 and headquartered in Seattle, Slalom has organically grown to over 8,500 employees. We were named one of Fortune's 100 Best Companies to Work For in 2020 for the 5th year in a row and are regularly recognized by our employees as a best place to work. Learn more at slalom.com.
Slalom in Canada began in 2015 and has grown to over 500 employees across 3 markets – Vancouver, Toronto, and Montreal.
Slalom in Montreal is hiring a Cloud Data Engineer to join our Data and Analytics practice. As a Cloud Data Engineer on our team, you will analyze, design and architect cloud-based solutions to address our clients’ needs for infrastructure-as-a-service, platform-as-a-service, and software-as-a-service. In this role, you’ll be using modern data architecture tools, including cloud (AWS/Azure/GCP), Hadoop, Spark, Kafka and other Big Data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics. We are looking for sharp, disciplined, and self-motivated individuals who have a passion for utilizing the cloud solutions to solve real business problems for our customers.

Responsibilities:
Work as part of a team, to design and develop cloud data solutions
Gather technical requirements, assess client capabilities and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Define Cloud Data strategies, including designing multi-phased implementation roadmaps
Lead analysis, architecture, design, and development of data warehouse and business intelligence solutions
Be versed in Azure cloud solutions, architecture, related technologies and their inter-dependencies
Proven experience with ETL (Azure Data Factory), data warehousing, data ingestion, data profiling and data visualization (PowerBI…)
Proficient in SQL and debugging complex queries
Research, analyze, recommend and select technical approaches for solving difficult and challenging development and integration problems
Learn and adopt new tools and techniques to increase performance, automation, and scalability
Assist business development teams with pre-sales activities and RFPs
Understand business goals and drivers and translate those into an appropriate technical solution
Qualifications:
3+ years architecting and implementing MS Azure infrastructure
Understanding implementing Lambda architecture-based data designs
Experience configuring and tuning virtual private clouds
Practical experience sizing hardware and storage needs
Strong analytical problem-solving ability
Self-starter with the ability to work independently or as part of a project team
B.S. in Computer Science, related fields or commensurate work experience
Azure Databricks, Azure Dev/Ops experience is a plus
Experience experience coding in Python and .Net is a plus
Understanding of cloud ecosystem and leading-edge cloud emerging technologies
Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
Senior Data Engineer,"Richmond Hill, ON",Paymentus,None,Organic,"Summary/Objective
The BI Data Engineer will assist in growing our monitoring and BI infrastructure. The individual will be responsible for supporting the stack 24/7 and working with the OPS team. An ideal candidate has a strong desire to learn new things, and the capability to “learn-as-you-go”. A passion for applying innovative technology in combination with excellent problem solving and communication skills will make a successful BI Data Engineer.
Essential Functions/ Responsibilities
Build and maintain batch and real-time data pipelines to power our BI and operational reports and dashboards
Generate graphs and reports in Grafana for business and operational needs.
Create Kapacitor alarms for business and operational events
Support the stack 24/7
Identify, design, and implement internal process improvements
Assist stakeholders with data-related technical issues and support data infrastructure needs
Work with data and analytics experts to strive for greater functionality in our data systems
Supervisory Responsibility
This position does not have any supervisory responsibility or direct reports.
Education and Experience
A bachelor's degree in Software Engineering, Computer Science, or related technical degree
6+ years of experience in software development or DevOps engineering
Basic understanding of time series and relational databases (Oracle, InfluxDB)
Proficiency in bash scripting or Python; SQL; Unix/Linux
Experience as a successful problem solver and communicator
Preferred but not required:
Prior knowledge with InfluxDB stack is plus
Knowledge of at least one programming language like Scala, Java or Python is plus.
Experienced in Apache Spark, or other big data processing frameworks
Software development and source code management (Git etc)
Work Environment
This job operates in a professional office environment. This role routinely uses standard office equipment such as laptop computers, photocopiers and smartphones.
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
Specific vision abilities required by this job include close vision and ability to adjust focus. Prolonged periods sitting at a desk and working on a computer. Must be able to lift up to 15 pounds at times.
Position Type/Expected Hours of Work
This is a full-time position. Days and hours of work are Monday through Friday, during normal business hours. Occasional evening and weekend work may be required as job duties demand.
Travel
Little to no travel is expected for this position.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.
EEO Statement
Paymentus is an equal opportunity employer. We enthusiastically accept our responsibility to make employment decisions without regard to actual or perceived race, creed, color, age, sex or gender (including pregnancy, childbirth and related medical conditions), gender identity or gender expression (including transgender status), sexual orientation, national origin, ancestry, citizenship status, religion, marital status, physical or mental disability, military service or veteran status, genetic information, protected medical condition as defined by applicable state or local law, genetic information, or any other classification protected by applicable federal, state, and local laws and ordinances. Our management is dedicated to ensuring the fulfillment of this policy with respect to hiring, placement, promotion, transfer, demotion, layoff, termination, recruitment advertising, pay, and other forms of compensation, training, and general treatment during employment.
Reasonable Accommodation
Paymentus recognizes and supports its obligation to endeavor to accommodate job applicants and employees with known physical or mental disabilities who are able to perform the essential functions of the position, with or without reasonable accommodation. Paymentus will endeavor to provide reasonable accommodations to otherwise qualified job applicants and employees with known physical or mental disabilities, unless doing so would impose an undue hardship on the Company or pose a direct threat of substantial harm to the employee or others.
An applicant or employee who believes he or she needs a reasonable accommodation of a disability should discuss the need for possible accommodation with the Human Resources Department, or his or her direct supervisor.
Job Type: Full-time
Pay: $0.00 per year
Additional pay:
Bonus pay
Benefits:
Dental care
Extended health care
Paid time off
Vision care
Schedule:
Monday to Friday
Experience:
Oracle: 3 years (Required)
Apache Spark: 2 years (Required)
Programming: 3 years (Required)
InfluxDB: 2 years (Preferred)
Bash or Python: 3 years (Required)
Work remotely:
Yes, temporarily due to COVID-19"
Associate Data Engineer,"Toronto, ON",Jarvis,None,Organic,"Jarvis Consulting Group identifies high potential individuals and develops them into professional technology consultants working with the hottest technologies in some of Canada’s top companies.
The Opportunity
We’re looking for individuals with the right attitude and aptitude to become Data Engineers and work with some of Canada’s top companies to help them unlock the value of their data. You’ll be involved in projects of high strategic importance to design, build and integrate data pipelines that provide a consistent flow of high quality data. Also, you’ll have the opportunity to explore the use of new and emerging data related technologies and pioneer new patterns and practices.
What we offer
Comprehensive training on the newest technologies provided by industry experts
Access to the most exciting and innovative projects within some of Canada’s top companies
Dedicated support to help you develop your career through coaching, professional networking opportunities and attaining industry recognized certifications
Who we're looking for
Curious - You’re interested in experimenting, learning, innovating and trying new things
Customer Focused – You strive to create value for your customers and always deliver with quality
Adaptable - You maintain a focus on results even as plans and priorities change and consistently deliver value
Humble - You recognize your strengths as well as your opportunities and are always willing to learn from others
Collaborative - You value the success of the group and freely share your knowledge, experience and insight
What we’re looking for
Experience and education
You should have academic or professional experience developing solutions (batch or streaming) for data ingestion, data mapping, reporting and data processing. Ideally you’ve used technologies like Hadoop, Kafka and Spark to build data pipelines.
Diploma or degree in computer science, technology, engineering or a mathematical discipline
Aptitude for logical reasoning and quantitative problem solving
Good oral and written communication skills
Good collaboration and teamwork skills
Willing and able to commit to working with Jarvis for at least two years following a training period
Legally permitted to work in Canada (citizen/permanent resident/work permit)
Core Technical Skills
Familiar with one or more programming languages (C++, Java, Python, Scala, etc)
Have completed at least one course related to designing algorithms and data structures
Why you should work with us
Practical and relevant training utilizing the newest technologies
Support to help you establish your career and achieve your goals
The opportunity to work on innovative projects within a variety of Canada’s top companies
A competitive compensation package with support for continuous learning"
Intermediate Data Engineer - Maternity Leave Contract,"Toronto, ON",BlueDot Inc.,None,Organic,"Our data engineering team is looking for an individual who is talented and passionate about using modern data tools that can be applied to our early warning system for infectious diseases to cover for a maternity leave on a 1-year contract.

As an Intermediate Data Engineer, you can create, expand, and optimize data pipelines to build upon our data platform. You thrive on writing complex queries, working with big data, and enjoy automating data pipelines and workflows. With us, you will solve problems with data integration and be working with data that has an impact on global connectivity as well as local areas of analysis. You will be working with both structured and unstructured datasets that are both spatial and non-spatial in nature.

Who we are:

BlueDot protects people around the world from infectious diseases using human and artificial intelligence. Our software-as-a-service solution combines medical and public health expertise with advanced data analytics to track, contextualize, and mitigate infectious disease risks. Our global early warning system combines more than 100 datasets with proprietary algorithms to deliver critical insights on the spread of infectious diseases. In December 2019, we flagged an undiagnosed respiratory syndrome in Wuhan, China. In January 2020, we published the world's first scientific paper on COVID-19, accurately predicting its global spread.

Our team understands the complexity of the challenge in front of us – and that the urgency to solve the problem has never been greater. BlueDot’s culture as described by our people is the diversity, openness, creativity, respect, intelligence, and kindness we share with each other. We are hard workers, motivated by a common purpose and excited to collaborate with a team who come from diverse backgrounds and schools of thought. We are a Certified B Corp, which means we meet rigorous standards of social and environmental performance, accountability, and transparency. We have also been recognized as a Best Place to Work for Women, Best StartUp and a top 25 Best Workplace in the country!

Our shared values:

Our values are not just words on a wall. They are our compass and they guide us in our work, in the decisions we make and in how we treat each other:

Driven by Purpose - We are united by a purpose that is bigger than ourselves. We are on a mission to create a healthier, safer and more prosperous world.
Do No Harm - Dealing with lives, we must be intentional, diligent, and precise.
Think Without Borders - When we free our minds from conventional thinking, impossible challenges become possible.
Strength in Diversity - We’re better at solving complex problems when we draw from our diverse backgrounds and experiences. We are composed of an intentionally eclectic collective of people with varying expertise united in a singular desire to outsmart infectious disease, every voice is heard and valued.
Success from Happiness - We believe that happiness is an essential ingredient for success.

What you will do in more detail:

Create and maintain optimal data pipeline architecture
Build and manage microservices on AWS
Design, develop, maintain cross-platform ETL processes
Use python to automate data processes and analyses to maximize efficiency
Help design, develop, test, document, and data pipeline related applications, programs and systems
Perform spatial analyses and create information products utilizing GIS and related software
Help design and implement data quality control procedures and policies
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Oversee and execute data migration from existing data stores
Assist with operationalizing data science models

What you have done to get here:

Degree in Computer Science, Engineering, related quantitative fields, or equivalent experience
Knowledge and experience with Python
Knowledge and experience with SQL
Experience with enterprise databases (MS SQL, MySQL or PostgreSQL)
Experience with source code control frameworks (Git).
Knowledge and experience with ETL software such as FME Desktop and FME Server
Knowledge and experience using NoSQL databases such as MongoDB or DynamoDB
Experience with GIS Products (QGIS, ArcGIS Enterprise, ArcGIS Pro) - nice to have
Collaborative attitude, with ability to excel in a team environment
Ability to quickly learn new tasks and systems, flexible in skills and attitude
Commitment to internal/external customer satisfaction through delivered excellence
Excellent written and verbal communication and interpersonal skills
Goal orientation with a drive to exceed expectations
Superior attention to detail and ability to produce professional deliverables & documentation on time.
Ability to manage personal time and priorities effectively

Ideally, you also have:

A basic understanding of modern techniques and tools for Data Science, Modeling and Analytics
An understanding of API architecture and integration
Experience with infrastructure as code (terraform, cloudformation)
Experience in the health sector

What we offer our team:

Meaningful work that truly has purpose
As a smaller, agile team, we offer roles with impact
Your contributions are integral, your voice will be heard
A competitive comprehensive compensation package
Outstanding health, vision and dental benefits
Generous vacation and other PTO
Fresh fruit and healthy office snacks
Downtown office with beautiful lake views, minutes from Union station

Together let’s create a healthier, safer, and more prosperous world.

For more information, visit us at: http://bluedot.global

BlueDot is an equal opportunity employer. We are committed to a diverse, inclusive and accessible workplace for all. All qualified applicants will receive fair consideration without regard to race, religion, citizenship, colour, creed, sex, sexual orientation, gender identity/expression, age, marital status, disability, or other protected characteristics.

BlueDot is committed to fair and accessible employment practices. If you are contacted for a job opportunity, please let us know how we can best meet your needs and advise us of any accommodations required to ensure fair and equitable access throughout the recruitment and selection process.

We thank and appreciate all applicants for their interest. Only those selected for an interview will be contacted."
Data Engineer/Ingénieur de données,"Kirkland, QC",GlobalVision,None,Organic,"Affichage en français ci-dessous

What We Are Looking For
The Data Engineer is responsible for building and maintaining GlobalVision’s data infrastructure. You will design and build the data source pipelines from the ground up and make the data accessible in BI tools. You’ll work with cross-functional teams to gather requirements for internal analytics, and then build a scalable, secure, and resilient technology stack.

Who We Are
We are GlobalVision, a software and technology company founded in 1990 and headquartered in Montreal. We develop high-tech industrial quality control solutions for international businesses. We’re a global company and businesses everywhere use and depend on our software. We have a strong vision, and it’s carried out by a tight-knit, talented, and diverse team.

Work Hard, Play Hard
At GlobalVision balance is important to us. Growing our people is just as important as building the best products possible. We have carefully cultivated our team, and we are always looking for new talent to join our company.

In This Role, You’ll Get To
Design, build, test and optimize data pipelines and warehouses
Design, build, test and optimize tools for data generation
Create the foundation to allow for real-time analytics and meaningful data-driven insights
Maintain and update our product data structures alongside the development team
Build a culture of data literacy across the company

We Are Looking For People Who
2+ years of work experience as data engineer or scientist
Experience with both relational (SQL) and non-relational (NoSQL) databases
Interest/experience in data mining, machine learning, statistical methods, AI, etc.
Experience working in an Agile development environment
Fluent with Java and Python
BS/BA in Computer Science, Math & Statistics, or equivalent

Nice to have
Experience with BI tools such as Tableau

Perks
Competitive salary
RRSP company matching
Flexible schedules
Fully Remote
Summer Friday’s
On-site gym
Auditorium cafeteria
Mini-scooters
Modern facilities
Free parking
Office Dogs
Office casual dress code
Monthly Employee Events
Remote Office Downtown right on the Lachine canal

Ce que nous recherchons
L'ingénieur de données est responsable de la création et de la maintenance de l'infrastructure de données de GlobalVision. Vous concevrez et créerez les pipelines de sources de données à partir de la conception et vous rendrez les données accessibles dans les outils de BI. Vous travaillerez avec des équipes interfonctionnelles pour rassembler les besoins en matière d'analyse interne, puis vous construirez une base technologique évolutive, sécurisée et résiliente.

Qui sommes nous
Nous sommes GlobalVision, une entreprise de logiciels et de technologies fondée en 1990 et dont le siège social est situé à Montréal. Nous développons des solutions de contrôle de qualité automatisées de haute technologie pour des entreprises internationales. Nous sommes une entreprise mondiale et les entreprises du monde entier utilisent et dépendent de nos logiciels. Nous avons une vision forte, réalisée par une équipe soudée, talentueuse et diversifiée.

Travailler fort, jouer fort
Chez GlobalVision, l’équilibre est important pour nous. La croissance de notre personnel est tout aussi importante que la création des meilleurs produits possibles. Nous avons soigneusement cultivé notre équipe et nous sommes toujours à la recherche de nouveaux talents pour rejoindre notre entreprise.

Dans ce rôle, vous allez
Concevoir, construire, tester et optimiser les bases de données et les entrepôts
Concevoir, construire, tester et optimiser les outils de génération de données
Créer les bases pour permettre des analyses en temps réel et des aperçus significatifs basés sur des données
Maintenir et mettre à jour nos structures de données sur les produits en collaboration avec l'équipe de développement
Créer une culture de la maîtrise des données dans toute l'entreprise

Nous recherchons des personnes qui
2+ ans d'expérience professionnelle en tant qu'ingénieur de données ou scientifique
Expérience avec les bases de données relationnelles (SQL) et non relationnelles (NoSQL)
Intérêt/expérience dans l'exploration de données, l'apprentissage machine, les méthodes statistiques, l'IA, etc.
Expérience de travail dans un environnement de développement Agile
Maîtrise de Java et de Python
Licence/BA en informatique, mathématiques et statistiques, ou équivalent

Agréable d'avoir
Expérience avec des outils de BI tels que Tableau

Avantages
Salaire compétitif
Cotisation de l’employeur pour les REER
Horaires flexibles
Travail à distance
Gymnase sur place
Cafétéria dans un auditorium
Mini-scooters
Installations modernes
Stationnement gratuit
Chiens dans nos bureau
Demi-journée les vendredi de l'été
Code vestimentaire décontracté
Événements mensuels des employés
Bureau au Centre-Ville, directement sur le canal de Lachine pour le travail à distance"
Senior Data Engineer,"Montréal, QC",Behavox,None,Organic,"About Behavox
Behavox is shaping the future for how businesses harness their most important raw material - data. Our mission is bold: Organize enterprise data into actionable information that protects and promotes the business growth of multinational companies around the world.
From managing enterprise risk and compliance to maximizing revenue and value, our data operating platform presents a widespread opportunity to build multilingual, AI / ML - based solutions that activate data for every function within a global enterprise.
Our approach is unique, and it's validated by our customers who tell us to keep forging ahead because no one else is aggregating, analyzing, and acting on data to uncover opportunities or solve problems quite the way we are.
We are looking for fearless innovators who have an insatiable appetite for building what no one has built before.
About the role
The Data Operations team is responsible for the creation and management of large datasets that are used by our analysts and engineers to train and test machine learning applications and analytics. Due to their variation and complexity, it is vital that our datasets are named, formatted, structured, stored, categorized and catalogued consistently, enabling rapid, coordinated and effective development of applications across the Data Science functions.
As our sole Senior Data Engineer, you will be responsible for the technical solutions and processes that facilitate and automate the dataset lifecycle.
What You'll Do
Data creation and synthesis
Transformation of unstructured data into standardized formats
Integration of data from different sources
Anonymization, pseudonymization and removal of PII
Classification and labelling
Storage and version management
What You'll Bring
Experience in python for data analysis and visualization (Jupyter notebooks / Pandas / Matplotlib / Seaborn)
Experience working with databases like SQL
Experience with management and manipulation of very large datasets
Good understanding of Amazon AWS infrastructure including S3 API / CLI
Strong knowledge of Git
Good understanding of CI/CD automation tooling (for example, Jenkins, Gitlab/Bitbucket pipelines)
Experience with data preparation for machine learning and NLP applications
What We Offer
A truly global mission with a passionate community in locations all over the world
Huge impact and learning potential as our aspirations require bold innovation
Highly competitive compensation with 100% bonus pay already integrated
Benefits include fully covered health coverage for employee and family
Generous time-off policy and flexible work schedule
About Our Process
We take Talent very seriously and we are building a community of extraordinary individuals working together in very high performing teams. We also know that the best Talent always has options so we believe that the process has to be a two way assessment - the company AND the candidate assessing the business needs alignment, the career next step alignment, and the cultural alignment.
During the process we will begin by exploring the core factors regarding salary and location along with core experience and skills and values alignment. We will then deep dive explore the critical technical competencies we have identified for the role, and then we will deep dive in behavioral competencies.
The most aligned candidate will then be asked to do a practical work task simulation activity so we can make sure that you will enjoy the kind of work the role requires, and this task will typically be presented and discussed with a group of colleagues and managers. Finally we will ask you to meet with a number of our senior leaders to make sure that you are making the most informed call possible."
Data Engineer,"Toronto, ON",shopkick,None,Organic,"Shopkick is on a mission to bring delight and reward to the consumer shopping experience. As part of the Trax Retail family, we are a pre-IPO unicorn looking to accelerate our growth trajectory as we partner with major brands and retailers, ranging from H&M and Unilever to Best Buy and Purina, to transform the way people engage with stores and products, both online and in the physical world.
Shopkick is in search of a Data Engineer. As a high growth company, we are looking for someone who thrives in a fast-paced environment. Essential duties and responsibilities may include, but are not limited to:
Implement and maintain our data infrastructure.
Build streaming and batch ETL pipelines to extract, transform, and load data from various sources.
Create tools and automation to support our data analysts.
Designing a framework for ensuring data quality.
Help troubleshoot complex data issues and recommend solutions
Qualifications:
5+ years of experience creating streaming and batch ETL systems
Strong ability to write efficient SQL queries
Strong coding ability in an object oriented language (Java, C#, C++, Python, etc.)
Experience with Google BigQuery, AWS Redshift, or AWS Athena
Experience with message distribution systems like Kafka or Pub/Sub
Familiarity with streaming ETL systems like StreamSets
Excellent team player with strong communication skills (verbal and written)
Enthusiastic about collaborative problem solving
Eagerness to solve challenging problems and a love of learning and trying new things
BS in Computer Science or related field or equivalent experience
Any unsolicited resumes/candidate profiles submitted through our website or to personal email accounts of employees of Shopkick are considered the property of Shopkick and are not subject to payment of agency fees."
Data Engineer (Aboriginal Entry Program),"Toronto, ON",Jarvis,None,Organic,"Jarvis Consulting Group identifies high potential individuals and develops them into professional technology consultants working with the hottest technologies in some of Canada’s top companies.
We're committed to diversity and creating opportunity for all, so we're looking for candidates with an Indigenous background that we can develop into Data Engineers. If you think you have what it takes to be a Data Engineer and just need the opportunity to prove yourself, then please apply!
The Opportunity
We’re looking for individuals with the right attitude and aptitude to become Data Engineers and work with some of Canada’s top companies to unlock the value of their data. If you want to become a Data Engineer working with top companies on high profile projects, Jarvis just might be what you're looking for.

You’ll be involved in projects of high strategic importance to design, build and integrate data pipelines that provide a consistent flow of high quality data. Also, you’ll have the opportunity to explore the use of new and emerging big data related technologies and pioneer new patterns and practices.
What we offer
Comprehensive training on core BigData technologies provided by industry experts:
Cloud
DevOps
Java
Hadoop/Spark
Access to the most exciting and innovative projects within some of Canada’s top companies
Dedicated support to help you develop your career through coaching, professional networking opportunities and attaining industry recognized certifications
The opportunity to dramatically improve your skills by writing a lot of code
Who we're looking for
Curious - You’re interested in experimenting, learning, innovating and trying new things
Customer Focused – You strive to create value for your customers and always deliver with quality
Adaptable - You maintain a focus on results even as plans and priorities change and consistently deliver value
Humble - You recognize your strengths as well as your opportunities and are always willing to learn from others
Collaborative - You value the success of the group and freely share your knowledge, experience and insight
What we’re looking for
Experience and education
You should have experience working with in Java or Python to develop applications but even if you just have a strong grasp of Java fundamentals please apply, we want you!
Be an Indigenous person (First Nation {Status or Non-Status}, Métis or Inuit)
Diploma or degree in computer science, technology, engineering or a mathematical discipline
Aptitude for logical reasoning and quantitative problem solving
Good oral and written communication skills
Good collaboration and teamwork skills
Willing and able to commit to working with Jarvis for at least two years following a comprehensive training program
Legally permitted to work in Canada (citizen/permanent resident/work permit)
Core Technical Skills
Familiar with Java or another object oriented programming language ( C++, Python, Scala, etc)
Have completed at least one course related to designing algorithms and data structures
Why you should work with us
Practical and relevant training utilizing the newest technologies
Support to help you establish your career and achieve your goals
The opportunity to work on innovative projects within a variety of Canada’s top companies
A competitive compensation package with support for continuous learning"
Business Intelligence Data Analyst / Engineer,"Vancouver, BC",Aurora Cannabis,None,Organic,"Business Intelligence Data Analyst / Engineer

As a member of the Business Intelligence & Analytics team, the successful candidate will possess very strong technical and analytical skills to gather and analyze all forms of data in detail. You will support the cloud base enterprise data warehouse maintenance and modernization through your advanced knowledge of data orchestration, ingestion, and reporting. You will identify the business needs and requirements, assemble data sets through efficient data processing pipelines that balance speed and reliability. You will develop automated reporting solutions and deliver analytical results for the business across the company. You will be a key contributor who enables Aurora to make data driven decisions.
Join an enthusiastic team that always enjoys learning, working collaboratively, and solving complex business problems with data.
MAIN RESPONSIBILITIES:
Maintain strong relationships with business stakeholders and work closely with Data Scientists, Analysts and Developers to ensure business needs are addressed.
Automate reporting solutions and operationalize machine learning models.
Implement optimised pipelines to extract and merge data sets from multiple sources.
Perform data cleansing and transformation using advanced Azure tools.
Work with the team to deploy new cloud-based reporting and analytics tools.
Set up automated deployment processes.
Implement data process improvements such as optimizing data delivery and scalability.
Execute multiple tasks simultaneously while exhibiting professionalism to ensure an incredible stakeholder experience.
JOB REQUIREMENTS:
Very strong experience in database systems including data warehouses and multi-dimensional analytic solutions.
Deep understanding of cloud technologies and infrastructure for data retention and processing such as Microsoft Azure Data Lake, Functions, Data Factory and Databricks.
Strong experience with Microsoft Azure data ingestion. Demonstrated via hands-on project experience.
Experience in managing and deploying code to cloud services, including Azure DevOps experience.
Proficiency in scripting and programming using Python, C#.
Working knowledge of Linux, Visual Studio.
Hand on experience with deployment and monitoring of machine learning models.
Hand on experience with Microsoft Power Suite – Power Apps and Power BI.
Knowledge in statistics and data mining.
Strong Agile project execution experience with proven ability to prioritize and manage time.
We would like to thank all applicants for their interest but only those selected for an interview will be contacted.

Aurora Cannabis Inc. is an Equal Opportunity Employer."
"Software Engineer, Big Data, Tim Hortons","Toronto, ON",Restaurant Brands International,None,Organic,"Our Opportunity:
Reporting to the Lead, Digital Solutions for Tim Hortons, the Software Engineer, Big Data will be responsible for the creation, standardization and validation of big data pipelines. The Big Data Software Engineer will utilize this information to drive recommendation and segmentation solutions across the Tim Hortons business. With oversight on all business-critical Big Data use cases, this person will help establish and maintain secure, accessible and cost-effective data platforms.
Responsibilities:
Evaluate, deploy and productionize high-availability Big Data platforms (in public cloud, and on premise), for use cases in Advanced Analytics and AI/ML.
Migration of data and workflow from legacy platforms to the new technology stacks.
Establish best practice and conduct knowledge sharing with platform users on the secure and efficient use of the Big Data platform resources – compute, storage and network bandwidth.
Big Data platforms' operations and support - implement health monitoring and service restoration technologies that effectively improve KPIs.
Integration with Enterprise Data Warehouse and other applications that generate and consume data from Big Data platforms – Databricks, Mparticle, Braze, SAP Business Object, for example.
Collaboration with internal and external partners on data governance, project delivery, and security hardening.
Skills & Qualifications:
University degree in Computer Science, Business, or other related fields
A master's degree in Data Analytics, Data Science or other related fields would be an asset
Minimum 5+ years experience building database and ETL solutions
Experience with at least 2 of the following technologies: Python, Scala, R, SQL and Java
Databricks or Apache Spark experience is a plus
Experience with AWS cloud technologies for creating data pipelines
Working knowledge of data modeling, data mining & Big Data Techniques
Experience analyzing large volumes of data
Good understanding of Software development process and knowledge of storage and data management process
Ability to complete multiple projects simultaneously in a fast pace environment
Strong teamwork, interpersonal and negotiation skills
Restaurant Brands International and all of its affiliated companies (collectively, RBI) are equal opportunity and affirmative action employers that do not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or veteran status, or any other characteristic protected by local, state, provincial or federal laws, rules, or regulations. RBI's policy applies to all terms and conditions of employment. Accommodation is available for applicants with disabilities upon request.

#LI-POST"
Software Engineer - AWS Fargate Data Plane,"Vancouver, BC","AMZN CAN Fulfillment Svcs, ULC",None,Organic,"· BS in Computer Science or equivalent work experience
The AWS Container Services organization is responsible for building and operating some of the core services that help customers run containerized microservices, including Amazon ECS, Amazon EKS, AWS Fargate, Amazon ECR, and AWS App Mesh. We have an ambitious roadmap that seeks to redefine how our customers build their applications and run them at scale. AWS Fargate is one of our big areas of investment. We want to let developers take full advantage of the speed, agility, and immutability that containers offer so they can focus on building applications rather than managing infrastructure.

To achieve this vision, we are looking for a Software Engineer for the Fargate Data Plane team. The Fargate Data Plane is responsible for running container workloads (https://aws.amazon.com/blogs/containers/under-the-hood-fargate-data-plane/). This is where Fargate and Firecracker (https://firecracker-microvm.github.io/) meet and also where we work with other cutting-edge technologies such as container security, logging, metrics, and observability in the broadest possible sense to help customers run their tasks.

If you are someone who is interesting in building high performance software and passionate about freeing developers to focus on building applications instead of managing infrastructure, this job is for you. You will be joining a team with the charter to make containers the new compute primitive and the first choice when architecting modern applications. You will have the chance to be a part of defining how developers use containers at scale.

If you join our team, you will:
Obsess over your customers and deliver a first-class experience for them
Take ownership of ambiguous problems and deliver solutions for them
Work on the complete software development life cycle: definition to delivery
Use data to make decisions and validate assumptions
Learn from others and help grow those in your team to achieve their best

Amazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age

4+ years of professional software development experience
MS in Computer Science or related field or equivalent work experience
Technically sound in software development activities and life cycles.
Experience with containers and the broader container ecosystem.
Strong verbal and written communication skills.
Strong analytic and problem solving skills.
Solid knowledge of Linux internals.
Internet and operating system security fundamentals."
Senior Azure Data Engineer,"Toronto, ON",AstraNorth,None,Organic,"This positions required Security Clearance (Enhanced Reliability Status) from the Government of Canada.
Candidates should have been a resident of Canada or USA or both for 5 continuous years.
Key Skills
Prior experience with Azure projects in a senior role with at least 2 data engineering projects
7+ years data engineering experience (overall)
Experience with Azure Data Factory, Databricks and storage technologies
Experience with deploying of ,maintaining data infrastructure in cloud (Experience with Azure DevOps, Jenkins, Bitbucket or ansible)
Experience with Steam ingestion platforms such as event hub, loT hub and Azure steam analytics
Proficient in SQL and NoSQL databases such as Azure SQL Data and Cosmos DB
Experience with Apache spark and Pyspark Libraries
Experience with PowerBi or Tableau
Performance tuning of the pipelines
Microsoft Data Engineer associate level certificate will be considered as an asset
Job Types: Full-time, Contract, Permanent
Schedule:
Monday to Friday
Work remotely:
Yes, temporarily due to COVID-19"
Lead Big Data Engineer,"Toronto, ON",RBC,None,Organic,"What is the opportunity?
As Lead Engineer (Development Manager) for the OTIS application team you will instrumental in the build out of the service based architecture and delivery model for the application. OTIS is a fundamental component of the Finance IT (Technology & Operations) strategic data strategy, the team is responsible for acquiring data from across the bank (Capital Markets, Retail Banking, Private Wealth etc) and transforming into standardized data sets for consumption by Finance and Risk control functions. The role involves responsibility for 250+ applications and the tech stack includes Java, Data Stage, StoneBranch, NAS, Spark, Hadoop, Vertica, Solace, Unix, MySQL, DevOPs (GitHub, UCD, Jenkins, JIRA).

What will you do?
Drive technology stack and direction, ensuring platform stability and efficiency
Drive the architecture design to build on the service based architecture, maximizing central services to build scale, drive down time to market whilst also ensuring consistent and repeatable deliveries
Oversee the development coding process to ensure it aligns to the service based architecture and delivers high-quality, robust code that minimizes defects and production issues
Build out of strong development unit test practices leading toward the automation of regression testing
Debug Java code or application performance issues to identify root cause and propose permanent solutions
Manage development projects through to successful completion including oversight of onshore and offshore development resources
Oversee the DevOps continuous integration processes to ensure the SDLC code promotion process is as efficient as possible

What do you need to succeed?
Must Have:
Java expert (must be current, hands-on programmer)
Architecture design experience; Good communication skills
Experience working with multi-threaded, real-time, and batch-processing applications
Strong database and SQL knowledge as well as knowledge of Unix/Linux OS, including shell scripting
Experience with Big Data technologies (Hadoop, Spark, Vertica, etc…)
Experience with messaging middleware eg Solace

Nice to have:
Background in financial services industry
Proficiency in XML technologies (XSD, XPath, etc)
Experience with web application and web services development
Experience within large team environment

What’s in it for you?
We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual.
A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable
Leaders who support your development through coaching and managing opportunities
Ability to make a difference and lasting impact
Work in a dynamic, collaborative, progressive, and high-performing team
Flexible work/life balance options
Opportunities to do challenging work
Opportunities to take on progressively greater accountabilities

Learn more about RBC Tech Jobs

Join our Talent Community
Stay in-the-know about great career opportunities at RBC. Sign up and get customized info on our latest jobs, career tips and Recruitment events that matter to you.
Expand your limits and create a new future together at RBC. Find out how we use our passion and drive to enhance the well-being of our clients and communities at rbc.com/careers.
Discover the Indigenous Space
Indigenous communities have a long history of partnering with RBC. By building relationships based on mutual respect, shared values and a common understanding, together we can create a strong, sustainable future for communities from coast to coast to coast.

Learn more at https://www.rbc.com/indigenous/index.html

JOB SUMMARY
City: Toronto
Address: 155 Wellington St West
Work Hours/Week: 37.5
Work Environment: Office
Employment Type: Permanent
Career Level: Experienced Hire/Professional
Pay Type: Salary + Variable Bonus
Required Travel(%): 0
Exempt/Non-Exempt: N/A
People Manager: No
Application Deadline: 11/15/2020
Platform: Technology and Operations
Req ID: 281620"
Data Engineer - Master Data Management,"Bedford, NS",IBM,None,Organic,"Introduction
At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities
IBM Global Business Services (GBS) helps our clients fundamentally redesign experiences to create new sources of value, digitally reinvent their operations for great efficiency and transform their entire enterprises through cognitive. We invite you to consider joining IBM's global reach, outcome-focused methodologies, domain skills and deep industry expertise that are helping transform the way we live and work. IBM's state-of-the-art Client Innovation Centre (CIC) opened in Nova Scotia in March 2013 and is the first of its kind in Canada. The Centre weaves together IBM’s business insights and industry-leading software portfolio and is ideally suited for any client that needs flexible access to emerging or niche skills that may not be cost effective to grow internally. In addition, our focus on our team is unparalleled as demonstrated by the following:

The CIC NS is one of IBM’s highest performing delivery centres worldwide – for retention, client satisfaction and utilization. Our employees are empowered to stay and grow within IBM.

We are very focused on continuous skill development – staff training is our third largest annual Centre investment. Employees are immersed in a culture of learning and constant growth

Investment in key partnership with universities, government and private sector groups which has resulted in IBM having a key influencing role in Nova Scotia’s ICT industry, especially in talent development.

We are currently recruiting for top talent in Data Engineer - Master Data Management. If you are on top of your profession and possess the following key skills, we invite you to apply to our openings. Below are the Responsibilities for the position include:

Job/Role description
Write new programs or modify existing programs to meet customer requirements, using current programming languages and technologies.
Verify the structure, accuracy, or quality of warehouse data.
Perform system analysis, data analysis or programming, using a variety of computer languages and procedures.
Implement business rules via stored procedures, middleware, or other technologies
Develop and implement data extraction procedures from other systems, such a CRM systems and HR systems.
Develop or maintain standards, such as organization, structure, or nomenclature, for the design of data warehouse elements, such as data architectures, models, tools, and databases
Provide or coordinate troubleshooting support for data warehouses
Creates supporting documentation and administrative reports (e.g. Data models, Design documentation, System Architectures and timesheets)
Review and interprets all documentation including business requirements, functiona and technical design specifications, etc
Nice to Have: Experience with DB2
Possess good communication and writing skills

Required Technical and Professional Expertise
4+ years of experience managing data integration between application
4+ years of experience in architecting, designing and configuring Informatica MDM.
4+ years of relevant data management consulting or industry experience (master data, metadata, data architecture, data governance, data quality, data modeling)
2+ years’ experience with Informatica MDM multi-domain edition or one of the 360 solutions in architecting, designing, and configuring solutions.
2+ years of experience leading workstreams with significant experience leading components of data engagements and team sizes ranging from 3 to 10 resources.
Practical experience with Websphere MDM v11.5




Preferred Technical and Professional Expertise
6+ years of experience managing data integration between application
6+ years of experience in architecting, designing and configuring Informatica MDM.
6+ years of relevant data management consulting or industry experience (master data, metadata, data architecture, data governance, data quality, data modeling)
4+ years’ experience with Informatica MDM multi-domain edition or one of the 360 solutions in architecting, designing, and configuring solutions.
4+ years of experience leading workstreams with significant experience leading components of data engagements and team sizes ranging from 3 to 10 resources.
Practical experience with Websphere MDM v11.5




About Business Unit
IBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter businesses by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.

Your Life @ IBM
What matters to you when you’re looking for your next career challenge?

Maybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.

Impact. Inclusion. Infinite Experiences. Do your best work ever.

About IBM
IBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.

Location Statement
This role will involve working with technology that is covered by Export Regulations sanctions. If you are a Foreign National from any of the following US sanctioned countries (Cuba, Iran, North Korea, Sudan, and Syria) on a work permit, you are not eligible for employment in this position.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Sr. Data Engineer -- Tech Lead,"Toronto, ON",autoTRADER.ca,None,Organic,"The Sr data Engineer – Tech Lead reports to the Director of Data working as part of the Data Tribe responsible for ensuring the technologies, systems and processes developed to support data warehousing, reporting and analytics are built efficiently and are in line with enterprise technology standards and best practices.
Key Accountabilities
Provide a leadership role in the development and communication of Data Warehousing strategies, principles, standards and best practices
Develop and maintain both operational and data warehouse conceptual, logical and physical data models.
Partner with Business Analysts to gather business, data and reporting requirements and translate the business needs into logical data structures.
Act as an important contributor to the development and maintenance of IT strategies and plans.
Experience with meta data management, data stewardship, data quality, IT governance, CEO/CFO Certification, Enterprise Data Models, and Enterprise Frameworks.
Direct team members on architecture concepts

Data warehouse Tech Lead - Architect requires experience with the following:
Business intelligence, query, and reporting tools
Data Warehouse / Data Lake / MDM architecture principles
Data integration and streaming integration concepts and architecture
Database design for read-only access
Data warehousing design issues such as star schema
Data warehousing technologies such as OLAP (including ROLAP, MOLAP, and HOLAP)
Data transformation and conversion
Data quality issues
Data formats for loading and unloading of data
Middleware
Platform specific workload management rules
Data Partitioning
Architecting for Mixed workload
Data Modelling for Data Warehousing / Data Lakes (Denormalized – Star & Snowflake Schemas, Data Vault)
Parallel execution
Indexing and statistics strategies
Code Reviews & Coaching for developers

Requirements:
University Degree or College Equivalent
10+ years IT experience in large scale technology architecture, operations and design related disciplines
4+ years in a Data Modeler or Data Architect role
3rdNormal Form and Dimensional modeling expertise
Team player with excellent communication skills
Facilitation and workshop coordination experience
Business information requirement analysis experience
Adept at balancing the immediate needs of a project with the future vision of the enterprise to achieve a win-win situation
Experience in Automotive and /or eCommerce
Google BigQuery, Oracle and AzureSQL Server experience an asset
ETL and data integration experience
CA Erwin experience would be an asset
Solid understanding of Project Management and SDLC methodologies
Advanced business analysis, architecture and design skills
Ability to communicate highly technical and complex security concepts effectively across all levels of the organization (both IT and business)"
Senior Data Engineer,"Montréal, QC",Lightspeed POS,None,Organic,"Lightspeed is growing quickly and we're looking for a Data Engineer to join our global Data & AI team. As a Data Engineer, you will be responsible for supporting Lightspeed's data operations and infrastructure underlying all of Lightspeed's products. As we continue our growth you will also play a key role in ensuring scalability of our data management systems and practices.
If you have worked on large projects and your background is in Java or in any other object-oriented language, and you're a natural-born problem-solver and always game to develop products that solve real-world problems for customers around the globe, this might be the role for you.
What you'll be responsible for:
Design and architecture our data systems; data sourcing, accessibility, security, quality, governance, discovery, and integration into business processes
Design, build and develop ETL pipelines consolidating various data sources (streaming or batch) into application specific Data Warehouses
Design, build and develop Analytical and Business Intelligence solutions
Prototype and develop solutions using statistical methods; data mining, machine learning, AI, etc.
Work as part of a team to deliver product features and functionality
Translate requirements into conceptual and detailed designs with estimates
Develop clean, maintainable code in a continuous integration + continuous deployment environment
Assist QA and Support staff in troubleshooting software issues as well as implementing bug fixes
What you'll be bringing to the team:
Senior level experience developing reliable, highly available and scalable software
Fluent with Java and Python
Experience with cloud environments like GCP, AWS, as well as cloud solutions like Kubernetes, Docker, etc.
Experience with both relational (SQL) and non-relational (NoSQL) databases
Experience with real time messaging systems (Pub/Sub, Kafka, etc.)
Proven skills in server side resource profiling, optimization and debugging
Strong interest/experience in data mining, machine learning, statistical methods, AI, etc.
Strong proficiency in a UNIX/Linux environment
Excellent communication skills
Experience writing automated unit and functional tests
Experience working in an Agile development environment
BS/BA in Computer Science, or equivalent experience
Even better if you have, but not necessary:
Research or work experience in AI, automation and/or Data Engineering
Good understanding of Design Patterns
Experience in a continuous delivery model
Experience in building APIs
Experience with event based and messaging systems
DevOps background
Advanced degree Computer Science, Statistics or related fields

Who we are
Lightspeed (TSX/NYSE: LSPD) powers small and medium-sized businesses with its cloud-based, omni-channel commerce platforms in over 100 countries around the world. With smart, scalable, and dependable point of sale systems, Lightspeed provides all-in-one solutions that help restaurants and retailers sell across channels, manage operations, engage with consumers, accept payments, and grow their business.
Headquartered in Montréal, Canada, Lightspeed is trusted by favourite local businesses, where the community goes to shop and dine. Lightspeed has offices in Canada, USA, Europe, and Australia.
We're passionate about enabling people to do their best work. Come work with us and find out what you can do!"
[REMOTE] Data Engineer,"Toronto, ON",HireValues,None,Organic,"Data Engineer, Leading Marketplace Platform, DT Toronto & Remote
This role is with an innovative tech firm that owns one of Canada's largest and most-trusted marketplaces. The engineering team is in growth mode and plans to grow the size from 25 to 35+ in the next 3-6 months. With multiple projects going on, they are looking for a seasoned technologist who is experienced with Python to join the team.
Ideal experience
3+ years of experience with data engineering
Must have some coding expertise with Python and/or R
Snowflake experience a STRONG Nice to have (not a dealbreaker)
Experience with Informatica and/or Tableau a big plus!!
Will be working through a number of business problems
Balancing of the market place data, developing data pipelines surrounding supply/demand, identifying how to manage S/D within platform
Privacy oriented projects – ensuring user data is securely handled, maintained and stored
Some predictive analytics based on history data to determine marketplace behaviour
Job Types: Full-time, Permanent
Additional pay:
Bonus pay
Benefits:
Casual dress
Extended health care
Flexible schedule
Paid time off
RRSP match
Work from home
Schedule:
8 hour shift
Monday to Friday
Experience:
Date Engineering: 2 years (Required)
Python or R: 2 years (Required)
Building Data Pipelines: 2 years (Required)
Work remotely:
Yes, always"
Backend Software Engineer (Data & Insights),"Burnaby, BC",Food-X,None,Organic,"Overview:
FoodX Technology (“FoodX”) is a grocery delivery platform for third party grocers. We are committed to developing and sharing sustainable practices with other retailers to help reduce the environmental impact of delivering groceries. Think about it as carpooling for your groceries. We are a passionate team of nice people, who want to make a difference, because we believe that sustainability works best when shared. If the idea of helping to reduce carbon emissions, food waste, and single use plastics resonates with you, please apply to join our team now!

FoodX is looking for a full-time, permanent Backend Software Engineer with a database expertise focus to work on the Data and Insights team. Our head office is our South Burnaby, BC (#105 5566 Trapp Avenue). Remote work in GVRD is acceptable.
Duties & Responsibilities:
Data and Insights Team

The Data and Insights team is responsible for helping power the FoodX's end-to-end grocery and warehouse management solution with data and intelligence. Our work will produce outcomes for millions of customers at some of the world’s most beloved sustenance providers. If we do our job right, we will be helping everyone eat fresher, healthier and more sustainably.

Our team produces tools and insights that make grocery operations more productive and safe, and internal tools that help FoodX teams understand how to achieve higher level performance for our Fortune 100 enterprise clients. We aim for clarity, performance and ever improving customer outcomes.

In this role, you will have the opportunity to:
Help architect scalable data infrastructure of FoodX, the worlds most advanced end-to-end system of intelligence for grocery fulfillment
Collaborate on organizational standards on data, streaming and APIs that will empower your peers to craft exceptional software
Design, development and maintenance of data services that help improve the operations of grocery warehouses around the globe
Provide lines of sight on data to other teams, to help them be effective
Additional duties and projects as assigned

Qualifications & Requirements:
We're looking for someone who:
Thinks, speaks and writes clearly in a way that helps others understand and make decisions
Adjusts quickly to changing priorities and copes effectively with complexity and change
Able to determine priorities, constraints, and offer proposals and solutions that balance short term needs, with long term strategy
Plans, organizes, schedules, and budgets in an efficient, productive manner
Is able to produce significant output with minimal wasted effort
Is eager to teach and learn from your team. We value making each other successful

Skills we’re looking for:
Experience designing and building highly performant data systems
Database design and performance expertise, and experience getting “dirty with the data”
Experience leading software development teams and is dedicated to helping your team and others grow
3+ years of experience with at least one of the following languages: node.js, python, .net Core, Java
2+ years working with devops in GCS, AWS, AZURE, as well as API stubbing tools (swagger, API Management, JIRA or MSFT DevOps productivity tools
Demonstrated experience with databases and distributed systems at scale
Proven track record of delivering well-designed and tested software
Rockstar SQL abilities
Working Knowledge of Apache Spark/Kafka/Scala based events handling
Workign knowledge of Evenhubs, and Azure storage/DB eco-system
Compensation:
An annual salary, paid bi-weekly
20% discount on SPUD.ca purchases
Extended Health Care Benefits (effective the first of the month after 90 days) including: massage therapy, acupuncture, naturopath, physiotherapy, psychologist, chiropractor, 80% dental and prescriptions covered, life insurance, travel insurance, discounted eye exam- and more"
Data Engineer,"Markham, ON",Pet Valu,None,Organic,"Location: Markham, Ontario
Job Description:
Job Overview: Pet Valu is looking for a dynamic individual with retail experience and passion for using data and analytics to drive business results to help us build the foundation to use our rapidly expanding data lake to improve business decisions. As a member of the Marketing team, the Data Engineer will be a key contributing resource to continually measure, evaluate, and make recommendations on our website marketing efforts as well as ecommerce user-experience.
Essential Details and Responsibilities
Working in a retail environment, the incumbent will design, implement, track performance and refresh advanced analytic automated segmentation and predictive models
Monitor Data quality with IT to ensure robust and accurate data
Partner with IT to define data solutions from new data sources, and build requirements for extraction into the data lake
Develop an strong acumen in source and downstream data storage system, and understand how the data is associated with business actions and potential solutions
Will function as the primary liaison between marketing and IT
Experience, Education, Certifications (List minimum required to enter the role.)
5+ years designing data processes to automate organizational decisions ideally in a retail organization
Bachelor's Degree in Statistics, Business, Quantitative Economics, Mathematics, Marketing, Economics, Engineering, Operations Research or similar programs
Competencies and Skills
Strong Personal Drive for Excellence
Excellent organizational and time management skills, with the ability to manage multiple priorities in a high demand environment
Great sense of urgency and accountability, results-oriented with strong execution skills
An independent problem solver, must be able to find creative solutions to unusual or unprecedented questions
Quick learner – eagerness to learn about new tools and business systems to help craft solid solutions
Technology
Experience in relational and cloud data storage using advanced SQL, using technologies such as Snowflake, Oracle, SQL Server
Must be proficient in R, Python, Alteryx, Azure ML, or similar Machine Learning and data processing technology,
Solid understanding of BI tools such as Tableau, MicroStrategy or QlikView
Experience in Spark, Kafka, JAVA a strong plus
Proficiency in processing Web Analytic raw data, ideally from Google Analytics 360, a large plus
Snowflake, Orcale, SQL ServerSnowflake, Oracle, SQL Server
Must be proficient in either R or Python
Solid understanding in data visualization tools such as Tableau, MicroStrategy or QlikView
Experience in Spark, Kafka, JAVA a strong plus
Very comfortable with MS office (Word/Excel/Access/PowerPoint)
Understanding of using SQL against relational databases
Experience in data visualization and reporting tools, such as Tableau
#INDC"
Data Engineer,"Toronto, ON",MEDCAN,None,Organic,"About Us :

Established over 30 years ago, Medcan is a global leader in assessing clients' overall well-being and inspiring them to live well. Medcan has a comprehensive range of diagnostic assessments, which, in combination with innovative programs tailored to specific needs, are designed to successfully reach improved health outcomes.
Over 80 physicians and specialists, alongside a broad roster of complementary health care disciplines, provide health and lifestyle management services on site and by video consult for individuals, families and organizations. Our team of 550 staff see clients physically and virtually from our downtown Toronto location.
The Role
Do you have a background in data or informatics? Do you understand the importance well-maintained and managed data as it relates to the overall operations and strategy of a business? Do you like solving interesting problems?
Medcan is searching for a Data Engineer to join a small but mighty (and growing) Analytics team. Reporting to the Manager, Business Insights, the Data Engineer will be responsible for building Medcan’s data warehouse and refining our data collection and consolidation efforts. The successful candidate will be the technical lead on the creation of a data warehouse that consolidates information from a variety of systems into a functional dimensional database.
If you’re interested in being involved in an exciting data product from inception to completion, we invite you to apply for this job.
The Accountabilities:
Build data systems and pipelines to collect and store data from Salesforce, Workday, the EMR and legacy platforms
Analyze and organize raw data to support management and operational needs
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues
Work closely with all business units and operational teams to implement strategy for long term data platform architecture
About You:
You understand the importance of data structure for use in both business strategy, operations planning, and forecasting.
You are a quick learner and are willing to undertake both formal training and self-led knowledge building.
You leverage business requirements to create your technical solutions.
You are comfortable working remotely in a small team as part of a larger enterprise.
You have a roll up your sleeves and get stuff done attitude.
You are a critical thinker who easily adapts to change.
You understand the vital role data plays in analyzing and delivering amazing customer experiences.
The Requirements:
2+ years experience designing, building and maintaining SQL databases
Ability to manage and communicate data warehouse plan
Strong organizational skills
Experience with schema design, dimensional data modeling and ETL processes
Fluency in any of: Python, Java, C++
Familiarity building database connections to Salesforce, Workday or Accuro EMR an asset
Knowledge of MuleSoft or other ETL tools a major asset
The position offers an attractive total compensation package including a base salary, a comprehensive benefits plan and a genuine opportunity to grow at one of Canada's Best Managed Companies. The position also offers outstanding health and wellness benefits through Medcan's Live Well program.
In your cover letter please include why you wish to align yourself with Medcan and how you believe you can add value to our organization.
We thank all applicants for their interest; however only those selected for an initial interview will be contacted. No phone calls and no agencies please."
Senior Data Engineer,"Toronto, ON",Pinterest,None,Organic,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It’s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. As a Pinterest employee, you’ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You’ll grow as a person and leader in your field, all the while helping users make their lives better in the positive corner of the internet.
The mission of the Business Intelligence team is to empower high-impact decisions at Pinterest by engineering data drive solutions. We are looking for a senior Business Intelligence Engineer who will design, build and maintain the most critical data sets and visualizations for our company. And as the lead for one of our verticals like Ads, Shopping, Infrastructure, Sales and Finance - you’ll be working directly with the Analysts, Data Scientists and Business Leaders in that area to ensure they have the data they need.
What you’ll do:
Understand the business drivers and analytical use-cases and translate these to data products
Explore new technologies and learn new techniques to solve business problems creatively
Think big and drive the strategy for better data quality within Pinterest
Design, implement and maintain pipelines that produce business critical data reliably and efficiently using cloud technology
Become the voice of business within engineering, and of engineering within business
Create data visualizations that allow easy consumption of the data learnings and insights
Collaborate with many teams from Product, Engineering and Business to produce relevant data solutions that can be used across multiple use cases
What we’re looking for:
5+ years of experience with big data (Hadoop, Hive, Presto, Spark), scripting language (Python) and data visualization (Tableau) technologies
Hands-on experience in principled data warehouse design, data visualization and data pipeline design and development
Prior experience working with business stakeholders in the technology space is a plus
Great communication skills. You should be able to directly communicate with senior business leaders, embed yourself with business teams, and present solutions to business stakeholders
Experience in working independently and driving projects end to end
Strong analytical skills
#LI-MJ1"
Data Engineer (II & SR) (Retail Product),"Toronto, ON",Lightspeed POS,None,Organic,"As a part of Lightspeed's Retail product group, you will be contributing to initiatives that will expand our reach into new markets and countries while enhancing our omnichannel commerce platform. Your work will also have a direct impact on supporting new and existing customers on their quest to enter the cloud era. At Lightspeed, we are dedicated to bringing cities and communities to life by powering SMBs. Come and help us build our communities!
What you'll be responsible for
Work alongside some of the most brilliant minds in the industry, you will contribute to omnichannel product by building new features and creative workflows.
Report to the Development Manager and have frequent interactions with Product Managers, Product Designers as well as Platform Teams.
Take part in daily agile ceremonies, brainstorm on innovative ideas that challenge the status quo and implement solutions
Act as an advocate for highest code quality and ownership, while contributing to the health of the infrastructure
Raise the bar and elevate your team by sharing knowledge and best practices, while always seeking improvement and progress
What you'll be bringing to the team
Senior level experience developing reliable, highly available and scalable software (preferably in a scalable SaaS setting)
Strong experience developing software in a modern cloud infrastructure
Experience programming in Python
Experience with data modelling and writing optimized SQL
Experience with relational and non-relational databases
Strong knowledge of Data Warehousing and Big Data Technologies
Experience with TDD and writing automated unit and functional tests
Proven skills in server side resource profiling, optimization and debugging
Good understanding of Software Design Patterns
Basic proficiency in a UNIX/Linux environment
Excellent communication skills and ability to mentor teammates
Experience working in an Agile development environment
Experience with Git/Github
Even better if you have, but not necessary
Experience in a continuous delivery model
Experience with Golang and/or PHP
Experience in building APIs
Experience with event based and messaging systems
Experience deploying and maintaining software in a production environment
Who we are
Lightspeed (TSX/NYSE: LSPD) powers small and medium-sized businesses with its cloud-based, omni-channel commerce platforms in over 100 countries around the world. With smart, scalable, and dependable point of sale systems, Lightspeed provides all-in-one solutions that help restaurants and retailers sell across channels, manage operations, engage with consumers, accept payments, and grow their business.
Headquartered in Montréal, Canada, Lightspeed is trusted by favourite local businesses, where the community goes to shop and dine. Lightspeed has offices in Canada, USA, Europe, and Australia.
We're passionate about enabling people to do their best work. Come work with us and find out what you can do!"
Lead Data Engineer,"Toronto, ON","Amazon Web Services Canada, In",None,Organic,"Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, Mathematics or a related field
8+ years of experience of IT platform implementation in a highly technical and analytical role.
5+ years’ experience of Data Lake/ platform implementation, including 3+ years of hands-on experience in implementation and performance tuning /Spark implementations.
Ability to think strategically about business, product, and technical challenges in an enterprise environment.
Experience with analytic solutions applied to the Marketing Risk needs of enterprises
Highly technical and analytical, possessing 5 more years of IT platform implementation experience.
Understanding of Apache and the ecosystem. Experience with one more relevant tools (Sqoop, Flume, Kafka, Oozie, Hue, Zookeeper, HCatalog, Solr, Avro).
Familiarity with one or more SQL-on- technology (Hive, Impala, Spark SQL, Presto).
Experience developing software code in one or more programming languages (python, scala, etc)
Current hands-on implementation experience required

At Amazon Web Services (AWS), we’re hiring highly technical cloud computing architects and engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.
In this role, you will work with our partners, customers and focus on our AWS Analytics and ML service offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.
AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.
In our Professional Services Delivery Practice, you will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers, and, most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.
The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.
Excellent business and communication skills are a must to develop and define key business questions and to build data sets that answer those questions. You should be able to work with business customers in understanding the business requirements and implementing solutions.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.

Masters PhD in Computer Science, Physics, Engineering Math.
Hands on experience leading large- global data warehousing and analytics projects.
Ability to lead effectively across organizations.
Understanding of and analytical technologies in the industry including MPP and databases, Data Warehouse , BI reporting and Dashboard development.
Demonstrated industry leadership in the fields of , data warehousing data sciences.
Implementation and tuning experience specifically using Amazon Elastic Map Reduce (EMR).
Implementing services in a variety of computing, enterprise environments.
Computer Science Math background preferred.
Customer facing skills to represent well within the customer’s environment and drive discussions with senior personnel regarding trade-offs, best practices, project management and risk mitigation. Should be able to interact with Chief Marketing Officers, Chief Risk Officers, Chief Technology Officers, and Chief Information Officers, as well as the people within their organizations.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.

Amazon is committed to providing employment accommodation in accordance with the Ontario Human Rights Code and the Accessibility for Ontarians with Disabilities Act. If contacted for an employment opportunity, please advise Human Resources if you require accommodation."
Data Engineer,"Calgary, AB",Blackline Safety,None,Organic,"Blackline Safety is looking for a Data Engineer to help build and drive our Blackline Analytics and Blackline Vision platform. We have a world class connected safety system and we are building a Data analytics, and Data science platform to match it.
Who you are:
You believe big data helps businesses solve real-world problems. You have an aptitude for engineering and manipulating various forms of data frameworks including but not limited to Data warehousing, Modeling, Analytics, Integrations from various data sources. You are not bound to a single tool or platform and can solve problems in creative ways. You are proficient in different computing languages such as SQL, Python, DAX and aspire to learn alternate ways of resolving issues. You trust in data driven problem solving and believe visualizing large datasets is a necessity to achieve that. You like to explore datasets to find insights that nobody else sees. You can communicate these ideas through visualizations. You are team-oriented, self-motivated, creative and excited to find answers to questions that the answer might not be obvious without going through large amounts of data.
What you will do:
You have an ability and desire to work in our collaborative environment: open team room, pair programming and fluid interactions with all products and operations teams. You will be a part of a high performing team that is focussed on building solutions utilizing various approaches including agile; capable of digesting real time feedback and working smartly to advance Blackline Analytics, and Blackline Vision platforms. You are self-driven, need minimal supervision and are comfortable pushing your own projects and getting things done.
You will be responsible for expanding and optimizing our data pipeline architecture, as well as optimizing data flows and collection for cross functional teams. The pipeline needs to be scalable, precise, repeatable, secure and accurate. You will work with some of the largest and most varied data sets (both batch and real-time) in wireless industry. You will expand and develop the Blackline analytics platform alongside other data analysts and data scientists to make data driven decisions, build innovative data products and roll out advanced analytics. Your contribution will help us increase internal efficiencies and evolve our products and services by leveraging large and diverse datasets generated by customers from around the world.
About Blackline
Blackline Safety is a world leader in the development and manufacturing of wirelessly connected safety products — we offer the broadest and most complete portfolio in the industry. We monitor personnel working alone in populated areas, indoors within complex facilities, and employees operating in the most remote reaches of our planet. Our products are used to keep people safe in the event of falls, missed check-ins, man-downs and exposure to explosive or toxic gas. Our products save lives. Our design, marketing, customer care, sales and production are all performed in-house at our headquarters in Calgary, AB. Blackline Safety is a publicly traded company (TSXV: BLN). Blackline Safety’s headquarters are in an historic building in Ramsay. We have a robust COVID protocol, with employees working both in the office and remotely.
Requirements:
Experience with SQL, DAX, Python programming languages*
Experience building ‘big data’ pipelines, architectures, and datasets
Experience with Amazon AWS, RedShift and other cloud platforms
Experience with Databricks, Object Oriented Development in Scada/Java Platforms
Experience with Agile methodologies as well as familiar with CI/CD tools (Jenkins, Travis, github)
Experience in ELT/ETL and Data Modeling; Translate complex functional and technical requirements into detail design implementing ELT/ETL process for Integration of data from different sources.
Experience in designing and implementing streaming applications is preferred
Fully understand standard architecture methodologies, processes and best practices
Background in Machine Learning and productizing Data Intelligence pipelines
Strong communication skills and commitment to being a team player
Strong analytical abilities and proven design skills
Strong sense of ownership, high sense of integrity and an incredible work ethic
Proven leadership abilities in an engineering environment in driving operational excellence and best practices
Optional:
Knowledge of NetSuite, Redshift and other 3rd party software integrations
Knowledge of Satori Reporting or any other turnkey integrative reporting
Knowledge of Power platform operations, query, flow and BI and integrations
Blackline Safety offers:
An exciting high-growth environment
An experienced, dynamic and motivated team
Supportive, challenging and collaborative work
Competitive salary and vacation
Medical, dental and drug benefits
Company stock purchase plan with matching contributions
Our clients depend on Blackline Safety to monitor the wellbeing of their employees at work — you can help to make a difference. Come work with Blackline Safety in an exciting, fast-paced work environment.
Job Types: Full-time, Permanent
Schedule:
Monday to Friday
Experience:
data programming : 2 years (Preferred)
Education:
Bachelor's Degree (Preferred)
Work remotely:
Temporarily due to COVID-19"
Big Data Engineer - Analytics,"Ottawa, ON",Micro Focus,None,Organic,"Job Description:

Interset Software uses big data and advanced behavioral analytics to detect and prevent the theft of intellectual property...simply put, WE CATCH BAD GUYS WITH MATH!!!
Part of the Micro Focus group of companies, we are a fast-paced, all-hands-on-deck kind of environment where you are respected and listened to from day one. We have a start-up feel within the stability and structure of a large global company.
We are currently looking to fill a development position focused on extending the existing analytics platform and related capabilities to add unprecedented analytics flexibility for our customers. This will include enabling Data Scientists to manipulate and combine events and models to extend and customize the analytics in ways that provide unique value for each customer.
We’re looking for a software developer who’s passionate about what they do, takes a creative approach to problem solving and will be the champion for creating innovative machine learning hooks that deliver real value and perform in big data environments.
If you’re passionate about true machine learning and want to be part of a company building solutions that leverage the latest in big data technology, we want to talk to you!!
What you'll do:
Implement model data flows to support running cutting-edge machine learning techniques on massive amounts of data
Work with product managers and data scientists to turn new features and algorithms into beautiful, battle-tested code
Work with the technologies we use to analyze and identify cyber-security threats for our customers (Elasticsearch, Spark, HBase, Kafka, Vertica, NiFi, using Java and Scala)
Work side by side with some of the smartest minds in the fields of machine learning and behavioural analytics
Create efficient and robust cloud-based solutions, leveraging the best in cloud technologies.
Who you are:
Undergraduate or Masters’ degree in Computer Science or equivalent engineering experience
Strong interest in software design, distributed computing, and databases
Experience developing in a JVM environment (Java, Scala, Clojure)
At least two years of experience developing with or using Big Data & Analytics stacks/tools such as Hadoop, HBase, Spark, Presto and Vertica.
Experience implementing and using streaming platforms such as SparkSQL, Flink, Kafka, Storm, etc.
Experience with Kubernetes, Docker, Ansible or any other infrastructure or containerization management/automation platform.
Familiarity leveraging AWS EMR, Azure, GCP cloud technologies best practices to enable the distribution and analysis of big data on the cloud would be considered an asset.
Nice to haves:
Familiarity with data science or machine learning packages (pandas, R, TensorFlow, etc...)
Familiarity with virtualization technologies (VMWare ESX, Docker)
Contributions to open source software (code, docs or mailing list posts)
Interest in understanding and analyzing diverse types of data
#LI-DK1
#DiceDK
Job:
Engineering

Micro Focus is proud to be an Equal Opportunity Employer. Prospective employees will receive consideration without discrimination because of race, colour, religion, creed, gender, national origin, age, disability, marital or veteran status, sexual orientation, genetic information, citizenship or any other legally protected status"
Senior Data Engineer,"Toronto, ON",fabriik,None,Organic,"Our mission at Fabriik is to assemble the best minds across Fintech, Digital Assets, A.I., Quant and Behavioural Finance to shift our industry.
It is an exciting time here at Fabriik as we expand our global team. We offer meaningful and rewarding career opportunities within a collaborative and engaging work environment where innovation thrives. Our talented team is passionate about moving the world into a frictionless, digital-first financial future.
We are looking for an experienced Senior Data Engineer to join our growing global Technology team.

Role Overview
Are you an experienced data engineer looking for the next big challenge in your career? Does the opportunity to shape the modus operandi of a fledgling data engineering team excite you?
Here at Fabriik Labs, our objective is to tame the wild world of crypto-financial data, channeling anything and everything available to us into intelligent trading solutions, business decisions, and more. We need talented and experienced engineers, perhaps like yourself, to help lead the way on that journey.
Ours is a new team; our only cultural values are supportiveness, decisiveness, personal accountability, and a reverence for technical quality. The rest is up to you to shape.

Key Responsibilities
Delivering new components of our data processing pipeline
Rigorously testing these components to ensure reliability
Designing new data pipelines to meet business requirements
Developing new libraries to be used across our software systems
Integrating with external and internal APIs to initiate new data feeds
Supporting teammates by reviewing their code and being solving problems collaboratively
Guiding junior teammates as they learn the way of the data engineer

Candidate Profile
Minimum qualifications:
Significant professional experience in the field of data engineering
Professional experience in the Scala or Java programming languages
Expertise in software testing
Fluency in the git version control system
A mind open to unfamiliar programming paradigms and perspectives
Preferred qualifications:
Professional experience working with Apache Kafka
Professional experience working with Apache Spark
Experience with the Akka library for Scala/Java
Welcome but in no way prerequisite qualities:
Enthusiasm for blockchain, cryptocurrencies and crypto market
If this sounds like a role that could be an exciting next step in your career, our team looks forward to hearing from you!
At Fabriik, we value diversity of all types. Our team is made up of smart, collaborative and thoughtful people with a wide range of backgrounds, skills and experiences.
Fabriik is an equal opportunity employer and we are committed to a diverse and inclusive workplace. We welcome applications from qualified individuals from all backgrounds. Accommodations are available upon request in all phases of the selection process. We thank all applicants for showing an interest in this position. Only those selected for an interview will be contacted."
"Data Engineer, Vancouver","Vancouver, BC",Rad Power Bikes,None,Organic,"Why We’re Rad (about us):
Rad Power Bikes is a leading consumer direct ebike manufacturer specializing in high quality yet affordable electric bikes for weekend warriors, hardcore commuters, and family cyclists.

Madly growing and located in Seattle, WA we are seeking a gifted Data Engineer to join our Vancouver team to help us design, build, and maintain the data-foundation that powers our warehouse and technologies that are central to Rad’s day-to-day operation, providing critical business value every day.

Our Data Engineer will be responsible for managing the interchange of data between systems, servers, employees and our Rad Riding customers. We are looking for someone who can architect, design and implement data processes that are crucial to Rad’s data-driven decisions. This role reports into our Director of Engineering and is part of a cross-functional Technology department.
Why You’re Rad (about you):
5+ years of experience in a data engineering role, working with big datasets
Knowledge of professional software engineering practices & best practices for full software development life cycle, including coding standards, code reviews, source control management, continuous integration/deployments, testing and operations
Strong desire to work in a team environment
Ability to think and solve complex data engineering problems in a maintainable and scalable fashion
Desire to find new and better ways of doing things, generating original and imaginative ideas, products, and/or solutions
Desire to embrace documentation creation & maintenance
Ability to work independently and collaboratively with both local & non-local teammates
Strong problem solving and organizational skills
Impeccable written & oral communication skills with strong attention to detail
Additional Requirements:
Bachelor’s degree in Computer Science, Mathematics, Electrical Engineering or equivalent work experience
Expert knowledge and proficiency with SQL and no-SQL databases.
Familiarity with Big Data tools such as Google’s BigQuery
Experience with Visualization tools and libraries such as Power BI or Google’s Data Studio
Strong understanding of Dev Ops practices with a good understanding of AWS and Google Cloud infrastructure
You get bonus points for:
Experience working with Shopify’s systems and API
Experience with cloud-based ERPs and Warehouse Management Systems
Experience with NetSuite
Experience with GraphQL, BigQuery and Postgres
Open-source contributions or have personal projects you’ve shipped successfully
Project management experience
Had you been with us last month, you would have:
Developed business intelligence and reporting solutions
Utilized and built out the data warehouse foundation and set up the pipes for data sources
Managed the building of data and documentation across company wide systems
Worked cross functionally with all teams, including Supply Chain, Customer Experience, etc. to capture and report on inventory data
Evaluated the feasibility and effectiveness of proposed data solutions
Designed schemas, optimized data transformation, and managed deployments and maintenance of the data pipeline
Completed end-to-end administration and maintained tools, systems, pipelines and ETLs
Communicated designs, issues and trade-offs to stakeholders
Helped develop the foundational data stores that power our internal/external web apps and reporting systems
Dealt with 3rd party vendors, managing the relationships & troubleshooting high-level issues
Additional duties and overtime as required
Rad Power Bikes is proud to be an Equal Opportunity Employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.

If you need assistance or an accommodation due to a disability, you may contact us at 800-939-0310 or jobs@radpowerbikes.com.

Recruitment Agencies: Although we value the services you provide, at this time we are not leveraging external 3rd party recruitment resources for this search. Should those needs change, we will seek your assistance directly."
"Staff Software Engineer, Data Engineering","Burnaby, BC",Zscaler,None,Organic,"Position: Staff Software Engineer, Data Engineering

Location: Burnaby, BC
For over 10 years, Zscaler has been disrupting and transforming the security industry. Our 100% purpose built cloud platform delivers the entire gateway security stack as a service through 150 global data centers to securely connect users to their applications, regardless of device, location, or network in over 185 countries protecting over 3,900 companies and 100 Million threats detected a day.
We work in a fast paced, dynamic and make it happen culture. Our people are some of the brightest and passionate in the industry that thrive on being the first to solve problems. We are always looking to hire highly passionate, collaborative and humble people that want to make a difference.
The Staff Software Engineer, Data Engineering will create the next generation of Zscaler's security analytics platform. The candidate will help build a platform to collect and ingest several billion (and growing) log events from Zscaler's globally distributed security infrastructure and provide actionable insights to customers and Zscaler's security researchers.
Responsibilities:
Design and build multi-tenant systems capable of loading and transforming large volumes of structured and semi-structured fast moving data
Build robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users
Implement measures to address data privacy, security and compliance
Work with product management, marketing, security research teams to identify requirements and evolve data architecture
Required:
10+ years of hands-on experience working with software development and enterprise data warehouse solutions
Must be proficient with coding in Java/Scala, some Python would be a plus
Must have expertise with the Spark platform - building a Spark infrastructure
Must have expertise with Hadoop architecture (Ability to set-up a Hadoop cluster from scratch and maintain, troubleshoot and tune by reviewing logs from various Hadoop services
3+ years of experience in building high performance data processing infrastructure taking into account concurrency, latency and efficiency by profiling, reviewing logs etc
2+ years working with query engines such as Presto, Hive or Spark SQL preferred
Identify the right kind of data serialization techniques and data stores for persisting events
Strong understanding of the Hadoop stack - HDFS, map-reduce, YARN/Mesos, Zookeeper
Working with data processing frameworks such as Spark, Kafka, Storm, Elastic search for 2+ years
Experience architecting systems following the REST architectural style
Excellent interpersonal, technical and communication skills
Ability to learn, evaluate and adopt new technologies
Ability to prioritize multiple tasks in a fast-paced environment
Bachelor's Degree in computer science or equivalent experience
Highly Desirable:
Experience working with AWS - EC2, S3, EMR, Redshift, etc.
Experience working with Spark
Basic understanding of statistical analysis.
Practical experience with the Scala programming language
Application of machine learning to security log analytics
What You Can Expect From Us:
An environment where you will be working on cutting edge technologies and architectures
A fun, passionate and collaborative workplace
Competitive salary and benefits, including equity
The pace and excitement of working for a Silicon Valley Unicorn
Why Zscaler?
People who excel at Zscaler are smart, motivated and share our values. Ask yourself: Do you want to team with the best talent in the industry? Do you want to work on disruptive technology? Do you thrive in a fluid work environment? Do you appreciate a company culture that enables individual and group success and celebrates achievement?

If you said yes, we’d love to talk to you about joining our award-winning team!

Learn more at zscaler.com or follow us on Twitter @zscaler. Additional information about Zscaler (NASDAQ : ZS ) is available at http://www.zscaler.com. All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability.

#LI-JM1"
Frontend Software Engineer (Data & Insights),"Burnaby, BC",Food-X,None,Organic,"Overview:
FoodX Technology (“FoodX”) is a grocery delivery platform for third party grocers. We are committed to developing and sharing sustainable practices with other retailers to help reduce the environmental impact of delivering groceries. Think about it as carpooling for your groceries. We are a passionate team of nice people, who want to make a difference, because we believe that sustainability works best when shared. If the idea of helping to reduce carbon emissions, food waste, and single use plastics resonates with you, please apply to join our team now!

FoodX is looking for a full-time, permanent Frontend Software Engineer to work on the Data and Insights team. Our head office is our South Burnaby, BC (#105 5566 Trapp Avenue). Remote work in GVRD is acceptable.
Duties & Responsibilities:
Data and Insights Team

The Data and Insights team is responsible for helping power the FoodX's end-to-end grocery and warehouse management solution with data and intelligence. Our work will produce outcomes for millions of customers at some of the world’s most beloved sustenance providers. If we do our job right, we will be helping everyone eat fresher, healthier and more sustainably.

Our team produces tools and insights that make grocery operations more productive and safe, and internal tools that help FoodX teams understand how to achieve higher level performance for our Fortune 100 enterprise clients. We aim for clarity, performance and ever improving customer outcomes.

In this role, you will have the opportunity to:
Help design the worlds most advanced insights and business intelligence solution for grocery fulfillment
Set organizational standards for UX and visualization that will empower your peers to craft exceptional software
Design, development and maintain reactive dashboards, and mission critical reports that help improve the operations of grocery warehouses around the globe
Provide lines of sight on visualization to other teams, to help them be effective
Additional duties and projects as assigned
Qualifications & Requirements:
We're looking for someone who:
Thinks, speaks and writes clearly in a way that helps others understand and make decisions
Adjusts quickly to changing priorities and copes effectively with complexity and change
Able to determine priorities, constraints, and offer proposals and solutions that balance short term needs, with long term strategy
Plans, organizes, schedules, and budgets in an efficient, productive manner
Is able to produce significant output with minimal wasted effort
Is eager to teach and learn from your team. We value making each other successful

Skills we’re looking for:
Experience designing and building real time interactive dashboards for operations and business users
SASS, LESS, SCSS wizardry
3+ years with Reactful Frameworks (React, Vue)
D3.js experience
Visual Component Design and Maintenance Strategy
2+ years working with Git abnd JIRA or MSFT DevOps productivity tools
Demonstrated experience transforming data into insight
Proven track record of delivering well-designed and tested software
Excellent documentation skills
Compensation:
An annual salary, paid bi-weekly
20% discount on SPUD.ca purchases
Extended Health Care Benefits (effective the first of the month after 90 days) including: massage therapy, acupuncture, naturopath, physiotherapy, psychologist, chiropractor, 80% dental and prescriptions covered, life insurance, travel insurance, discounted eye exam- and more"
Software Engineer – Data and Machine Learning Infrastructure,"Toronto, ON",TD Bank,None,Organic,"Tell us your story. Don't go unnoticed. Explain why you're a winning candidate. Think ""TD"" if you crave meaningful work and embrace change like we do. We are a trusted North American leader that cares about people and inspires them to grow and move forward.

Stay current and competitive. Carve out a career for yourself. Grow with us.

Department Overview

Overview

Headquartered in Toronto, Canada, with more than 85,000 employees in offices around the world, The Toronto-Dominion Bank and its subsidiaries are collectively known as TD Bank Group (TD). TD is the sixth-largest bank in North America by branches offering a full range of financial products and services to approximately 24 million customers worldwide through three key business lines:

Canadian Retail including TD Canada Trust, Business Banking, TD Auto Finance (Canada), TD Wealth (Canada), TD Direct Investing and TD Insurance
U.S. Retail including TD Bank, America’s Most Convenient Bank, TD Auto Finance (U.S.), TD Wealth (U.S.) and TD’s investment in TD Ameritrade
Wholesale Banking including TD Securities
TD also ranks among the world’s leading online financial services firms, with approximately 10 million active online and mobile customers and had CDN$1.1 trillion in assets on July 31, 2015. Our mission is to give our clients the best banking experience possible, every day. To do that, we depend on our team of talented, ambitious people who share our passion for excellence. Join the innovators of TD Technology.
We know that tech is constantly evolving, and we’re committed to growing with it, right across the board. Our Technology Solutions team works closely with each department at TD to create the platforms, applications, and ideas that shape the future of our business – and yours. We’re reimagining the way people think about their banking, every single day. This is your opportunity to impact the future of banking technology.

Job Description

The Role
Software Engineer – Data and Machine Learning Infrastructure
The Opportunity
The Cross Asset Platform, Data & Innovation (CAP) team at TD Securities is responsible for defining and executing on the bank's next gen data and advanced analytics strategy.

The CAP team is looking for a skilled engineer with experience working in at least one of the different data and advanced analytics technology domains such as distributed data systems, stream processing, information retrieval, machine learning, data governance, security and operations to help design, develop and operate the bank's next generation data and machine learning infrastructure.

Job Requirements

Overview

More specifically, the Data & ML Infrastructure Engineer will:

Work on modern data preparation, integration and AI-enabled metadata management tools and techniques
Architect and govern data consumption patterns for consumers to ensure both the security and performance of business applications
Govern and monitor schema changes for datasets.
Evaluate new and emerging data ingestion technologies such as streaming and microservice orchestration
Collaborate with our data science teams and business (data) analysts in refining their data requirements for various data and analytics initiatives and their data consumption requirements
Monitor new data best practices while ensuring it’s shared with teams appropriately and implemented where needed
Have experience in data management disciplines including data integration, modelling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks
Have experience working in cross-functional teams and collaborating with business stakeholders in Microsoft Azure in support of a departmental and/or multi-departmental data management and analytics initiative
Have strong experience with advanced analytics tools for Object-oriented/object function scripting using languages such as R, Python, Java, C++, Scala and others
Have working ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management
Have solid experience working with popular database programming languages including SQL, PL/SQL and others for relational databases and certifications on upcoming NoSQL/Hadoop-oriented databases like MongoDB, Cassandra and others for nonrelational databases
Proven work experience tackling large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies such as ETL/ELT, data replication/CDC, message-oriented data movement, API design and access and upcoming data ingestion and integration technologies such as stream data integration, CEP and data virtualization
Proficient working with at least a subset of open source distributed data systems such as Hadoop, Spark, Kafka, Hive, HBase, Druid with a strong grasp of fundamental concepts of data engineering, warehouses, operational databases, messaging and data management.
Demonstrated ability to work across multiple deployment environments including cloud, on-premises and hybrid; multiple operating systems and through containerization techniques such as Docker, Kubernetes, Azure Kubernetes Service and others
Adept in agile methodologies and capable of applying DevOps and increasingly DataOps principles to data pipelines to improve the communication, integration, reuse and automation of data flows between data managers and consumers across an organization
Ability to work with data science teams in refining and optimizing data science and machine learning models and algorithms
A bachelor’s or master’s degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field or equivalent work experience is required
An advanced degree in computer science (MS), statistics, applied mathematics (PhD), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience is preferred.
The ideal candidate will have a combination of IT skills, data governance skills, analytics skills and Microsoft Azure knowledge with a technical or computer science degree
Additional Information

Join in on what others in TD Technology Solutions are doing:
Inspire a positive work environment and help champion quality, innovation, teamwork and service to the business.
Learn voraciously, stretch your thinking,

Inclusiveness

At TD, we are committed to fostering an inclusive, accessible environment, where all employees and customers feel valued, respected and supported. We are dedicated to building a workforce that reflects the diversity of our customers and communities in which we live and serve. If you require an accommodation for the recruitment/interview process (including alternate formats of materials, or accessible meeting rooms or other accommodation), please let us know and we will work with you to meet your needs.

Job Family

Engineering

Job Category - Primary

Technology Solutions

Job Category(s)

Technology Solutions

Hours

37.5

Business Line

Corporate

Time Type

Full Time

Employment Type

Regular

Country

Canada

**Province/State (Primary)

Ontario

City (Primary)

Toronto

Work Location

130 Adelaide Street West

Apply to job
Save
Send to friend"
Data Engineer,"Montréal, QC",PixMob,None,Organic,"As a Data Engineer, you will participate in the development of our state-of-the-art technology to deliver unique event experiences for the klik and Safeteams ecosystem built on proprietary connected BLE wearables and beacons, along with our award-winning event app.
What you'll do with us
Design a data pipeline from the ground up;
Work closely with the data scientists to design data processes and optimize the availability of data for cloud and non-cloud based data services;
Work closely with our Software team in the development of data infrastructures;
Implement and monitor data quality indicators across the entire data pipeline;
Participate in the evolution of the Safeteams and klik dashboard solutions and help define features to improve the platform.
What you'll bring
3+ years of experience designing and developing data pipelines, data cleansing routines utilizing typical data quality functions involving standardization, transformation, anomaly detection, etc.;
University degree in software engineering, computer science, or related field;
Proficiency in Python;
Proficiency in SQL, MongoDB and database design;
Experience with Apache Spark, Hadoop, Kafka or similar technologies;
Experience building data pipelines on Google Cloud Platform or AWS;
Proven ability to streamline and optimize data transformation processes;
Experience working on a platform to deliver a cloud-based data visualization service, a plus;
Team spirit and sense of responsibility towards the team;
Solid problem solving and disambiguation skills;
Excellent written and verbal communication skills in both French and English;
Able to work efficiently in a multifunctional team.

What we offer
Flexible working hours
4 weeks vacation
An attractive group insurance plan
A variety of social activities (within the new COVID reality)
A stimulating and inclusive company culture

We thank all applicants for their interest. However, only candidates selected for an interview will be contacted."
Data Engineer,"Cambridge, ON",Gore Mutual Insurance Company,None,Organic,"The Data Engineering practice at Gore Mutual Insurance is going through an exciting transformation to modernize technologies and approaches, and to better enable a data driven future for the company. As such, the Data Engineering team is looking for a new member to assist with several new and ongoing data migration, acquisition, and development projects, with the goal of expediting their delivery.
We are on a new path called NEXT HORIZON
The insurance industry is changing at an exponential pace, driven by changing customer experience, climate change and technology. In response Gore Mutual is making bold moves to reposition our company as a purpose driven, digitally led national insurer. Next Horizon is our 10-year vision and strategy that sets a new path for our company.
Come and make your mark on our Next Horizon!

What will you do?
In this role you will be part of architecting, engineering and solutioning the foundation of our Next Horizon initiatives. This will involve working with all of our business stakeholders to understand their data processes from present state and developing modern, scalable solutions to move our business forward.
In particular, you will:
Design, develop, and document new data solutions, including data pipelines and data warehouses
Collect and analyze requirements in collaboration with business teams and stakeholders to help inform development activities and objectives
Monitor, evaluate and continuously improve current data pipelines and systems
Collaborate and support other Data & Analytics team members with solution implementation and technology adoption

What will you need to succeed?
Undergraduate degree in a technical field such as Computer Science, Software Engineering, etc.
Proficiency and in-depth understanding of development best practices in Python and SQL.
Experience with MS Azure Cloud environment and toolkits including Kubernetes (AKS), Databricks, Azure SQL
Excellent understanding of data modelling and data warehousing best practices; familiarity with data warehouse and data mart design principles.
Knowledge and experience with modern data pipeline development and orchestration tools.
Experience working with and analyzing structured, semi-structured, and unstructured datasets, and identification and extraction of key metrics from large and disparate data sources.
Nice to Have
Graduate degree in a technical field with specialization in Analytics, Data Science, or a related subject
Experience with large scale data migration and data platform modernization projects
Azure and/or Databricks certifications

Gore Mutual Insurance is committed to providing accommodations for people with disabilities during all phases of the recruiting process, including the application process. If you require accommodation because of a disability, we will work with you to meet your needs. If you are selected for an interview and require accommodation, please advise the HR representative who will consult with you to determine an appropriate accommodation."
Backend/Data Engineer,"Toronto, ON",NLogic,None,Organic,"Backend/Data Engineer
Permanent
Toronto Ontario
About Us
We turn insights into action.
As Canada's leading provider of audience analysis tools for the TV and radio broadcast industry, media agencies, martech firms and more, we help our clients bring data to life. We’ve developed industry first APIs, media trading platforms and data integration systems that drive innovation and growth across the industry.
We could not achieve these accomplishments without our amazing team. We are passionate about hiring individuals who push us to be our best and to do more. We encourage them to speak up and share their ideas. We also support their thirst for training and development to ensure our employees build a strong future along with ours. But we’re not all work; we like to have fun too. An important part of building a cohesive and collaborative team is creating work-life balance. From flexible work hours to Friday social sessions, we know how to have fun. Even in a virtual world, we are constantly finding unique ways to bring us all together including virtual BINGO games, photo and cake decorating contests, online scavenger hunts and more.
About the Role
NLogic is looking for a hybrid Backend/Data Engineer to join a talented team of engineers that share a common interest of building distributed backend systems, big data, their scalability and continued development. This is a role for engineers that are familiar with standard web backend architecture, and capable in database design and interaction. Your ability to visualize the flow of data through a complex application is critical to your success, and to the team’s.
Are you: a strong believer and practitioner of the following:
You are an experienced Software engineer with particular expertise using Microsoft technologies
You have rich experience in building distributed, well-designed services.
You have worked with large-scale data systems. Experience working in a “big-data” environment (Hadoop, Hive, Spark, Snowflake, etc.) an asset
You have excellent understanding of system design, data structures, and algorithms
You have excellent debugging and optimization skills
You care about quality and attention to detail
You are energetic, eager and have a “curious” personality and approach
You have experience working with Cloud platforms such as Microsoft Azure
Responsibilities
Design, develop and operate business critical systems with focus on high availability, low latency and scalability.
Work on custom projects involving data and systems integration
Collaborate with other engineers, product managers and designers to solve challenging problems
Building new systems to securely store and retrieve large volumes of data for computational purposes
Building new tools for our operational teams
Qualifications and Experience
Knowledge and experience in Microsoft development stack, including .Net Framework, C#, Python, SQL Server; Big data systems experience like Databricks, Hadoop, Spark, etc.; Azure experience; Agile software development process; testing frameworks; knowledge of parallel/multithreaded algorithms and processing methods, algorithms and data structures, including expert knowledge in at least some of these areas.
What's in it for you
Projects - Opportunity to work on exciting projects and make an impact on the Canadian broadcast media industry
Work-life balance - In addition to a flexible summer hours program, we offer paid days off during the holiday closure between Christmas and New Year’s Day
Competitive benefits package and group savings and retirement program
Education reimbursement - For those wishing to pursue additional professional development, funding of up to $1000 per year is available through our education reimbursement program
Currently working in a remote environment
Values
Stronger together, Strive for better, Always learn, Be passionate
NLogic is an equal opportunity employer
We are committed to inclusive, barrier-free recruitment and selection processes and work environments. If you are contacted for a job opportunity, please advise the People and Culture department if any accommodations are needed to ensure you have access to a fair and equitable process. Any information received relating to accommodation will be addressed confidentially.
Job Types: Full-time, Permanent
Additional pay:
Bonus pay
Benefits:
Casual dress
Company events
Company pension
Dental care
Employee assistance program
Extended health care
On-site parking
Paid time off
RRSP match
Tuition reimbursement
Vision care
Wellness program
Work from home
Schedule:
8 hour shift
Experience:
Microsoft technologies: 5 years (Preferred)
Big Data: 2 years (Preferred)
Education:
Bachelor's Degree (Preferred)
Work remotely:
Yes, temporarily due to COVID-19"
Backend/Data Engineer,"Toronto, ON",Numeris,None,Organic,"Backend/Data Engineer
Permanent
Toronto, Ontario

About NLogic
We turn insights into action.
As Canada's leading provider of audience analysis tools for the TV and radio broadcast industry, media agencies, martech firms and more, we help our clients bring data to life. We’ve developed industry first APIs, media trading platforms and data integration systems that drive innovation and growth across the industry.
We could not achieve these accomplishments without our amazing team. We are passionate about hiring individuals who push us to be our best and to do more. We encourage them to speak up and share their ideas. We also support their thirst for training and development to ensure our employees build a strong future along with ours. But we’re not all work; we like to have fun too. An important part of building a cohesive and collaborative team is creating work-life balance. From flexible work hours to Friday social sessions, we know how to have fun. Even in a virtual world, we are constantly finding unique ways to bring us all together including virtual BINGO games, photo and cake decorating contests, online scavenger hunts and more.
www.nlogic.ca
About the role
NLogic is looking for a hybrid Backend/Data Engineer to join a talented team of engineers that share common interests around building distributed backend systems, managing data at scale, and driving agile development across the organization. This is a role for engineers that are familiar with standard web backend architecture, and capable in database design and interaction. Your ability to visualize the flow of data through a complex application is critical to your success, and to the team’s.
In this role, you would also display the following characteristics:
Strong attention to quality and detail
Energetic and eager to collaborate across departments, supporting corporate objectives
Curious and passionate in your approach to development
Qualifications and Experience
Knowledge and experience in:
Software development on Microsoft development stack including .Net Framework, C#, TFS
Building distributed, well-designed services
Data platforms and Big data systems like Databricks, Hadoop, Spark, Python, SQL, DAX etc.
Cloud platforms such as Microsoft Azure
Agile software development process
Parallel/multithreaded algorithms and processing methods, systems design, algorithms and data structures
Testing frameworks
Responsibilities
Design, develop and operate business critical systems with focus on high availability, low latency and scalability.
Work on custom projects involving data and systems integration
Collaborate with other engineers, product managers and designers to solve challenging problems
Building new systems to securely store and retrieve large volumes of data for computational purposes
Building new tools for our operational teams
What's in it for you
Projects - Opportunity to work on exciting projects and make an impact on the Canadian broadcast media industry
Work-life balance - In addition to a flexible summer hours program, we offer paid days off during the holiday closure between Christmas and New Year’s Day
Competitive benefits package and group savings and retirement program
Education reimbursement - For those wishing to pursue additional professional development, funding of up to $1000 per year is available through our education reimbursement program
Currently working in a remote environment
Values
Stronger together, Strive for better, Always learn, Be passionate
NLogic is an equal opportunity employer

We are committed to inclusive, barrier-free recruitment and selection processes and work environments. If you are contacted for a job opportunity, please advise the People and Culture department if any accommodations are needed to ensure you have access to a fair and equitable process. Any information received relating to accommodation will be addressed confidentially."
Data Engineer,"Sherbrooke, QC",CGI,None,Organic,"Give your career a boost.

The information technology (IT) sector is in the midst of an extraordinary period. Organisations’s digital transformation is accelerating, and CGI is at the forefront of this evolution. We accompany our clients in their transformation and innovation process, which allows us to offer our professionals exciting career opportunities.

CGI’s success rests on the talent and engagement of our professionals. Together, we take on challenges and share in the benefits derived from our company’s growth. This approach reinforces our shareholder-owner culture, all our professionals thus benefit from the value we collectively create.

Join us to participate in the growth of one of the world’s largest information technology (IT) and business consulting companies.

Summary:
You love technology? You like working with people and data on actual projects? You are curious by nature and like to challenge established ideas? If you are a passionate person with contagious energy, have demonstrated initiative and like teamwork, we would like you to read the following!

We are looking for a savvy Data Engineer to join our growing Business Intelligence Competency Centre team. The hire will be responsible for expanding and optimizing our client’s data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams and systems. The right candidate will be excited by the prospect of optimizing or even re-designing our client’s data architecture to support their next generation of products or services.

Responsibilities:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud-based ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across national boundaries through multiple data centers and cloud providers regions.
Work with data and analytics experts to strive for greater functionality in our clients’ data systems.
Qualifications

REQUIRED:
2-5 years of experience in a Data Engineer role
Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing ‘big data’ data pipelines.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Should also have experience using the following software/tools:
o Experience with big data tools: Hadoop, Spark, Kafka, etc.

o Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.

o Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.

o Experience with AWS, MS Azure, Google cloud services

o Experience with stream-processing systems: Storm, Spark-Streaming, etc.

o Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Fluency in French, oral and written
BONUS POINTS:
Experience with ERP and CRM environments and data: SAP, SalesForce, PeopleSoft, MS Dynamics, etc.
Fluency in English, oral and written
Location

Sherbrooke, Eastern Township area

We are located in a unique area in the southernmost part of Québec, where both the French and English languages co-exist harmoniously. We have a vast array of farms, wineries and other producers here that provide home cooks with the highest quality regional ingredients. The quality of life and robust economy in the Eastern Townships are second to none, and we’re seeing more people choosing the area as a place to settle down long term. Housing here is affordable, there are excellent French and English schools including two local universities, great medical care, as well as access to nature and recreation in every season, with a number of lakes and ski hills nearby.
Build your career with us.

It is an extraordinary time to be in business. As digital transformation continues to accelerate, CGI is at the center of this change—supporting our clients’ digital journeys and offering our professionals exciting career opportunities.

At CGI, our success comes from the talent and commitment of our professionals. As one team, we share the challenges and rewards that come from growing our company, which reinforces our culture of ownership. All of our professionals benefit from the value we collectively create.

Be part of building one of the largest independent technology and business services firms in the world.

Learn more about CGI at www.cgi.com.

No unsolicited agency referrals please.

CGI is an equal opportunity employer.
Your future duties and responsibilities

Required qualifications to be successful in this role"
Senior Support Engineer for Data Analytics Software,"Waterloo, ON",Tangam Systems,None,Organic,"What we’re hoping you will achieve … (circa 2021) one year from now..
Your potential story, one year from now……. We’d like to thank you for your contributions to our timely product rollouts, client support, and internal developer systems/infrastructure maintenance that helped the company achieve record customer satisfaction levels and a consecutive year of triple digit revenue growth. Here are some of your contributions as a key member of our software installation, support, and developer operations team:
You acted as the primary contact point for customers regarding all installation, technical training and second/third tier software support. Your excellent work resulted in you owning a “portfolio” of client sites that included some of our largest clients including a Fortune 500 company.
Applied a data driven approach to monitoring the health of the software installed at the various customer sites, which resulted in resolving problems before they escalated into larger issues. Furthermore, you championed the creation of an automated health tracking application along with the development team. These initiatives culminated in increasing your team’s productivity by 20% and reduced the number of support tickets.
Provided pre- and post-sales technical assistance to the sales team. You also helped the sales team prepare demo setups of the product and gathered information on customer needs, complaints and requests, using this information to influence the development of the product line.
Contributed to the ongoing improvement of Tangam’s online knowledge base and support processes. Helped improve the customer support architecture through automation and web based software tools resulting in 20% improvement in issue resolution time.
Coordinated and performed software upgrades of the product with customers and their IT teams. While you performed the upgrades, you also identified all the bottlenecks in the process, and worked with developers to minimize the bottlenecks for a smoother upgrade process. This reduced the down-time during upgrades.
Ensured that the internal team (developers, business analysts) have the right systems environment, client data stores, backups, monitoring, and software tools so they can focus on building great software and advanced analytics.
We can’t wait until next year. Now Back to Today. If you’d like this story to be yours, Apply Now.

Who we are
http://www.tangamsystems.com
Tangam Systems is a small, profitable, and growing company that is at the cutting edge of developing data analytics and visualization software. Our clients range from Fortune 500 businesses to some of the best-known brands in the resort industry. Our software product helps casino resorts optimize their operations and logistics in order to increase profitability and enhance the patron experience. Our software takes the complexity out of data analytics and makes data understandable and actionable to ordinary users. Our global client base includes operators in China, United States, Australia, New Zealand, United Kingdom, and Canada. Our patented products have won numerous industry awards and recognitions. We continue to push the envelope on what is possible in the area of data analytics software, and we are always seeking new talent to join us on our exciting journey.

Requirements
Relevant IT or support engineering experience. At least 4 years working experience in a support engineer or IT technician role. Database and SQL knowledge.
Education. Undergraduate degree in Computer Science / Software Engineering or a related discipline.
Troubleshooter extraordinaire. Proficiency with computer networking and troubleshooting (Windows operating system, hardware, etc.).
Top notch communicator. Strong interpersonal and communication skills.
Must be willing and able to travel internationally up to 15% of working time.
Have an eye for detail. Must be highly organized and detail oriented.
Previous DBA experience is an asset
Available after business hours for critical issues.
No criminal record and licensable to work in a highly regulated industry.
Your playground / what you’ll learn:
At Tangam you’ll get a chance to play, learn and build with the following tools and technologies as part of a cross-functional team that is the world’s foremost innovator in cutting edge casino optimization software.
Languages: .NET Core, JavaScript, TypeScript, PowerShell
Web Server: IIS, NGINX
Databases: MS SQL, Oracle, DB2
Infrastructure: Azure, Docker, Kubernetes
Operation Systems: Windows, Linux
Development Processes: Agile, CI/CD
Compensation and Benefits:
Base salary of $80K - $95K + performance-based bonus or stock options
Work-Life Balance: Flex time, work from home days and travel incentives.
Set-up: Standing / adjustable desks, massage chair & quiet rooms, employee lounge with Xbox, Switch & PS4.
Benefits Plan: Fitness allowance, dental/prescription/vision, massage & physio, and healthcare spending account.
Food & Fun: Fully stocked kitchen, fancy coffee machine, team lunches, long weekend bottle draws and monthly employee events.

We are committed to providing an inclusive work environment that welcomes diversity and supports accessibility across the organization. If you require accommodation at any time during the recruitment process, please let us know by contacting us at Tangam-Tngm0431@applications.recruiterbox.com."
Data Engineer (#101/20),"Vancouver, BC",Vancouver Fraser Port Authority,None,Organic,"BE PART OF THE TEAM THAT
BRINGS THE WORLD TOGETHER
Career Opportunity:
Want to work in an exciting environment with some of the most talented people in Canada? Join our team and connect with your potential.
Port of Vancouver is Canada’s largest and most diversified port, stretching along 360 kilometres of shoreline and waterways and bordering on 16 municipalities. We lead the efficient and reliable movement of cargo and passengers, annually trading $184 billion in goods with more than 160 trading economies.
This position provides exceptional client-centered service on a consistent basis to all stakeholder groups. This involves working on multiple projects involving the implementation of new or enhancement of existing systems, solutions and processes.
The main duties and responsibilities of this position will be:
Planning / Development
As part of the data infrastructure team participates in analysis, planning and developing detailed roadmaps for the modernization of the existing VFPA data platform. Works closely with the senior systems analyst –databases on the implementation of roadmaps.
Technical owner of organization's datalake; including both on-premises (SQL server enterprise) and cloud-based environments (Azure/AWS).
Working closely with the manager, data infrastructure to architect, create, design, implement, test data models and database management systems.
Support the full analytics cycle of data design, ETL development, validating and reporting (either Agile or Waterfall). Help improve overall design and delivery standards.
Collaborate with external resources, ensuring the data architectures and standard are being adhered to through proper knowledge transfer, code versioning, governance and quality assurance.
Ensure all upgrades, additions and changes to VFPA’s data infrastructure, tools and reporting portals are consistent in supporting the IS department’s service level commitments to the organization.
Research new technologies, implement proofs of concept/MVPs and make recommendations to build scalable and supportable infrastructure, as appropriate
Perform root-cause analysis for service interruption and develop solutions to complex technical problems and take appropriate urgent actions.
Data Infrastructure
Database Design and Administration
Proactively maintains data infrastructure: database servers, performance monitoring, security configuration, database software (SQL) - installation, patching, upgrades, decommissioning etc.
Develops feasible data backup and recovery procedures to prevent data loss - high availability and disaster recovery procedures.
Implements and maintains database security (user access, roles and privileges) and audit functions.
Plans for storage and capacity for raw data sources ingestion into the landing zone of VFPA datalake.
Monitors, tunes and optimizes the performance of databases.
Creates and updates database objects.
Writes/deploys database procedures and scripts where required.
Data/BI Development
Participate in the collection, extraction, analysis, and documentation of analytics solutions from design through to code level.
Create/enhance and document logical and physical database models.
Excellent SQL skills for extracting structured/unstructured data from VFPA data sources/systems, designing and implementing data marts using SSIS to extract, transform, and load
(ETL) business-critical information into on-premises SQL Server database or Cloud (AWS/Azure) based datalake solutions.
Creating VFPA semantic layer views for reporting/dashboard and analytics.
Participate in forming strategies for VFPA data availability, security, archival, recovery and migration.
Visualization
Build and deploy visualizations/reports using PowerBI or Tableau to either on-premise and/or cloud environments.
Create multi-dimensional and tabular models as required

SPECIFIC COMPETENCIES
The ideal candidate will have excellent organizational skills with the flexibility to perform multiple tasks and meet deadlines are required. Proven diagnostic, problem solving, team skills are essential, along with an excellent ""customer service"" approach. The individual must be self-motivated, self-starter and work with minimal supervision with an excellent written and verbal communication skills, along with the ability to communicate both technically and non-technically, as appropriate.


EDUCATION & EXPERIENCE
Must have a degree in Information Technology from a recognized post-secondary educational institution along with a minimum of 5 years’ of senior data development experience in a mid to large corporate enterprise environment.
Must have demonstrated experience with the following:
Physical and logical design of database architecture for relational databases, data warehouses and data lakes.
Database optimization, data replication, database recovery and performance tuning.
Solid skills with the Microsoft BI technology stack (SSIS, SSAS, SSRS, SQL Server, TSQL, MDX, DAX, Power BI, Power Pivot, Power Query).
Experience with Azure DevOps or TFS as code version controlling tool.
Understanding of on-premises and on cloud dashboard deployments.
Orchestration of the modern data pipelines for continuous delivery.
Data Modeling/ Dimension modeling – start schemas, CDC etc.
Functional programing languages (R, Python etc.), APIs and web services.
Data security principles, efficiency, quality and governance.
Complex technical and architectural documentation skills.

WORKING CONDITIONS
Normal working office conditions and participation in 7x24 on-call support.
MARINE TRANSPORTATION SECURITY CLEARANCE
The incumbent must obtain and maintain a valid Marine Transportation Security Clearance as this position is required to access restricted areas/information as defined by the Marine Transportation Security Regulations.


The Vancouver Fraser Port Authority offers a competitive total compensation package including an earned time off program.
We thank all applicants for their interest, however, only shortlisted candidates will be contacted.
Vancouver Fraser Port Authority is dedicated to employment equity and a workplace reflective of the diversity of our community."
Sr. Data Engineer,"Vancouver, BC","AMZN CAN Fulfillment Svcs, ULC",None,Organic,"5+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's Degree in Computer Science (or related disciplines) and/or equivalent years of work experience
7+ years finance information systems leadership with progressively increasing levels of responsibility
Previous experience designing and building large data warehouse systems
Extensive SQL and database skills with ability to deep dive into data logic

Amazon seeks an experienced Sr. Data Engineer to play a key role in the development and implementation of a comprehensive financial reporting platform to support the WW Amazon Devices business. This is an exciting opportunity to join one of the most transformative businesses at Amazon. The successful candidate will have a proven track record of developing integrated financial reporting tools and platforms to support worldwide businesses. The role operates in a fast-paced, global, results-oriented environment, and will have the opportunity to develop the platform to support key decisions for the Amazon Devices operations around the world.

Key Responsibilities
Develop the integrated Devices finance reporting platform leveraging existing systems and process and scalable technology.
Partner with Devices Central FP&A, Devices Product Line finance, online and offline channel finance, product management finance, and operations and development finance to develop a shared reporting platform as well as reporting tools to support custom specialized reporting needs
Collaborate with both the corporate accounting and central FP&A teams to understand the interdependencies and deliverables
Help to influence the long term Amazon company financial systems information systems with agile results
Develop reporting to support weekly financial and operational performance metrics including KPI metrics, monthly historical results, monthly forecasts, annual operating plans, and long range three year plans.

Previous experience designing and building large data warehouse systems
Demonstrated ability to understand financial reporting needs and develop custom reporting and metric tracking information systems
Superior attention to detail, project management skills and the ability to successfully manage multiple projects simultaneously
Recent experience with large datasets , and Amazon AWS tools/technologies
Strong ability to interact, communicate, present and influence within multiple levels of the organization"
Data Engineer Senior,"Toronto, ON",Digital On Us,None,Organic,"Who is DigitalOnUs?
At DigitalOnUs, we not only provide Agile and DevOps methodologies to our customers, we have adopted the same within the company as well. Our nimble processes are not mired in red tape, yet robust, flexible and result-oriented. We are Software Engineers, Technical Architects, Cloud and DevOps specialists. But the most important, we are dreamers, creators and challengers. Each day, we strive to make great come alive. Our lemma: ""work smart and play hard""
Our technology partners are Hashicorp, Cloudbees, Chef, Pagerduty, Docker and SAP.
We are always looking for the brightest candidates to come and we offer a work environment with everything you need to be your best. Does Ambition, Success, Fun, Friends & Learning define your idea of a career? Join us and be part of our family!
We are looking for a data engineer responsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow.
Will support software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
Responsibilities
Create and maintain optimal data pipeline architecture,
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Qualifications for Data Engineer
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience working with Go. Python, Java or Scala is a an alternative.
Strong SQL skills.
Experience with big data tools is a major plus but not required: Hadoop, Spark, Kafka, etc.
Experience with Snowflake would be ideal.
What you can expect from us
At DigitalOnUs, what distinguishes us from other teams is the comfortable environment which engenders trust within teams and with our customers. Trust and openness leads to quality, innovation, commitment to deliverables, efficiency and cost-effectiveness for all our customers.
Work with some truly remarkable IT engineers, architects, specialists and more.
We're growing at a phenomenal pace and we'd like some company.
Hear your voice, nurture your talent and help you strengthen your foot print!
Benefits above the law
Mentorship, and opportunities to grow and learn
Your resume contains personal data whose treatment has been authorized by its owner for Digital OnUs, S. de RL de CV (the ""Company""). If you are the owner of personal data in possession of the Company and wish to obtain further information regarding the processing of your personal data or the exercise of your ARCO rights, please consult our integral privacy notice on the website https://www.digitalonus.com/privacy-policy/"
Senior Support Engineer for Data Analytics Software,"Waterloo, ON",Tangam Systems,None,Organic,"What we’re hoping you will achieve … (circa 2021) one year from now..
Your potential story, one year from now……. We’d like to thank you for your contributions to our timely product rollouts, client support, and internal developer systems/infrastructure maintenance that helped the company achieve record customer satisfaction levels and a consecutive year of triple digit revenue growth. Here are some of your contributions as a key member of our software installation, support, and developer operations team:
You acted as the primary contact point for customers regarding all installation, technical training and second/third tier software support. Your excellent work resulted in you owning a “portfolio” of client sites that included some of our largest clients including a Fortune 500 company.
Applied a data driven approach to monitoring the health of the software installed at the various customer sites, which resulted in resolving problems before they escalated into larger issues. Furthermore, you championed the creation of an automated health tracking application along with the development team. These initiatives culminated in increasing your team’s productivity by 20% and reduced the number of support tickets.
Provided pre- and post-sales technical assistance to the sales team. You also helped the sales team prepare demo setups of the product and gathered information on customer needs, complaints and requests, using this information to influence the development of the product line.
Contributed to the ongoing improvement of Tangam’s online knowledge base and support processes. Helped improve the customer support architecture through automation and web based software tools resulting in 20% improvement in issue resolution time.
Coordinated and performed software upgrades of the product with customers and their IT teams. While you performed the upgrades, you also identified all the bottlenecks in the process, and worked with developers to minimize the bottlenecks for a smoother upgrade process. This reduced the down-time during upgrades.
Ensured that the internal team (developers, business analysts) have the right systems environment, client data stores, backups, monitoring, and software tools so they can focus on building great software and advanced analytics.
We can’t wait until next year. Now Back to Today. If you’d like this story to be yours, Apply Now.

Who we are
http://www.tangamsystems.com
Tangam Systems is a small, profitable, and growing company that is at the cutting edge of developing data analytics and visualization software. Our clients range from Fortune 500 businesses to some of the best-known brands in the resort industry. Our software product helps casino resorts optimize their operations and logistics in order to increase profitability and enhance the patron experience. Our software takes the complexity out of data analytics and makes data understandable and actionable to ordinary users. Our global client base includes operators in China, United States, Australia, New Zealand, United Kingdom, and Canada. Our patented products have won numerous industry awards and recognitions. We continue to push the envelope on what is possible in the area of data analytics software, and we are always seeking new talent to join us on our exciting journey.

Requirements
Relevant IT or support engineering experience. At least 4 years working experience in a support engineer or IT technician role. Database and SQL knowledge.
Education. Undergraduate degree in Computer Science / Software Engineering or a related discipline.
Troubleshooter extraordinaire. Proficiency with computer networking and troubleshooting (Windows operating system, hardware, etc.).
Top notch communicator. Strong interpersonal and communication skills.
Must be willing and able to travel internationally up to 15% of working time.
Have an eye for detail. Must be highly organized and detail oriented.
Previous DBA experience is an asset
Available after business hours for critical issues.
No criminal record and licensable to work in a highly regulated industry.
Your playground / what you’ll learn:
At Tangam you’ll get a chance to play, learn and build with the following tools and technologies as part of a cross-functional team that is the world’s foremost innovator in cutting edge casino optimization software.
Languages: .NET Core, JavaScript, TypeScript, PowerShell
Web Server: IIS, NGINX
Databases: MS SQL, Oracle, DB2
Infrastructure: Azure, Docker, Kubernetes
Operation Systems: Windows, Linux
Development Processes: Agile, CI/CD
Compensation and Benefits:
Base salary of $80K - $95K + performance-based bonus or stock options
Work-Life Balance: Flex time, work from home days and travel incentives.
Set-up: Standing / adjustable desks, massage chair & quiet rooms, employee lounge with Xbox, Switch & PS4.
Benefits Plan: Fitness allowance, dental/prescription/vision, massage & physio, and healthcare spending account.
Food & Fun: Fully stocked kitchen, fancy coffee machine, team lunches, long weekend bottle draws and monthly employee events.

We are committed to providing an inclusive work environment that welcomes diversity and supports accessibility across the organization. If you require accommodation at any time during the recruitment process, please let us know by contacting us at Tangam-Tngm0431@applications.recruiterbox.com."
Data Engineer/ETL Developer,"Markham, ON",Aviva,None,Organic,"We are looking for a Data Engineer/ETL Developer to join our growing team of Data and BI experts. Aviva has embarked on a journey to build next generation data platform to support the growing need of data from business intelligence and analytics. The candidate will be responsible designing, developing and productionizing ETL jobs to ingest data into Data Lake, load data to data marts; and extract data to integrate with various business applications.
Roles and Responsibilities:
Design and Develop ETL Pipeline to ingest data into Hadoop from different data sources (Files, Mainframe, Relational Sources, NoSQL Etc.) using Informatica BDM
Parse unstructured data, semi structured data such as JSON, XML etc. using Informatica Data Processor.
Analyze the Informatica PowerCenter Jobs and redesign and develop them in BDM.
Design and develop efficient Mapping and workflows to load data to Data Marts.
Perform the GAP analysis between various legacy applications to migrate them to newer platforms/data marts.
Write efficient queries in Hive or Impala and PostgreSQL to extract data on Adhoc basis to do the data analysis.
Identify the performance bottlenecks in ETL Jobs and tune their performance by enhancing or redesigning them.
Work with Hadoop administrators, PostgreS DBAs to partition the hive tables, refresh metadata and various other activities, to enhance the performance of data loading and extraction.
Performance tuning of ETL mappings and queries.
Write simple or medium complex shell scripts to preprocess the files, schedule ETL jobs etc.
Identify various manual processes, queries etc. in the Data and BI areas, design and develop ETL Jobs to automate them.
Participate in daily scrums; work with vendor partners, QA team and business users in various stages of development cycle.
Skill Required:
7+ years of experience in designing and developing ETL Jobs (Informatica or any other ETL tool)
3+ years of experience working on Informatica BDM platform
Experience on various execution modes in BDM such Blaze, Spark, Hive, Native.
3+ years of experience working on Hadoop Platform, writing hive or impala queries.
5+ years of experience working on relational databases (Oracle, Teradata, PostgreSQL etc.) and writing SQL queries.
Should have deep knowledge on performance tuning of ETL Jobs, Hadoop Jobs, SQL’s, Partitioning, Indexing and various other techniques.
Experience in writing Shell scripts.
Experience in Spark Jobs (Python or Scala) is an asset.
1+ years of experience with working on AWS technologies for data pipelines, data warehouses
Minimum 5+ years of experience with building ETLs to load data warehouse, data marts
Awareness of Kimball and Inmon data warehouse methodologies
Nice to have knowledge on all the products of Informatica such as IDQ, MDM, IDD, BDM, Data Catalogue, PowerCenter etc.
Must have experience working in Agile SCRUM methodology, should have used Jira, Bit bucket, GIT, Jenkins to deploy the codes from one environment to other.
Experience working in diverse multicultural environment with different vendors, onsite/offshore vendor teams etc.
P&C Insurance industry knowledge will be an added asset
Certifications in Informatica product suite as a developer
Nice to have 2+ years of experience with AWS data stack (IAM, 33, Kinesis Stream, Kinesis firehose, Lambda, Athena, Glue, RedShift and EMR
Exposure to other cloud platforms such as Azure and GCP are acceptable as well
Additional Information
Aviva Canada is committed to providing accommodations for people with disabilities during all phases of the hiring process including the application process. If you require an accommodation because of a disability, we will work with you to meet your needs. Applicants need to make their needs known in advance. If you are selected for an interview and require an accommodation, you are encouraged to advise the Talent Acquisition Partner who will consult with you to determine an appropriate accommodation."
Data Engineer,"Vancouver, BC",Port of Vancouver,None,Organic,"BE PART OF THE TEAM THAT
BRINGS THE WORLD TOGETHER
Career Opportunity:
Want to work in an exciting environment with some of the most talented people in Canada? Join our team and connect with your potential.
Port of Vancouver is Canada’s largest and most diversified port, stretching along 360 kilometres of shoreline and waterways and bordering on 16 municipalities. We lead the efficient and reliable movement of cargo and passengers, annually trading $184 billion in goods with more than 160 trading economies.
T his position provides exceptional cli ent-centered s ervice on a c onsistent b asis to all s takeholder groups. T his i nvolves w orking on m ultiple projects i nvolving the im plementation of new or enhancement of existing sys tems, s olutions and processes.
The main duties and responsibilities of this position will be:
Pl a nning / Development
As part of the data i nfrastructure team participates in a nalysis, pla nning and developing d etailed r oadmaps for the m odernization of the existing VFPA data platform. Works cl osely with the se nior syst ems analyst – databases on the im plementation of r oadmaps.
T echnical owner of organization's datalake; i ncluding both o n-premises (SQL s erver enterprise) and cl oud-based environments (Az ure/AWS).
Working closely with the manager, data in frastructure to architect, create, design, i mplement, test data models and database management systems.
Support the full analy tics cycle of data design, ETL development, validating and reporting (either Agile or Water fall). Help improve overall design and delivery standards.
Collaborate with external resources, ensuring the data architectures and standard are being adhered to through proper knowledge trans fer, code versioning, governance and quality assurance.
Ensure all upgrades, additions and changes to VFPA’s data in frastructure, tools and reporting portals are consistent in supporting the IS department’s service level co mmitments to the organization.
Research new technologies, im plement proofs of c oncept/MVPs and make r ecommendations to build sc alable and su pportable i nfrastructure, as appropriate
P erform r oot-cause analysis for s ervice i nterruption and develop s olutions to c omplex t echnical problems and take appropriate urgent actions.
Data I nfrastructure
Database Design and A dministration
Pr oactively m aintains data i nfrastructure: database s ervers, performance m onitoring, s ecurity c onfiguration, database s oftware (SQL) - i nstallation, p atching, u pgrades, d ecommissioning etc.
Develops f easible data backup and r ecovery procedures to prevent data l oss - high avail ability and disaster r ecovery procedures.
Implements and m aintains database s ecurity ( user access, r oles and privileges) and a udit f unctions.
Plans for s torage and ca pacity for raw data so urces in gestion into the la nding zone of VFPA datalake.
Monitors, tunes and o ptimizes the performance of databases.
Creates and updates d atabase objects.
Wri tes/deploys database procedures and scripts where r equired.
Data/BI D evelopment
P articipate in the c ollection, extraction, analysis, and d ocumentation of analytics s olutions from design through to code lev el.
Create/enhance and document l ogical and physical d atabase m odels.
Exc ellent SQL skills for extracting s tructured/unstructured data from VFPA data so urces/syst ems, designing and im plementing data m arts using SSIS to extract, transform, and load
(ETL) business-critical i nformation i nto o n-premises SQL S erver d atabase or Cloud (AWS /Azure) based datalake s olutions.
Creating VFPA s emantic layer views for r eporting/dashboard and a nalytics.
P articipate in f orming s trategies for VFPA data availa bility, s ecurity, archival, r ecovery and mi gration.
Vi s ualization
B uild and deploy vis ualizations/reports using Po werBI or T ableau to either o n-premise and/or cloud environments.
Create m ulti-dimensional and tabular m odels as r equired

S P E CIFIC COMPETENCIES
The ideal candidate will have exc ellent organizational skills with the fl exibility to p erform m ultiple t asks and meet deadlines are r equired. Proven diagnostic, problem s olving, team skills are essential, along with an excellent ""c ustomer s ervice"" approach. The indivi dual m ust be self-m otivated, self-s tarter and work with mi nimal s upervision with an excellent wri tten and v erbal c ommunication skills, along with the ability to comm unicate both technically and non-technically, as appropriate.

E DUCATION & EXPERIENCE
Must have a d egree in Information T echnology from a r ecognized p ost-secondary educational i nstitution along with a mi nimum of 5 ye ars’ of s enior data development experience in a mid to l arge c orporate enterprise environment.
Must have demonstrated ex perience with the f ollowing:
Physical and l ogical d esign of database architecture for r elational databases, data warehouses and data l akes.
Database optimization, data replication, database recovery and per formance tuning.
Solid skills with the Microsoft BI technology stack (SSIS, SSAS, SSRS, SQL Server, TSQL, MDX, DAX, Power BI, P ower Pivot, Power Query).
Experience with Azure DevOps or TFS as code version controlling tool.
Understanding of on-pre mises and on cloud dashboard depl oyments.
Orchestration of the modern data pipelines for continuous delivery.
Data Modeling/ Dimension modeling – start sche mas, CDC etc.
Functional programing languages (R, Python etc.), APIs and web services.
Data security principles, ef ficiency, qu ality and governance.
Complex technical and architectural docu mentation skills.

W O RKING CONDITIONS
Normal w orking office c onditions and p articipation in 7x24 on-call su pport.
M A RINE TRANSPORTATION SECURITY CLEARANCE
The i ncumbent m ust obtain and m aintain a valid Marine Tr ansportation S ecurity Cl earance as t his position is r equired to access r estricted areas/information as defined by the Marine Tr ansportation S ecurity R egulations.

The Vancouver Fraser Port Authority offers a competitive total compensation package including an earned time off program.

We thank all applicants for their interest, however, only shortlisted candidates will be contacted.
Vancouver Fraser Port Authority is dedicated to employment equity and a workplace reflective of the diversity of our community."
Apple Media Products - Senior Data Engineer,"Vancouver, BC",Apple,None,Organic,"Summary
Posted: Oct 5, 2020
Role Number:200196998
The Apple Media Products Engineering team is one of the most exciting examples of Apple’s long-held passion for combining art and technology! These are the people who power the App Store, Apple TV, Apple Music, Apple Podcasts, and Apple Books. And they do it on a massive scale, meeting Apple’s high expectations with high performance to deliver a huge variety of entertainment in over 35 languages to more than 150 countries. These engineers build secure, end-to-end solutions. They develop the custom software used to process all the creative work, the tools that providers use to deliver that media, all the server-side systems, and the APIs for many Apple services. Thanks to Apple’s unique integration of hardware, software, and services, engineers here partner to get behind a single unified vision. That vision always includes a deep commitment to strengthening Apple’s privacy policy, one of Apple’s core values. Although services are a bigger part of Apple’s business than ever before, these teams remain small, forward-thinking, and cross-functional, offering greater exposure to the array of opportunities here.
Key Qualifications
Experience in high level programming languages such as Java, Scala, or Python.
Proficiency with databases and SQL is required.
Proficiency in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.
Expertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.
Expertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.
Experience with large scale data warehousing, mining or analytic systems.
Ability to work with analysts to gather requirements and translate them into data engineering tasks
Aptitude to independently learn new technologies.
Description
As a senior member of the Data Engineering team, you will have significant responsibility and influence in shaping its future direction. This role is inherently cross-functional and the ideal candidate will work across disciplines. We are looking for someone with a love for data and ability to iterate quickly on all stages of data pipeline. This position involves working on a small team to develop large scale data pipelines and analytical solutions using Big Data technologies. Successful candidates will have strong engineering skills and communication, as well as, a belief that data driven processes lead to phenomenal products. You will need to have a passion for quality and an ability to understand sophisticated systems.
Education & Experience
Bachelor's degree or equivalent work experience in Engineering, Computer Science, Business Information Systems. Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other legally protected characteristics."
Intermediate Cloud Data Engineer- TELUS Digital,"Vancouver, BC",TELUS,None,Organic,"Create awesome experiences for our customers.

Join our team

We’re a customer-driven and product-minded team within TELUS, responsible for our company’s digital evolution. We simplify the path to production so teams can focus on what matters most.

TELUS Digital’s mission is to make life easier for millions of customers – and for our team. We build smart, customer-centric and data-driven experiences for telus.com, TELUS.com/business and the My TELUS app.

Our team includes a passionate group of strategists, UX and visual designers, full stack developers, content managers, scrum masters, testers, product owners, people experience specialists, and other digital experts.

Our team members are leaders in local and global technology communities, we value and support communities such as: Tech Masters, NodeSchool, Ladies Learning Code, Women Who Code TO and many more!

Learn more about our team at telus.com/digital

This is a full-time position.

Here’s the impact you’ll make and what we’ll accomplish together

As a Cloud Data Engineer (Intermediate), you will join the Home Solutions Analytics team and own the Google Cloud Platform. You will continue to evolve and optimize our cloud setup, and will work with our analysts to deliver cloud solutions to meet reporting needs. You will work with large scales of data from various sources, and will be responsible for ETLs and finding solutions to get data in the format required by the analysts.

As we mature with the Google Cloud Platform, we will move towards utilizing more machine learning algorithms to help make better predictions to help make better decisions.

We are looking for an individual who loves to solve complex problems with knowledge of Terraform, Google Cloud functionality, Python and SQL.

Here’s how
Create, model and transform data in various formats and from several sources to allow data to be easier analyzed for faster decision making
Build cloud functions and pipelines from various dataLakes and applications to ingest and transform data
Monitor data quality and follow data best practices
Minimize and fix any bugs that may occur on a daily basis
Work with analysts and stakeholders to gather requirements for different projects that touch the Google Cloud Platform
Use problem solving skills to identify deficiencies, and execute solutions to make functions more efficient
Collaborate with data supply teams to refine solutions and remove roadblockers

Qualifications
You're the missing piece of the puzzle
You have Google Cloud Platform experience including working with Cloud functions, Cloud scheduler, Big Query, Airflow and Terraform
Have 2+ years of working with cloud functions
Have strong Python and SQL skills
Passionate about problem solving and finding the most efficient solutions
Work well within a team
Great-to-haves
Google Cloud Platform Certification
You have worked with analytics teams in the past
Have interest in data visualization
Experience with machine learning best practices
Familiarity with agile/scrum methodologies"
"Senior Software Engineer, Data Platform","Vancouver, BC",Tableau,None,Organic,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
What you’ll be doing...

Einstein Analytics, recently rebranded as Tableau CRM, is the best-in-breed analytics for the CRM market. Within the Data Platform, our team is responsible for supporting customer dataflows at scale and building the next generation dataflow service that can handle phenomenal volumes of data resiliently to support our customer's business critical processes backed by Einstein Analytics. The EA Data Platform is at the heart of enabling our customers to curate and shape their data in preparation for analysis, join us and help build the future of our backend!
You will build out new features supporting multi billion row scale dataflows.
Iteratively improving the reliability, performance and resiliency of our dataflow architecture using a metrics driven approach.
Iterating rapidly on improvements with a biweekly release cycle.
Working with a modern container-based architecture.
Collaborating with other teams building new features on our service.
Delivering internal tooling to improve the operational visibility of flows to help rapidly diagnose and proactively catch production issues.
Keeping a customer success focused mindset in addressing production issues.
Work on owning and operating multiple instances of mission-critical infrastructure services including, monitoring, alerting, logging, and reporting applications.
You will be building data-processing or ETL products at scale


Who you are...
Experienced. Y ou have 3-5 years of relevant professional experience. You have experience with agile development methodology and testing. You have experience using telemetry and metrics to drive operational excellence. You may have a Bachelor’s degree in Computer Sciences or similar field of study. You may have experience with one or more of the following, Spark, Kafka, Java, REST, gRPC, Kubernetes, Docker, Jenkins, Splunk, Python, Spinnaker, Parquet, Avro, Go, Presto.
Technical. You have a deep knowledge of object oriented programming and experience with at least one object oriented programming language: Java, C++, C#, Ruby, Go, Scala, Python, but this team primarily uses java. You have a solid knowledge of database / big data / distributed technologies. You will have experience in public cloud engineering on GCP, AWS, Alibaba and/or Azure platforms
Passionate. You have a passion for data and building features for our customers that work.
Relentlessly High Standards. You understand what it takes to write software that is used by thousands or millions of people. You love writing things that are robust, scalable, and perform well. You thrill in your accomplishments but also know it's about doing and improving.
A True Team Player. You enjoy collaborating, learning from and teaching others so we can all become better developers. You assume good intent in others, and actively do your part to make a positive work environment.
You are a Recruiter! Tableau hires company builders and, in this role, you will be asked to be on the constant lookout for the best talent to bring onboard to help us continue to build one of the best companies in the world!


#LI-NF
Accommodations - If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesfore.com or Salesforce.org.
Salesforce welcomes all."
Cloud Data Engineer,"Richmond Hill, ON",Staples Canada,None,Organic,": 1139113

Who we are

Staples Canada is on a journey to become the Working and Learning Company. We are dynamic, inspiring partners to our customers and the communities in which we live. At Staples, we inspire people to work smarter, learn more and grow every day. We look for people who are curious, approachable and passionate, and who enjoy finding solutions.

If that’s you, let’s work, learn and grow together.

Some of what you will do

You will play a key part in a small-but-growing, data science and digital analytics team at Staples Canada. As part of the eCommerce Product group, we drive data availability and analytics, and empower customer and associate-facing features and functionality. You will be responsible for the design and implementation of Staples Canada eCommerce cloud data and analytics infrastructure. You should be comfortable establishing best practices in a small working group and working in an Agile environment.

Specifically you will:
Design, Develop and maintain ETL pipelines from a variety of online and offline sources to Google Cloud Platform
Manage code running on both serverless functions (GCP and Segment) and VMs
Deploy and help to maintain predictive models
Contribute to architecture design as the data platform grows
Manage the Cloud Source Repository
Conducting technical audits of existing architectures (Infrastructure, Performance, Security, Scalability and more) document best practices and recommendations
Maintain and enforce tracking plans/Solution Design Documents, and take ownership of Data Quality
Coordinate with Staples IT to ensure best security practices are in place

Some of What You’ll Need
5+ years’ experience in a related field, hands-on technical experience and at an eCommerce company
Experience with both Agile and Scrum
Demonstrated success in working within cross functional teams and effective project management and analytical/decision-making skills
Curious
Approachable
Passionate
Solutions finder
Employment Statement

Staples is an equal opportunity employer committed to diversity and inclusion and we encourage applications from all qualified candidates, including those with disabilities. We will accommodate applicants’ needs, upon request, throughout all stages of the recruitment process. Please inform us of the accommodation(s) that you may require. Information received relating to accommodation will be addressed confidentially.

Job eCommerce
Location(s) CA-ON-Richmond Hill
Schedule Full-time
:
:
:
Employment Statement
Staples Canada is an equal opportunity employer committed to diversity and inclusion and we encourage applications from all qualified candidates, including those with disabilities."
Staff Engineer - Data,"Toronto, ON",BenchSci,None,Organic,"BenchSci's vision is to bring medicine to patients 50% faster by 2025. We're doing this by empowering scientists with the world’s most advanced biomedical artificial intelligence to run more successful experiments. Backed by F-Prime, Gradient Ventures (Google’s AI fund), and Inovia Capital, our platform accelerates science at 15 top 20 pharmaceutical companies and over 4,300 leading research centers worldwide. We're a CIX Top 10 Growth company, certified Great Place to Work®, and top-ranked company on Glassdoor.

We are looking for a Staff Engineer to join our growing Engineering Team. In this role, you will work closely with key technical partners across the organization to ensure that BenchSci’s technical infrastructure and codebase will be able to support long-term growth. Reporting to the VP of Engineering, you will provide technical leadership across several functional areas within the team, lead complex projects, participate in roadmap and project planning, and actively contribute to BenchSci's data engineering and architecture initiatives. The Staff Engineer will help foster the growth and development of other individual contributors, while also playing a key role in helping us scale as we continue to make a long-term impact on scientific discovery.
You Will:
Work with multiple teams working on key customer features and our own significant internal development projects
Contribute to setting technical and architectural direction for our Data Engineering teams with a focus on expanding the scope of our products and our ability to make life-scientists more effective
Guide the architecture of BenchSci in order to help us handle data more securely and accurately
Decouple work done by teams to more readily allow for more independent work and smaller feature releases
Be involved in all aspects of software development, working to create and maintain the most reliable, secure, performant and high throughput service for our customers by using cutting-edge cloud technology (this can range from setting high-level technical direction down to implementation)
Be an inspiring mentor and coach to other engineers
Act as a champion and advocate for the company and its engineering teams through various channels
You Have:
A minimum of 8 years of professional development experience, with 2 years at a Staff level
Expertise in data management practices, including approaches to capturing provenance, versioning, and disaster recovery
Experience working with secure data, including compliance with regulations like HIPAA and GDPR
Expertise with data processing engines such as Apache Beam
Experience working with SaaS products in a fast-paced Agile environment
The ability to drive the engineering team towards the decisions they make while minimizing friction
The ability to translate business concerns into technical implementations
Experience with coaching and mentoring technical teams
Experience with big data
Experience communicating trade-offs in approaches to security, speed to ship, and performance
Domain expertise in either machine learning, distributed systems, data engineering, or cloud infrastructure
Experience providing feedback to other individual contributors while showing a sense of empathy, tact, thoughtfulness, and respect
Nice to haves, but not mandatory qualifications:
Successful delivery of at least one major project in a domain space that is relevant to BenchSci. Relevant domain areas may include, but are not limited to: machine learning, information retrieval, or distributed systems
Our benefits and perks:
A compensation package that includes equity options in the company
An annual Executive Health Assessment at Medcan: All employees get the “executive treatment”
Effectiveness coaching for managers: Onsite, personalized coaching from a trained clinical psychologist
Mental health tools and support: Optional mindfulness sessions and a free Headspace account
Complimentary genome sequencing from 23andMe: Find out what your DNA says about your health, traits, and ancestry
Three weeks of vacation, plus another week: Get 15 days to use anytime, and we’re closed Dec 25-Jan 1
Additional days off: Company summer day, your birthday, and earn +1 vacation day annually
Work from anywhere flexibility: Every day right now, and up to 4 days per week once we return to the office
An onsite gym: Keep fit, conveniently, with a Peloton and other great equipment
A great benefits package: Including health and dental
Here at BenchSci, these are our core values:
Focused: We focus on what will drive the greatest impact at all times.
Advancement: We believe in continuous growth, and discovering new ways to do things better. This applies to our product and business, but also to ourselves.
Speed: We recognize that without a sense of urgency, our team, our product and our mission lose their value.
Tenacity: What we’re trying to do isn’t easy, but we hire the best people, and give them the autonomy, tools, and resources to succeed. The hard work is up to them.
Transparency: We believe that sharing diverse ideas and information creates strong teams. Our success stems from research, collaboration, feedback, and trust.
BenchSci is an equal opportunity employer. We value diversity and are committed to fostering an inclusive environment. All four of our cofounders are immigrants to Canada, as are many of our employees. We welcome your fresh perspectives and ideas."
Senior Data Engineer,"Montréal, QC",Thales Group,None,Organic,"Location: Montreal, Canada
Dans des marchés en rapide évolution, les clients à travers le monde font confiance à Thales. Thales est une entreprise où les personnes les plus brillantes du monde entier se regroupent pour mettre en commun leurs idées et ainsi s'inspirer mutuellement. Dans tous les secteurs où œuvre Thales, notamment l’aérospatiale, le transport, la défense, la sécurité et l'espace, nos équipes d’architectes conçoivent des solutions innovantes qui rendent demain possible dès aujourd’hui.
Carrefour mondial de l’intelligence artificielle, Montréal est le foyer du nouveau centre de recherche et de technologie spécialisé en intelligence artificielle (cortAIx) collaborant avec les principaux groupes canadiens de recherche en intelligence artificielle à Montréal et à Toronto. S’appuyant sur ses compétences dans les principaux marchés industriels, Thales donne vie à l'intelligence artificielle au profit de ses clients tout en créant de passionnants emplois pour les chercheurs et les développeurs experts en intelligence artificielle en vue de trouver des solutions qui transformeront notre monde, du fond des océans aux confins de l'univers et du cyberespace. Ayant très tôt opté pour le modèle d’innovation ouverte et collaborative, Thales procède actuellement à la création de la structure du centre de recherche et de technologie spécialisé en intelligence artificielle (cortAIx). Piloté par Thales, le centre cortAIx, en collaboration avec l'Institut québécois d'intelligence artificielle (MILA), l'Institut de valorisation des données (IVADO) et l’Institut Vector de Toronto, est situé dans le célèbre quartier Petite-Italie, au cœur de la communauté de l’innovation à Montréal.
Thales people architect identity management and data protection solutions at the heart of digital security. Business and governments rely on us to bring trust to the billons of digital interactions they have with people. Our technologies and services help banks exchange funds, people cross borders, energy become smarter and much more. More than 30,000 organizations already rely on us to verify the identities of people and things, grant access to digital services, analyze vast quantities of information and encrypt data to make the connected world more secure.
Montreal – a world leading AI hub, is home to new Centre of Research & Technology in Artificial Intelligence eXpertise (cortAIx) collaborating with leading Canadian AI research groups in Montreal and Toronto. With competencies in major industrial markets Thales is bringing artificial intelligence to life for our customers creating exciting jobs for AI researchers and developers who will create solutions that will transform our world from the bottom of oceans to the depths of space and cyberspace. As an early adopter of open, creative and collaborative innovation model, Thales is building the Centre of Research and Technology in Artificial Intelligence eXpertise (cortAIx). Led by Thales, cortAIx, in collaboration with the MILA (Artificial Intelligence Institute of Quebec), the IVADO (Institute of Data Valorization) and the Vector Institute of Toronto, is located in Montreal’s famous Little Italy, in the heart of Montreal’s innovation community.
Guavus is looking for a highly motivated and talented Senior Data Engineer to participate in the development of the most advanced solutions in the Big Data space by using agile methodologies. The successful person will actively participate and collaborate with the data team to design and implement data pipelines to feed our products.
Key Responsibilities:
Contribute to the complete development lifecycle of our products including prototyping, development, testing, and deployment
Conduct ad hoc analyses to support key corporate initiatives
Interface with data scientists, product managers, and business stakeholders to understand data needs and help build data products that scale across the company
Take responsibility for continuous improvements in the coding and development of your own coding practices
Work closely with team members to deliver products within an agile work environment
Develop reporting and visualization tools using in-house tools as well as third-party tools
Define performance metrics and develop dashboards to evaluate business performance
Design, build, optimize, launch, and support new and existing data models and ETL processes into customer environments
Monitor and manage the SLA for all data sets and systems in allocated areas of ownership
Work with data infrastructure and data engineering teams to triage infra issues and drive to resolution
Required Skills and Experience:
Bachelors or Master’s degree in Computer Science or equivalent preferred
8+ years experience in the field
4+ years of industry experience as a Data Engineer, Business Intelligence Engineer or Data Scientist
Demonstrable understanding of systems, algorithms, and software design required
Preferred Skills and Experience:
Previous work experience, especially in an agile environment
Experience working with and analyzing big data in a distributed environment would be considered an asset
Knowledge of cloud computing principles or direct experience with a cloud platform would be considered an asset
Must be willing to travel and work closely with customers (Maximum 25% time)
Solid understanding of Analytics, Machine Learning, Data Science and Big Data Business Intelligence
Fluent French written/verbal communication skills are an asset
A self-starter, willing to learn new technologies and continuously striving to improve their own coding practices
Relational Databases (MySQL, PostgreSQL, Oracle)
Hadoop (Hive, Impala, HDFS)
Microservices, Docker, and Git
RESTful APIs
Continuous deployment
Bash
Ansible
Guavus est à la recherche d'un Ingénieur Data Senior très motivé et talentueux pour participer au développement des solutions les plus avancées dans l'espace Big Data en utilisant des méthodologies agiles. La personne qui réussit participera activement et collaborera avec l'équipe de données pour concevoir et mettre en œuvre des pipelines de données pour alimenter nos produits.
Principales responsabilités:
Contribuer au cycle de vie complet du développement de nos produits, y compris le prototypage, le développement, les tests et le déploiement
Mener des analyses ad hoc pour soutenir les principales initiatives de l'entreprise
Interface avec les scientifiques des données, les chefs de produit et les parties prenantes de l'entreprise pour comprendre les besoins en données et aider à créer des produits de données évolutifs dans toute l'entreprise
Assumez la responsabilité de l'amélioration continue du codage et du développement de vos propres pratiques de codage
Travailler en étroite collaboration avec les membres de l'équipe pour livrer des produits dans un environnement de travail agile
Développer des outils de reporting et de visualisation en utilisant des outils internes ainsi que des outils tiers
Définir des mesures de performance et développer des tableaux de bord pour évaluer les performances de l'entreprise
Concevoir, créer, optimiser, lancer et prendre en charge des modèles de données et des processus ETL nouveaux et existants dans les environnements clients
Surveiller et gérer le SLA pour tous les ensembles de données et systèmes dans les zones de propriété attribuées
Travailler avec les équipes d'infrastructure de données et d'ingénierie des données pour trier les problèmes infra et conduire à la résolution
Compétences et expérience requises:
Baccalauréat ou maîtrise en informatique ou équivalent préféré
8+ ans d'expérience dans le domaine
Plus de 4 ans d'expérience dans l'industrie en tant qu'ingénieur de données, ingénieur en intelligence d'affaires ou scientifique de données
Compréhension démontrable des systèmes, des algorithmes et de la conception logicielle requise
Compétences et expérience préférées:
Expérience de travail antérieure, en particulier dans un environnement agile
Une expérience de travail et d'analyse de mégadonnées dans un environnement distribué serait considérée comme un atout
La connaissance des principes du cloud computing ou une expérience directe avec une plateforme cloud serait considérée comme un atout
Doit être prêt à voyager et à travailler en étroite collaboration avec les clients (maximum 25% du temps)
Solide compréhension de l'analyse, de l'apprentissage automatique, de la science des données et de la Business Intelligence Big Data
Une maîtrise courante du français écrit / oral est un atout
Une personne autonome, désireuse d'apprendre de nouvelles technologies et s'efforçant continuellement d'améliorer ses propres pratiques de codage
Bases de données relationnelles (MySQL, PostgreSQL, Oracle)
Hadoop (Hive, Impala, HDFS)
Microservices, Docker et Git
API RESTful
Déploiement continu
Frapper
Ansible
Chez Thales, nous proposons des CARRIÈRES passionnantes, pas de simples emplois. Fort de ses 80 000 collaborateurs dans 68 pays, Thales a mis en place une politique de mobilité permettant, chaque année, à des milliers d'employés de faire progresser leur carrière tant dans leur domaine d’expertise que dans de nouveaux domaines de compétences, cela aussi bien dans leur pays d’origine qu’à l'étranger. Ensemble, nous pensons qu’adopter une politique de flexibilité est une manière plus actuelle de travailler. C’est ici que commence votre parcours exceptionnel, postulez sans tarder!
At Thales we provide CAREERS and not only jobs. With Thales employing 80,000 employees in 68 countries our mobility policy enables thousands of employees each year to develop their careers at home and abroad, in their existing areas of expertise or by branching out into new fields. Together we believe that embracing flexibility is a smarter way of working. Great journeys start here, apply now!
Thales s’engage à promouvoir un lieu de travail diversifié et inclusif pour tous. Thales s’engage à fournir des accommodements à toute les étapes du processus de recrutement. Les candidats retenus pour une entrevue qui ont besoin d’accommodement sont priés d’en informer à la suite de l’invitation pour une entrevue. Nous travaillerons avec vous pour répondre à vos besoins. Toutes les informations relatives à l'accommodement fourni seront traitées de manière confidentielle et utilisées uniquement dans le but de fournir une expérience de candidat accessible.
Thales is committed to a diverse and inclusive workplace for all. Thales is committed to providing accommodations in all parts of the interview process. Applicants selected for an interview who require accommodation are asked to advise accordingly upon the invitation for an interview. We will work with you to meet your needs. All accommodation information provided will be treated as confidential and used only for the purpose of providing an accessible candidate experience."
Software Engineer in Algorithms & Optimization for Data Anal...,"Waterloo, ON",Tangam Systems,None,Organic,"Role Summary:
Advances in data analytics and visualization are revolutionizing the way businesses operate. Tangam Systems is a leader in driving that transformation in the gaming industry.
As the newest member of our Algorithms & Optimization team, you will contribute to the complex suite of algorithms powering our platform’s core functions - predictive analytics, automated recommendations, and intuitive visualizations.
The ideal candidate must possess a sound grasp of Algorithms and Data Structures, strong critical thinking and reasoning skills, and the ability to produce readable, maintainable, and working software programs from abstract concepts and ideas. You will thrive in a small-team environment with fast-paced agile iterations, where every developer contributes code that delivers a meaningful impact to our customers and the company.

Responsibilities:
In this role, you will be switching hats between Software Engineer and Researcher depending on the problem at hand.
As a Researcher, you will:
Stay abreast of innovations and publications in the field of operations research and business intelligence systems.
Research and solve complex scheduling, resource allocation and pricing scenarios involved in operations optimization.
Analyze raw operational data and design algorithms that can automatically and consistently generate operational recommendations for clients.
Contribute to the invention of novel solutions to client’s operational problems by collaboratively working with product managers, co-developers, and our client success team.

As a Software Engineer, you will:
Utilize efficient algorithm design in a parallelized fashion capable of crunching gigabytes of operations data in minutes and scaling up with client growth.
Build dashboards that transform operational data into visualizations that are intuitive and actionable
Contribute refinements to our existing product through the development of new features as well as refactoring existing code to make it more efficient and object-oriented.
Advance your knowledge of new software tools, agile programming methods, business intelligence technologies and share your knowledge with the development team, thus catalyzing process / technology changes to help us be more effective.

Your playground / what you’ll learn:
At Tangam you’ll get a chance to play, learn and build with the following tools and technologies as part of a cross-functional team that is the world’s foremost innovator in gaming analytics and visualization software.
Languages: C#, JavaScript, TypeScript
Frameworks: .NET Core, Angular
Web Server: IIS, NGINX
Databases: MS SQL, Azure SQL
Infrastructure: Azure, Docker, Kubernetes, GitLab
Logistics engine: algorithms for discrete optimization problems
Operation Systems: Windows, Linux
Development Processes: Agile, CI/CD

Qualifications and Experience:
Required:
2+ years of experience in Software Development, preferably with high performance algorithms or data intensive applications.
A deep and intuitive understanding of Algorithms and Data Structures.
Ability to process, assimilate, and explain complex and abstract concepts from research publications.
Preferred:
Specialized knowledge. Seeking candidates with expertise in two or more of the following:
Operations Research or Management Engineering
Mathematical Optimization
Data Science / Machine Learning
Education:
Master’s Degree or PhD in Applied Mathematics/ Management Science/ Operations Research/ Computer Science / Engineering, or related technical discipline.

Who we are:
http://www.tangamsystems.com
Tangam Systems is a small, profitable, and growing company that is at the cutting edge of developing data analytics and visualization software. Our clients range from Fortune 500 businesses to some of the best-known brands in the resort industry. Our software product helps casino resorts optimize their operations and logistics in order to increase profitability and enhance the patron experience. Our software takes the complexity out of data analytics and makes data understandable and actionable to ordinary users. Our global client base includes operators in China, United States, Australia, New Zealand, United Kingdom, and Canada. Our patented products have won numerous industry awards and recognitions. We continue to push the envelope on what is possible in the area of data analytics software, and we are always seeking new talent to join us on our exciting journey.

Compensation and Benefits:
Base salary of $80K - $115K + performance-based bonus or stock options
Work-Life Balance: Flex time, work from home days and travel incentives.
Set-up: Standing / adjustable desks, massage chair & quiet rooms, employee lounge with Xbox, Switch & PS4.
Benefits Plan: Fitness allowance, dental/prescription/vision, massage & physio, and healthcare spending account.
Food & Fun: Fully stocked kitchen, fancy coffee machine, team lunches, long weekend bottle draws and monthly employee events.

We are committed to providing an inclusive work environment that welcomes diversity and supports accessibility across the organization. If you require accommodation at any time during the recruitment process, please let us know by contacting us at Tangam-Tngm0335@applications.recruiterbox.com"
Software Engineer with Data Integration,"Toronto, ON",Scotiabank,None,Organic,"Requisition ID: 90654
Join the Global Community of Scotiabankers to help customers become better off.
The Team
In partnership with the Customer Insights Data and Analytics teams and our IT partners the Data and Analytics Technology team supports the banks Data and Analytics needs with tooling, projects and IT operational support.
The Role
The Software Engineer role will be responsible for the Platform designing, building, monitoring, tuning, and troubleshooting bank wide initiatives supported by Data and Analytics Technology. This consists of platform building, tooling, integration, process automation, platform enhancement, and delivery of new projects.
Some of the key accountabilities include:
Build Big-Data and Data Science platform and frameworks using Big-Data and Distributed compute technologies.
Participate in and build tools to diagnose and fix complex distributed systems handling petabytes of data & drive opportunities to automate infrastructure deployments, and observability of data services.
Troubleshoot, system capacity, bottlenecks, basics of memory, CPU, kernel, and storage.
Test, monitor, administer, optimize and operate multiple big data clusters on premise and/or in cloud.
Partner with ETL developers in building best practices for Data Warehouse and analytics environment.
Investigate emerging technologies in big data space that relate to our needs and implement those technologies.
Documents and maintains operational procedures and processes.
Maintains effectiveness of data management processes to be complied with bank's policies and standards.
Understand of change control processes and write change requests.
What You Will Bring to Succeed
4 + years of recent experience in data engineering, handling services in a large-scale distributed systems environment
Exposure to data lineage, meta data management and ETL tools like Collibra/Atlas/IGC, Erwin and DataStage/ Talend / Diyotta
A solid track record of data management showing your flawless execution and attention to detail.
Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks
Experience implementing and using streaming platforms such as SparkSQL, Flink, Kafka, Storm, etc.
Knowledge of Linux operating system internals, file systems, disk/storage technologies and storage protocols and networking stack
Experience with cluster / Grid environment and integration among applications, data (Mulesoft exp is a plus)
Proven knowledge of systems programming (bash and shell tools) and/or at least one scripting language (Java, Python, Ruby, Perl)
Experience with RDBMS and/or NoSQL (e.g. ORACLE, MySQL, PostgreSQL, MongoDB, Cassandra)
Experience with building DevOps/deployment pipelines with Ansible and other related technologies
Experience with Docker or other containerization technologies (Kubernetes e.g.)
Experience in supporting or developing Micro Services, Serverless architecture, Service Oriented Architecture, Web Services, or Data Processing Pipeline
Exposure to working on cloud platforms like Azure/GCP/AWS
Experience with Agile/Scrum development methodologies
Team player with strong communication skills (verbal and written)
Able to see tasks through to completion without significant guidance
Self-managed and results-oriented with sense of ownership is required
Bachelor's degree or better in Computer Science, Mathematics or relevant.
The Workplace
We are technology partners who help the business transform how our employees around the world work
We have an inclusive and collaborative working environment that encourages creativity, curiosity, and celebrates success!
You'll get to work with and learn from diverse industry leaders, who have hailed from top technology companies around the world
We foster an environment of innovation and continuous learning
We care about our people, allowing them to design how they work to deliver amazing results
We offer a competitive total rewards package, including a performance bonus, company matching programs (on pension & profit sharing), and generous vacation
#INTECH
Location(s): Canada : Ontario : Toronto
As Canada's International Bank, we are a diverse and global team. We speak more than 100 languages with backgrounds from more than 120 countries. Our employees are committed to a superior customer experience and use the Bank’s six guiding sales practice principles to ensure they act with honesty and integrity.
At Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted."
Data Engineer,"Toronto, ON",EQ Works,None,Organic,"Data engineering at EQ means you're working in the hottest areas of today's technology landscape; machine learning, big data engineering and rich (location) data sets. You will be coming up with solutions to derive actionable insights about behaviour, demographics, and personality out of our multi-terabyte dataset of location data. Examples of the type of analysis we do include; understanding what university students do during summer holidays, predicting if someone is about to buy a house based on places they visit or trying to understand someone at the airport is a business or leisure traveller.

Your role will involve working very closely with our CTO, data scientists and the extended product team. With EQ leading the pack when it comes to location ad analytics in Canada and a top North American player - this role would let you define and shape the standards in this very vibrant and evolving industry.

Understand our current data sets and models and help us discovering new ways to enrich the data
Creatively extracting real-world behaviour and trends out of the location data
Monitor and build processes for cleaning up inbound data
Dream up a solution, perform the R&D and deploy to production within our fluid work environment
Requirements
Ability to work with large amounts of data
Experience with map-reduce frameworks - Hive, Hadoop and Spark (Elastic Map Reduce would be a plus)
Strong grasp of statistics, data modelling and designing algorithms
Benefits
Cloud service credits (AWS, Google Cloud, Azure, Digital Ocean)
Public Transit allowance
Mobile data allowance
Flex days
Tea/Coffee Bar
In-office Snacks"
"Senior Software Engineer, Data Platform, Tableau","Vancouver, BC",Salesforce,None,Organic,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
What you’ll be doing...

Einstein Analytics, recently rebranded as Tableau CRM, is the best-in-breed analytics for the CRM market. Within the Data Platform, our team is responsible for supporting customer dataflows at scale and building the next generation dataflow service that can handle phenomenal volumes of data resiliently to support our customer's business critical processes backed by Einstein Analytics. The EA Data Platform is at the heart of enabling our customers to curate and shape their data in preparation for analysis, join us and help build the future of our backend!
You will build out new features supporting multi billion row scale dataflows.
Iteratively improving the reliability, performance and resiliency of our dataflow architecture using a metrics driven approach.
Iterating rapidly on improvements with a biweekly release cycle.
Working with a modern container-based architecture.
Collaborating with other teams building new features on our service.
Delivering internal tooling to improve the operational visibility of flows to help rapidly diagnose and proactively catch production issues.
Keeping a customer success focused mindset in addressing production issues.
Work on owning and operating multiple instances of mission-critical infrastructure services including, monitoring, alerting, logging, and reporting applications.
You will be building data-processing or ETL products at scale


Who you are...
Experienced. Y ou have 3-5 years of relevant professional experience. You have experience with agile development methodology and testing. You have experience using telemetry and metrics to drive operational excellence. You may have a Bachelor’s degree in Computer Sciences or similar field of study. You may have experience with one or more of the following, Spark, Kafka, Java, REST, gRPC, Kubernetes, Docker, Jenkins, Splunk, Python, Spinnaker, Parquet, Avro, Go, Presto.
Technical. You have a deep knowledge of object oriented programming and experience with at least one object oriented programming language: Java, C++, C#, Ruby, Go, Scala, Python, but this team primarily uses java. You have a solid knowledge of database / big data / distributed technologies. You will have experience in public cloud engineering on GCP, AWS, Alibaba and/or Azure platforms
Passionate. You have a passion for data and building features for our customers that work.
Relentlessly High Standards. You understand what it takes to write software that is used by thousands or millions of people. You love writing things that are robust, scalable, and perform well. You thrill in your accomplishments but also know it's about doing and improving.
A True Team Player. You enjoy collaborating, learning from and teaching others so we can all become better developers. You assume good intent in others, and actively do your part to make a positive work environment.
You are a Recruiter! Tableau hires company builders and, in this role, you will be asked to be on the constant lookout for the best talent to bring onboard to help us continue to build one of the best companies in the world!


#LI-NF
Accommodations - If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesfore.com or Salesforce.org.
Salesforce welcomes all."
"Senior Software Engineer, Data Store","Vancouver, BC",Tableau,None,Organic,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
What you’ll be doing...

Einstein Analytics, recently rebranded as Tableau CRM, is the best-in-breed analytics for the CRM market. Within the Data Platform, our team is responsible for building the unified Salesforce Analytics Data store that will:
Support customers to access Salesforce data for self-serve query exploration, dashboards, and machine learning.
Built-in low-latency, reliable, scalable data pipelines from Salesforce properties.
Have a suite of data connectors to ingest external data and write out data.
Include powerful, but easy to use tools to filter, aggregate, shape and model data.
Optimized for Salesforce for connectivity, sharing, collaboration, and enterprise config.
Design so that it will have a rich ecosystem of ISVs, system integrators and apps.
Easy to connect with Tableau, dynamic scalability, performant and enterprise ready.
Building the integrations between Tableau and other Salesforce products sets.
Who you are...

Experienced. You have 5-7 years of relevant professional experience You have deep knowledge of object oriented programming and experience with at least one object oriented programming language preferably Java. Bachelor’s degree in Computer Sciences or equivalent field. You have experience owning and operating multiple instances of mission-critical infrastructure services. Experience designing, developing, debugging, and operating resilient distributed systems.
Technical. you have a solid knowledge of database / big data / distributed technologies. You have experience using telemetry and metrics to drive operational excellence. You have experience with building data storage/data management systems would be ideal. Experience with Spark, Kafka, REST, gRPC, Kubernetes, Docker, Jenkins, Splunk, Spinnaker, Parquet, Avro, Presto.
Passionate. You have a passion for design patterns, data structures, algorithms, and concurrency, and have experience working with them.
Relentlessly High Standards . You understand what it takes to write software that is used by thousands or millions of people. You love writing things that are robust, scalable, and perform well. You thrill in your accomplishments but also know it's about doing and improving.
A True Team Player . You enjoy collaborating, learning from and teaching others so we can all become better developers. You assume good intent in others, and actively do your part to make a positive work environment.
You are a Recruiter! Tableau hires company builders and, in this role, you will be asked to be on the constant lookout for the best talent to bring onboard to help us continue to build one of the best companies in the world!



#LI-NF
Accommodations - If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesfore.com or Salesforce.org.
Salesforce welcomes all."
Lead Software Engineer (Big Data),Canada,EPAM Systems,None,Organic,"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.


Description

You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Lead Software Engineer (Big Data). Scroll down to learn more about the position’s responsibilities and requirements.
#LI-DNI
#LI-DNP
What You’ll Do
Be passionate about writing loosely coupled, readable, reusable and modular code
Have a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before impacting the business
Design and architect different alternate solutions independently
Have a passion to innovate and learn
Take ownership and accountability
Understands the benefits of automation
Have a desire to simplify and reduce dependencies
Be entrepreneurial / business minded
Take risks and champion innovative ideas
What You Have
2+ experience with big data technologies such as Hadoop, Hive, Spark, Kafka, Yarn, Scala
3+ years of working in any object orientated programming language: Java, Python, .Net, C++, etc
Experience using TDD and unit testing as part of normal software development is required using packages such as PyUnit, SonarQube, Fortify, Postman, etc
1+ years of RDBMS SQL or database experience. No-SQL variants (Cassandra, MongoDB, DynamoDB) a plus
Experience working with large structured data sets (millions of rows) in distributed data environments
Experience with AWS cloud technologies a plus, such as Redshift, ElastiCache, Terraform, Snowflake, EMR, Lambda, Glue, Aurora
Experience developing abstractions and frameworks for organizations
1+ years being part of Agile teams – Scrum or Kanban
Understand and used Object Orientated design techniques
Experience developing in build environments such as Maven
Experience with parsing and transmitting data in JSON, CSV, or XML formats
Experience with Git version control tools, including automating CI pipelines using Git Hooks
Experience with defining RESTful and GraphQL web APIs, including documentation tools of swagger, GraphQL playground or iGraphQL
Experience with containerization platforms such as Docker, Kubernetes a plus
Experience using monitoring data visualization tools a plus, such as Cloud Watch, Grafana, Kibana, etc
Excellent communication skills and team collaboration skills
Experience developing in an CI/CD environment (Jenkins/Ansible experience a plus)
Experience with Pair Programming is a plus
Knowledge and/or experience with Healthcare domains is a plus
What We Offer
Extended Healthcare with Prescription Drugs, Dental and Vision Insurance (Company Paid)
Life and AD&D Insurance (Company Paid)
Employee Assistance Program (Company Paid)
Unlimited access to LinkedIn learning solutions
Long-Term Disability
Registered Retirement Savings Plan (RRSP) with company match
Paid Time Off
Critical Illness Insurance
Employee Discounts"
Data Center Regional Structural Engineer (Field Engineering),Ontario,"Amazon Data Services CAN, Inc.",None,Organic,"Bachelor’s degree in civil or structural engineering.
Professional Engineering (PE) license.
5-8 years of experience in structural design for industrial or commercial projects.

Amazon Web Services (AWS) is seeking a Structural Engineer to become part of a Data Center Global Services Engineering team. This engineer will be located in virtual location, CANADA and is responsible for the structural infrastructure strategy and continuous product innovation and development to support our rapidly expanding global data center footprint. AWS Data Centers are a major component of worldwide cloud-computing infrastructure and are industry leading examples of critical facilities.
Engineers at Amazon work to design resilient, cost effective data center structural systems. As engineers at Amazon we are responsible for achieving a world class uptime for our customers. We justify and communicate the technical decisions we make to Senior Management and work hard to drive continuous advancements and improvements with our designs. As an engineer at Amazon you have the ability to drive change and define/design the systems our customers rely on.
We are looking for an engineer with hands on mission critical data center structural design and implementation experience. If you have experience innovating new structural system approaches, master planning building structural systems, and understand the constructability of varying designs you may be a good fit.
Amazon offers a fast paced, fun, and exciting work environment. We continue to grow at exponential rates and are looking for individuals that can support our speed to market, enjoy a challenge, and have a desire for continued professional growth. Amazon’s work environment is unique in every aspect and offers an exceptional opportunity for the right candidate.
At Amazon team work is absolutely necessary for us to accomplish our goals. You must be able to work within a team and depend on others to accomplish the required work. As a structural engineer at Amazon you will be working with other internal groups as well as external groups including local jurisdictions, manufacturers, developers, and vendors.
If you are a creative, smart and driven individual that enjoys working with complex problems we want YOU! At Amazon you will be responsible for working on multi-million dollar designs that support our array of businesses and wide variety of customers. Great ideas are encouraged and supported. Ingenuity is the main mechanism which supports our focus on quality, speed, and cost.
The Structural Engineer will be responsible for the following:
Work with the other design and engineering professionals on concepts, build, and retrofit projects.
Create, review and release data center Structural improvements and architectural designs.
Provide relevant feedback into Basis of Design, prototype design, and template specifications for structural systems.
Serve as a technical advisor for AWS DC design projects including; review of foundation and structural systems, evaluating superstructure design options, and assessing structural calculation assumptions.
Create, review and release architectural/civil/structural design RFPs.
Manage our external architectural/civil/structural design consultants.
Coordinate with internal and external MEP design engineers.
Lead initiatives aimed at improving cost, quality, schedule, and consistency.
Work on multiple data center build and capital improvement projects simultaneously.
Travel for site assessments, consultant selection activities, internal design meetings, construction review, interface with AHJs, design consultants, quantity surveyors, and building owners.
Anticipated travel not to exceed 25%.

Master’s degree in civil or structural engineering.
10+ years of experience directly related to DC or mission critical facility design.
Detailed understanding of tilt-up and precast concrete structures, structural steel systems, seismic design, wind loading, snow loading, and building forensic analysis.
Detailed understanding of geotechnical engineering, structural modular design, building construction, forensic analysis of building design and construction defects, and evaluating value engineering for building structure and foundation.
Strong knowledge of international building codes and regulations including IBC, ASCE, ACI, AISC, ASTM, and OSHA.
Basic understanding of large scale mechanical and electrical system components and designs.
Direct experience with the design, construction, operation, or maintenance of DCs.
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Meets/exceeds Amazon’s leadership principles requirements for this role

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us."
"Senior Software Engineer, Data Store, Tableau","Vancouver, BC",Salesforce,None,Organic,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Products and Technology
Job Details
What you’ll be doing...

Einstein Analytics, recently rebranded as Tableau CRM, is the best-in-breed analytics for the CRM market. Within the Data Platform, our team is responsible for building the unified Salesforce Analytics Data store that will:
Support customers to access Salesforce data for self-serve query exploration, dashboards, and machine learning.
Built-in low-latency, reliable, scalable data pipelines from Salesforce properties.
Have a suite of data connectors to ingest external data and write out data.
Include powerful, but easy to use tools to filter, aggregate, shape and model data.
Optimized for Salesforce for connectivity, sharing, collaboration, and enterprise config.
Design so that it will have a rich ecosystem of ISVs, system integrators and apps.
Easy to connect with Tableau, dynamic scalability, performant and enterprise ready.
Building the integrations between Tableau and other Salesforce products sets.
Who you are...

Experienced. You have 5-7 years of relevant professional experience You have deep knowledge of object oriented programming and experience with at least one object oriented programming language preferably Java. Bachelor’s degree in Computer Sciences or equivalent field. You have experience owning and operating multiple instances of mission-critical infrastructure services. Experience designing, developing, debugging, and operating resilient distributed systems.
Technical. you have a solid knowledge of database / big data / distributed technologies. You have experience using telemetry and metrics to drive operational excellence. You have experience with building data storage/data management systems would be ideal. Experience with Spark, Kafka, REST, gRPC, Kubernetes, Docker, Jenkins, Splunk, Spinnaker, Parquet, Avro, Presto.
Passionate. You have a passion for design patterns, data structures, algorithms, and concurrency, and have experience working with them.
Relentlessly High Standards . You understand what it takes to write software that is used by thousands or millions of people. You love writing things that are robust, scalable, and perform well. You thrill in your accomplishments but also know it's about doing and improving.
A True Team Player . You enjoy collaborating, learning from and teaching others so we can all become better developers. You assume good intent in others, and actively do your part to make a positive work environment.
You are a Recruiter! Tableau hires company builders and, in this role, you will be asked to be on the constant lookout for the best talent to bring onboard to help us continue to build one of the best companies in the world!



#LI-NF
Accommodations - If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesfore.com or Salesforce.org.
Salesforce welcomes all."
Data Engineer,"Vancouver, BC",Providence Health Care,None,Organic,"Reporting to the Manager, Operations Research and Analytics, the Data Engineer develops and establishes scalable, efficient, automated processes for large scale data analyses, and model development, validation and implementation. The position collaborates closely with other members of the Operations Research and Analytics team to create and deploy automated data pre-processing and advanced analytics models through the innovative understanding and use of large data sets to improve clinical processes and patient outcomes, and support data-driven decision making. The ideal candidate will have extensive experience in dimensional modeling, developing advanced analytics algorithms, optimizing processes with advanced analytics solutions, and excellent problem solving ability dealing with huge volumes of corporate and clinical data. The position will also stay apprised of current trends and research on all aspects of data engineering and advanced analytics techniques and works in collaboration with provincial and national colleagues.
Skills
Demonstrated strength in data modeling, ETL development, and data warehousing with solid knowledge of various industry standards such as dimensional modeling, and star schemas etc.
Demonstrated ability to build and optimize ‘big data’ data pipelines, architectures and data sets and computing tools such as Spark, MySQL, Azure, AWS, etc.
Demonstrated knowledge of methods and techniques involved in Advanced Analytics, data mining, statistics, and optimization (including neural networks, reinforcement learning, and adversarial learning).
Coding proficiency in at least one modern programming language (Python, Java, etc)
Demonstrated proficiency working with both relational (SQL) and non-relational databases (NoSQL).
Demonstrated understanding of data privacy, security and related tools such as anonymization and encryption.
Excellent oral and written communication skills and ability to clearly and fluently translate technical findings to non-technical partners and to communicate to multiple audiences using data storytelling and through graphics.
Demonstrated ability to work collaboratively in an interdisciplinary environment and to develop recommendations using facilitation and consensus building.
Strong analytical, critical thinking, and evaluation skills to discern and help solve the important problems facing health care, to identify new ways to leverage our data, and to direct efforts in the right direction.
Education
A Masters’ Degree in Computer Science, Mathematics, Engineering, or other quantitative degree is required plus at least five (5) years’ experience as a Data Engineer or related specialty (e.g., Software Developer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets. Experience with healthcare analytics is an asset.
Duties
1. Creates and maintains optimal data pipeline architecture. Builds analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.

2. Builds the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and big data technologies. Re-designs infrastructure for greater scalability.

3. Assembles large, complex data sets that meet functional/non-functional business requirements. Creates data tools for the operations research and analytics team to assist them in building and optimizing our product into an innovative industry leader.

4. Identifies, designs, and implements internal process improvements including establishing standards for development processes and technical requirements; automating manual processes; optimizing data delivery, etc. Develops and ensures compliance for data management processes, policies and standards. Implements and enforces controls to maintain data availability and quality.

5. Recognizes and adopts best practices in data integrity, test design, analysis, validation, and documentation. Tunes application and query performance using profiling tools and SQL.

6. Researches innovative solutions, develops new concepts, and implements proof-of-concept prototypes by applying expertise in data engineering, advanced analytics, process modeling, and optimization tools and techniques.

7. Works in close collaboration with other members of the Data Analytics and Health Informatics teams to develop and implement innovative solutions that address pressing operational and clinical challenges within the organization.

8. Reviews clinical data at aggregate levels on a regular basis using analytical reporting tools to support the identification of risks and data patterns or trends. Creates analytical reports and presentations to facilitate review and adoption of data-driven choices. Collaborates with project/program teams to address data-related questions and to recommend potential solutions.

9. Works closely with clinical and management teams across PHC to strategize, develop, and implement advanced modelling projects that translate into improved quality of care, clinical outcomes, reduced costs, temporal efficiencies, and process improvements.

10. Works with stakeholders including the Executive, and Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

11. Keeps up-to-date with the latest technology trends and methods by staying abreast of state-of-the-art literature in the fields of advanced analytics, statistical modeling, statistical process control and mathematical optimization.

12. Perform other duties as required."
Azure Data Engineer,"Ottawa, ON",Accenture,None,Organic,"ARE YOU READY to step up and take your technology expertise to the next level?

There is never a typical day at Accenture, but that’s why we love it here! This is an extraordinary chance to begin a rewarding career at Accenture Technology. Immersed in a digitally compassionate and innovation-led environment, here is where you can help top clients shift to the New using leading-edge technologies on the most ground-breaking projects imaginable.

Interested in building end-to-end marketing solutions for clients? Bring your talent and join Data which operates in the Interactive, Mobility and Analytics space. You will have opportunities to get involved in digital marketing, eCommerce and end-to-end mobility capabilities to help clients to improve productivity and more!

WORK YOU’LL DO
Work across the Service Delivery Lifecycle to analyze, design, build, test, implement and/or maintain multiple system components or applications for Accenture or our clients
Responsible for the maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases
Support our database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects
Determines methods and procedures on new assignments with guidance
Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture

WHO WE´RE LOOKING FOR?
Minimum 5 years of experience as a Data Engineer
Must have hands-on experience with Spark and Hadoop
Must have experience with one of the Cloud Technologies (preferably with Azure)
Azure cloud includes Spark, Python, Databricks, Synapse, Snowflake, Data Factory and ADLS

Experience with Big Data technologies like MapReduce, Pig, Hive, HBase, Sqoop, Flume, YARN, Kafka, Storm and etc.
2+ years of experience with at least one SQL language such as T-SQL or PL/SQL
2+ years of work experience with ETL and data modeling
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience in both batch and stream processing technologies
Experience with object-oriented/object function scripting languages: Java, C++, Scala
Machine learning experience with Spark or similar
Must be eligible for security clearance
Microsoft /AWS Certified: Azure Data Engineer Associate/ AWS Certified Data Analytics (Specialty)

Professional Skills Qualifications:
Proven success in contributing to a team-oriented environment.
Proven ability to work creatively in a problem-solving environment.
Desire to work in an information systems environment.
Demonstrated teamwork and collaboration in professional setting; either military or civilian.
Competitive benefits, including a fair and balanced parental leave policy.
Fantastic opportunities to develop your career across industries with local and global clients.
Performance achievement and career mentorship: our performance management process focuses on your strengths, progress and career possibilities.
Opportunities to get involved in corporate citizenship initiatives, from volunteering to charity work.


We are committed to employment equity. We encourage all people, including women, visible minorities, persons with disabilities and persons of aboriginal descent to apply.
WHAT´S IN IT FOR YOU?
To learn more about Accenture, and how you will be challenged and inspired from Day 1, please visit our website at accenture.ca/careers.
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve our clients, our employees must be available to travel when needed.
Accenture does not discriminate on the basis of race, religion, color, sex, age, non-disqualifying physical or mental disability, national origin, sexual orientation, gender identity or expression, or any other basis covered by local law. Accenture is committed to providing employment opportunities to current or former members of the armed forces.
Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions — underpinned by the world’s largest delivery network — Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com."
Data Engineer - Calgary AB,"Calgary, AB",Teknobuilt,"$60,000 a year",Organic,"We are hiring a Data Engineer, based in Calgary-AB, Canada!
About Company:
Teknobuilt is an innovative construction technology company accelerating Digital and AI platforms to help all aspects of program management and execution for workflow automation, collaborative manual tasks and siloed systems. Our platform has received innovation awards and grants in Canada, UK and S. Korea and we are at the frontiers of solving key challenges in the built environment and digital health, safety and quality.
Teknobuilt's vision is helping the world build better- safely, smartly and sustainably. We are on a mission to modernize construction by bringing Digitally Integrated Project Execution System - PACE and expert services for midsize to large construction and infrastructure projects. PACE is an end-to-end digital solution that helps in Real Time Project Execution, Health and Safety, Quality and Field management for greater visibility and cost savings. PACE enables digital workflows, remote working, AI based analytics to bring speed, flow and surety in project delivery. Our platform has received recognition globally for innovation and we are experiencing a period of significant growth for our solutions.
Job Brief:
The Data Engineer is responsible for developing solutions for Machine Learning and AI based algorithms from programming, algorithms and Data handling aspects of the platform. It also involves assessing, transforming, improving, cleaning and manipulation of data in the business’s operations, analytics databases and external systems.
You will work closely with the industrial subject matter experts, engineers, data analytics teams, data scientists and data warehouse engineers in order to understand and aid in the implementation of database requirements, machine learning solutions, analyze performance and troubleshoot any existent issues. You will also manage the support operations and delivery of new project work streams.
Responsibilities include:
Design and implement machine learning data models and data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirement.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.
Build the infrastructure required for ETL operations from a wide variety of data sources.
Assist and research with Data scientists for training and implementing cloud based machine learning techniques.
Work with stakeholders including the Executives, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to build monitoring tools for live production of ML experiments & support the orchestration of end-to-end ML projects
Qualification:
Bachelor’s or Master's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Desired Skills required:
Minimum 7 years experience in the industry in computer programming, analytics, data science or engineering roles.
Experience with at least one big data tool: Hadoop, Spark, Kafka, etc.
Experience with relational any one SQL and any one NoSQL databases, such as MySQL, Postgres, Cassandra, HIVE etc.
Experience with AWS or similar cloud services.
Experience with object-oriented/object function scripting languages: Python, Scala, etc.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Additional Details -
Salary Range: Competitive
Position Type: Permanent
Employment Type: Full Time
WE APPRECIATE YOUR INTEREST IN TEKNOBUILT
Job Type: Full-time
Salary: From $60,000.00 per year
Schedule:
8 hour shift
Work remotely:
Yes, temporarily due to COVID-19"
Cloud Data Analytics Engineer,"Burnaby, BC",Best Buy,None,Organic,"Our Technology team has been on an incredible journey the last few years by creating first-to-market initiatives for our customers, adding the latest development languages to our tech stack and establishing Best Buy Canada as the best Omni-channel experience within the retail industry.
We believe empowered people and teams make smarter, faster, and more creative decisions, which is why we operate in a truly Agile environment where the distance between any one person and senior leadership is microscopic. Here, you’ll work on something big, small, or super cool and before you can blink 100,000 people will see it. You’ll create fast, learn fast, and develop fast! Oh, and sometimes you’ll fail fast too. That’s ok. (Honestly.) It’s all part of the process.
The Cloud Data Analytics Engineer is a creative thinker who is skilled and well versed in Power Apps, Power BI, Power Automate, Power Virtual Agents and Azure Machine Learning Studio. This individual enjoys challenges and likes solving complex data problems and will focus on transforming business requirements into functionally using Microsoft Power Platform. You will directly report to the Manager Data Analytics and you will work closely with Manager Data Platform Products, Manager (Web Analytics – Testing Platforms) and Digital Intelligence Team as they provide guidance and vision so you can develop, construct and maintain our enterprise data catalogs and enterprise information models. You will work with large scale data processing system that collects data from a variety of structured and unstructured data sources, stores data in a cloud scale-out data lake/cloud data warehouse and prepare the data using ELT techniques in preparation for the data science data exploration and analytic modeling.
As a Cloud Data Analytics Engineer you will…
Harness, model and transform data (structured, unstructured) from several sources that empowers users to analyze data in different ways leading to better and faster decision making.
Process business requests and bring business meaning to data from various data sources, aggregating and synthesizing data using SQL and other BI tools, communicating the results in an effective and efficient manner.
Build self-service pipelines and data flows for the business using the Microsoft Power Platform.
Extract relevant data from a centralized system and convert data into information and knowledge for the business to enable self-serve data analysis
Create information models and define semantic layer on data for various business functions and stakeholders.
We hope you are passionate about…
Data Analysis - you creatively navigate through a complex network of data structures, pull relevant data, apply ML/AI and extract valuable insights.
Listening and Communication – you speak to the business, can synthesize complex analyses and communicate in a simple manner.
Relationship Building - you are a communicator, able to influence, gain buy-in and provide guidance to key stakeholders.
The experience we need…
5+ Years of experience with Microsoft Power Platform (Power BI, Power Apps, Power Automate, Power Agents).
Experience in designing Information Semantic models using taxonomy and ontology concepts.
Experience with Microservices design.
Bonus points…
Familiarity with agile/scrum methodologies.
Experience designing and developing data applications for a retail environment.
Bachelor’s Degree or Diploma in Computer/Data/Information Science or related discipline/experience."
Data engineer,"Toronto, ON",Q4,None,Organic,"About Q4
Q4’s objectives are simple. Hire smart, diverse and capable people to build the best platforms possible and provide exceptional client experiences.

We’ve been revolutionizing the Investor Relations space, connecting over 2,300 public companies including Nike, Amazon, Shopify & Apple with investors using our cloud-based full-stack IR solutions for the past 14 years.

If you’re looking for a career opportunity with a fast-growing tech company and a ‘get it done’ attitude then we want to hear from you.

The gig.
The data engineer will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Key Responsibilities
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Keep our data separated and secure through multiple data centers and AWS regions.
Create data tools for analytics and different team members that assist them in building and optimizing our product into an innovative industry leader.
Qualifications
Bachelor's Degree in Computer Engineering or a related field required, or equivalent education
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing system
Analytical mind, critical thinker, problem-solver
Understanding of the financial services industry and global capital markets
Why Q4?
We are motivated by solving complex problems in unorthodox ways. Emphasis on your well-being means you experience your true potential. We offer a variety of benefits to ensure you can always work hard and have fun:
360 Support. Leverage our lifestyle benefit and employee assistance program to spruce up your workspace, invest in personal wellness or simply spoil yourself!
Unlimited paid time off and flexible working hours. Rest is important. Enough said.
Flexible working environment. Choose your home, one of our trendy offices or mix it up.
Generous health and lifestyle benefits. You are in charge of your benefit dollars.
Virtual team building and socials. Keeping people connected is important. Invest in your development. We’ll help you with your tuition
Join #Q4orce
Q4’s diverse culture fosters a friendly, open-minded workplace. As a member of a dynamic, high-performing team, each Q4 employee is hungry to learn, valued for their contribution, and approaches each day excited to make an impact. With great reasons to work here, take advantage by submitting your application to join our growing team.

Q4 values diversity and people of all backgrounds and abilities. Should you require any accommodations prior to or during the interview process, please indicate this during the interview process."
Senior Data Engineer,"Montréal, QC",Brain Finance,None,Organic,"Job Description
Status: Permanent – Full-time
Location: Montréal – Griffintown
We are looking for a passionate and experienced Senior Data Engineer with strong Python or Scala experience to join our expanding analytics team consisting of data- and algorithm-focused individuals. In this team, you will participate in growing and improving BrainFinance’s data environment and as such, will be responsible for building, deploying, and maintaining data pipelines for batch and real-time data analytics. You will be working alongside experienced developers and researchers as part of a growing and exciting team of quantitative-minded individuals at the core of the business’ operations. We expect the candidate to be comfortable working in a dynamic start-up environment requiring high autonomy, resourcefulness, and strong problem-solving skills.
Responsibilities
As part of the analytics team, you will be contributing to the development of our data environment through the integration and evaluation of a high number of data sources. You will be working closely with both our technical experts as well as our machine learning experts to build and support the analytical operations.
With us you will:
Develop and maintain data pipelines;
Evaluate and integrate third-party data;
Analyze, parse and extract data from structured and unstructured datasets;
Gather and process raw data at scale;
Create and maintain various datasets using complex data transformation both in batch and real-time modes;
Implement event-based and status-based rule-engines;
Identify and correct data quality issues;
Design and maintain machine learning serving infrastructure;
Work closely with internal partners (technology, machine learning, and business experts);
Advantages
Competitive salary.
Fun and relaxed work environment.
Full health benefits - Medical.
Free healthy snacks and refreshments.
Advancement opportunities.
Great office location.
Providing opportunities to attend trainings and conferences
Requirements
BS/BA in Computer Science, or equivalent experience
3+ years of professional experience in data pipeline development and maintenance
Strong experience in data engineering processes developing scripts to integrate and normalize third-party data using APIs as well as web scraping/crawling (beautifulsoup, scrapy, selenium, etc.)
Experience with most of the following technologies: Batch processing (Spark or MapReduce), Streaming processing (Spark-Streaming or other), Event Processing (Kafka, RabbitMQ, etc.) NoSQL Database -Elastic Search, MongoDB, Cassandra, etc.- Visualization Tool (Tableau, GCP Data Lab, etc.) Containerization (Docker), Micro Service architectures, Pipelining frameworks (Luigi and/or Airflow)
Strong knowledge of computer science fundamentals: data structures, algorithms, programming languages, distributed systems, and information retrieval
Experience writing automated unit and functional tests
Strong knowledge of UNIX environment including shell scripting
Experience with versioning tools (Git)
Experience working in an Agile development environment
Excellent communication skills"
Cloud Data Engineer,"Burnaby, BC",Best Buy,None,Organic,"Our Technology team has been on an incredible journey the last few years by creating first-to-market initiatives for our customers, adding the latest development languages to our tech stack and establishing Best Buy Canada as the best Omni-channel experience within the retail industry.
We believe empowered people and teams make smarter, faster, and more creative decisions, which is why we operate in a truly Agile environment where the distance between any one person and senior leadership is microscopic. Here, you’ll work on something big, small, or super cool and before you can blink 100,000 people will see it. You’ll create fast, learn fast, and develop fast! Oh, and sometimes you’ll fail fast too. That’s ok. (Honestly.) It’s all part of the process.
The Cloud Data Engineer reports to the Manager Data Platform Products and also works closely with the Manager Data Analytics and Manager (Web Analytics – Testing Platforms) as they provide guidance and vision towards developing, constructing and maintaining our enterprise data capabilities. You will work with large scale data processing system that collects data from a variety of structured and unstructured data sources, stores data in a scale-out data lake and prepare the data using ELT techniques in preparation for the data science data exploration and analytic modeling.
As a Cloud Data Engineer you will…
Harness, model and transform data (structured, unstructured) from several sources that empowers users to collaborate and analyze data in different ways leading to better and faster decision making.
Build, design and launch data pipelines, raw landing zones and data lakes that store, transform and move data.
Monitor data quality processes and compliance processes in accordance with industry and data governance COP best practices.
Collect, combine and integrate data from our omni and multi-channel footprints using our enterprise tech-stack that intuitively conveys insights to the business regarding data trends and consumer behavior.
Design, Build and write code for cloud-compatible CI/CD frameworks to deploy solutions on Cloud Data Platforms.
We hope you are passionate about…
Big Data and Data Labs – you creatively navigate through a complex network of data structures, pull relevant data, apply ML/AI and extract valuable insights.
Team Work – you work collaboratively with your team to connect the dots and provide valuable information to the business.
Relationship Building - you are a communicator, able to influence, gain buy-in and provide guidance to key stakeholders.
The experience we need…
5+ Years building, designing and implementing data pipelines.
5+ years of experience with Azure Data stack development (Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Factory, Databricks and Python)
2+ Years experience with Qlik Replicate, Azure Stream Analytics, Azure Data Factory and Informatica Power Center
Experience with MSSQL or Oracle database
Good understanding of a tech stack such as Microsoft (Azure Data Lake Storage, Azure Data Factory, Azure Streams, Databricks) or Google (BigQuery, Dataflow, Datafusion, Dataproc)
Bonus points…
Familiarity with agile/scrum methodologies
Experience designing and developing data applications for a retail environment
Bachelor’s Degree or Diploma in Computer, Science, Electrical, Engineering or related discipline/experience"
Senior Software Engineer - Data Platform Engineer,"Vancouver, BC",Prenuvo Inc,None,Organic,"Prenuvo Inc. (dba Voxelwise Imaging Technologies Inc.)

Overview
We are looking for a high-calibre, driven, technically-skilled software engineer to join what we expect to be the most rewarding role in your career. Prenuvo is literally transforming and reshaping preventative health by rolling out a data/ML heavy advanced MRI based screen that catches cancer at stage 1 and 400+ other diseases.
Every week in your job you will be saving lives as you help our team build out infrastructure to process medical images and ML/CV algorithms at scale, prepare and generate insights from data, and develop APIs for 3rd-party integration.
Why we need you
The passion and energy you bring to teams is infectious while you see challenges as opportunities and opportunities for personal, professional and team growth.
We look for a Bachelor’s Degree in Computer Science or a related field, equivalent experience, or evidence of exceptional ability to help elevate the team.
You’ve honed your skills over the last 4+ years solving technical challenges with your strong command of back-end tech stack (primarily python), strong proficiency in databases, and framework such as Flask/FastAPI.
You have production experience with ML pipelines, frameworks such as Airflow/Metaflow, and distributed event-based processing (Kafka or similar).
Your bias toward action has kept teams moving forward as has your ability to “GSD” without sacrificing quality by leveraging judgment to ship fast while building products in a sustainable, responsible way.

What’s in it for you
You’ll be part of a close-knit team that’s purpose is to solve complex problems in creative ways.
Work in an environment where it is safe to experiment, challenge yourself and others, fail, and learn.
Build systems and products that make a meaningful impact on the lives of people and their health.
Get a competitive salary with an equity stake
Extended health benefits including comprehensive body scans.

We welcome people from all backgrounds who seek the opportunity to help build a future where everyone can contribute and help bring amazing technology to millions of people. If you have the curiosity, passion, and collaborative spirit, work with us, and let's move the world forward, together.

The position is based out of Vancouver, BC. Most of the team is working remotely these days. Local candidates are strongly preferred.

Powered by JazzHR
DOPWMVHifo"
Data Engineer,"Toronto, ON","AMZN CAN Fulfillment Svcs, ULC",None,Organic,"Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline.
4+ years of industry experience in Software Development, Data Engineering, Business Intelligence, Data Science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
Hands-on experience and advanced knowledge of SQL.
Experience in Data Modeling, ETL Development, and Data Warehousing.
Experience using business intelligence reporting tools (Power BI, Tableau, Cognos, etc.).
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.).
Knowledge of Data Management fundamentals and Data Storage principles.
Experience coding and automating processes using Python or R.
Strong customer focus, ownership, urgency, and drive.
Excellent communication skills and the ability to work well in a team.
Effective analytical, troubleshooting, and problem-solving skills.

Come and be a part of Amazon's amazing growth story! Amazon.com's Forecasting group is searching for a Data Engineer to join our team. We are pioneers in the fields of demand analysis and forecasting. If you want to learn how to leverage technology to predict the future, this is the team for you.

As a member of the Demand Forecasting team, you'll play a key role in solving some of the world's most complex technical challenges associated with Forecasting. You will apply Large-scale computing, Distributed systems, Data mining, Scalability, Machine learning and Statistical Algorithms techniques - just to name a few.

Our Data Engineer needs to be able to gather and understand data requirements, present it to software engineers, and work in the team to achieve high quality data ingestion goals. You need a passion for complex problems, and enjoy the challenge of operating complex and mission critical systems under extreme loads. Do you think you are up to the challenge? Would you like to learn more and stretch your skills and career?
In this role, you will be a technical expert with significant scope and impact. You will work closely with a group of Software Development Engineers, Product Managers, Data Scientists, and Business Intelligence Engineers to create the data infrastructure and pipelines necessary to drive our team’s initiatives.

Successful candidates should come from a strong data engineering background. You need to have experience with structured data, and being able to analyze/transform the data using various tools. Although SQL is a strong requirement, being flexible enough to work in a scripting environment is a must. Often, the pace of innovation and change implies a need to move to new data sources, and our Data Engineers get to participate in deep diving business data in order to understand/measure sources of disparity. Your analytical skills and knowledge of schema metadata will be essential.

Masters in computer science, mathematics, statistics, economics, or other quantitative fields.
Experience working with AWS big data technologies (Redshift, S3, EMR, Glue).
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.
Experience providing technical leadership and educating other engineers for best practices on data engineering.
Background in Big Data, non-relational databases, Machine Learning and Data Mining is a plus."
Sr. Big Data Engineer (HBase),"Vancouver, BC",Capgemini,None,Organic,"Duration: FT
Requirement:

Purpose of Job:
Performs research, design, implementation and support tasks as a member of the team. Works in accordance with project guidelines, quality standards and code conventions.
Responsible for area/areas within the team area of responsibility (AOR). One of the current team AOR is migrating petabytes of data from HBase to Druid and building Data access services on top of Druid to be used by other applications of highly loaded social platform.
Investigate, create, and implement the solutions for existing technical challenges, including building/enhancing the frameworks and tools used by other development teams.

Responsibilities:
Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all stakeholders.
Ensures that assigned area/areas are delivered within set deadlines and required quality objectives.
Provides estimations, agrees task duration with the manager and contributes to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on his/her experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond his/her AOR.
Responsible for resolving crisis situations within his/her AOR.
Initiates and conducts code reviews, creates code standards, conventions and guidelines.
Suggests technical and functional improvements to add value to the product;
Constantly improves his/her professional level.
Supervises and coaches newcomers and more junior team members.
Collaborates with other teams.
If required, make yourselves available for the visits to the client location.

Must have:
University degree in Computer Related Sciences or similar
5+ years of commercial development experience including Java
Hands on experience with HBase
Experience of development of Distributed storage systems
Experience of work with Big Data technologies
Rigor in high code quality, automated testing, and other engineering best practices
Strong OOP skills
Strong communication, collaboration and interpersonal skills
Result oriented approach
Good English (oral & written) and communication skills in general

Would be a plus:
Development experience in Python

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
Principal Software Engineer - Cloud Data Collection,"Vancouver, BC",Splunk,None,Organic,"Are you interested in being part of a team building new and innovative enterprise and SAAS products integrating with cloud and cloud native technologies? Splunk is looking for a Software Engineer to be part of an agile team in building new, innovative and user-friendly ways of onboarding and ingesting cloud monitoring data. This team is a part of GDI (Getting Data In) group in Splunk, that has multiple teams working on the cutting edge of cloud integrations, streaming data ingest and cloud onboarding solutions that will provide the data for various Splunk products and solutions in the IT and Security monitoring, investigation and analytics space.
We give our engineers an environment in which they can contribute from day one while also providing opportunities for learning and growth. You’ll learn how our entire stack works – from data ingestion to search, building exciting end user experiences which will directly impact our customers.
Join us as we pursue our disruptive new vision to harness machine data in the next generation of IT and Security analytics products! We are a company filled with people who are passionate about our products and seek to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun and commemorate each other’s success.
What We’re Looking For:
You have experience building cloud native applications in an agile manner using DevOps concepts and principles.
Experience having lead, designed, implemented and released highly performant and scalable, distributed SaaS applications to a large customer base, successfully.
Have designed and developed APIs and micro services and released them using automated CI/CD pipelines.
Have distributed systems design experience and integrating multiple systems using enterprise integration patterns and standard methodologies.
You are proficient in coding in two or more of these languages: Java, Go OR Python.
Expertise on test-driven development, developing different levels of automated tests, such as unit test, functional test, integration test, system test, or performance / load test.
Experience with cloud technologies, such as AWS, Azure, or GCP. Ideally with certifications.
Experience collaborating with PM, QA and cross-functional teams, as well as coaching and mentoring junior engineers.
Experience with a messaging system, such as Apache Pulsar, Kafka or equivalent is a big plus.
Experience with a stream processing platform, such as Flink, Storm or equivalent is a big plus.
10+ years of experience and a MS in Computer Science preferred.
What We Offer You:
A constant stream of new things to learn. We’re always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.
A set of incredibly hardworking and dedicated peers, all the way from engineering and QA to product management and customer support.
Breadth and depth. You want to work on an area that spans backend and frontend? We have that. You want to go deep into optimizing how we gather data from disparate sources? We have that too, and more.
Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both sides of the equation.
A stable, collaborative and supportive work environment. We work in an open environment, have a shared kitchen and sit down for a quick sync every morning.
We don’t expect people to work 12-hour days. We want you to have a successful time outside of work too. We trust our colleagues to be responsible with their time and commitment and believe that balance helps cultivate a great environment.

We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying. For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran, status, or disability status"
Senior Data Quality Engineer,"Toronto, ON",Instacart,None,Organic,"OVERVIEW
Every few years, a company appears that transforms an industry. Instacart has the chance to be one of those companies, and you could be one of the early people that shape Instacart and help us change the way people shop.
In a few short years we've launched same-day delivery in nearly 100 major US markets, provided flexible work to tens of thousands of Personal Shoppers on our platform, and have sold and delivered more than $1B in grocery products.
The Instacart Product Team is dedicated to making grocery shopping effortless for everyone. By helping take care of the shopping, we help people get great food and give them back time so they can do what matters most to them. We're finding solutions to large-scale challenges that will forever change the way people feed themselves and their loved ones.
ABOUT THE JOB
The Catalog team is responsible for defining how products should be represented to customers. We spend a lot of time focusing on questions like - What do customers want to know when they buy meat for dinner? How do we help customers discover new products to love by knowing more about what's in the grocery store than they do? What data is needed to tailor our Catalog to a specific customer's preferences and dietary needs? You'll be joining a team of individuals that are passionate about answering these questions.
Our Catalog's data powers our customer experiences, our advertisements, our shopper app, and our retailer platform - as such, this role works closely with our product and engineering teams across our whole business. This role works with both the Catalog and other Instacart engineering teams to understand data quality issues and work with product and engineering to build systems to find and proactively prevent data issues in the future. You will be joining our rapidly growing Catalog team - a group responsible for managing over 5 billion data points (and growing) that represent assortments from retailers from across North America.
ABOUT YOU
A minimum of 5+ years working in software quality, test engineering, data analysis or relevant technical background
Ability to identify data and software quality issues and work with engineering teams to find and fix the root causes
Improve testing and validation infrastructure to proactively prevent future issues
Be a user advocate for our customers, retailers and shoppers
Strong data analysis skills using languages like SQL and Python
Ability to communicate with both technical and non-technical audiences"
Sr. Big Data Engineer (Java),"Vancouver, BC",Capgemini,None,Organic,"Duration: FT
Requirement:

Purpose of Job:
Performs research, design, implementation and support tasks as a member of the team. Works in accordance with project guidelines, quality standards and code conventions.
Responsible for area/areas within the team area of responsibility (AOR). One of the current team AOR is improving BigData Platform used by of the world's largest social media platform which deals with few petabytes of data coming to the system daily.
Investigate, create, and implement the solutions for existing technical challenges, including building/enhancing the frameworks and tools used by other development teams.

Responsibilities:
Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all stakeholders.
Ensures that assigned area/areas are delivered within set deadlines and required quality objectives.
Provides estimations, agrees task duration with the manager and contributes to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on his/her experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond his/her AOR.
Responsible for resolving crisis situations within his/her AOR.
Initiates and conducts code reviews, creates code standards, conventions and guidelines.
Suggests technical and functional improvements to add value to the product;
Constantly improves his/her professional level.
Collaborates with other teams.

Must have:
University degree in Computer Related Sciences or similar
5+ years of commercial development experience including Java
2+ years experience with SparkSQL and Hive including understanding on how these technologies work under the hood
Strong experience in distributed Big Data processing (batch/offline, Terabytes and more)
Rigor in high code quality, automated testing, and other engineering best practices
Strong OOP skills
Strong communication, collaboration and interpersonal skills
Result oriented approach
Good English (oral & written) and communication skills in general

Would be a plus:
Python development experience

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
Senior Software Development Engineer - Big Data,"Toronto, ON","AMZN CAN Fulfillment Svcs, ULC",None,Organic,"4+ years of professional software development experience
3+ years of programming experience with at least one modern language such as Java, C++, or C# including object-oriented design
2+ years of experience contributing to the architecture and design (architecture, design patterns, reliability and scaling) of new and current systems

Amazon Advertising operates at the intersection of eCommerce and advertising, offering a rich array of digital display advertising solutions with the goal of helping our customers find and discover anything they want to buy. We help advertisers reach Amazon customers on Amazon.com, across our other owned and operated sites, on other high quality sites across the web, and on millions of Kindles, tablets, and mobile devices. We start with the customer and work backwards in everything we do, including advertising. If you’re interested in joining a rapidly growing team working to build a unique, world-class advertising group with a relentless focus on the customer, you’ve come to the right place.

We are responsible for end to end processing of impressions, views, and clicks as well as video interaction events or rich media events in ads. We are applying the latest machine learning and big data technologies available on terabytes of data a day (over 30B new events per day) operating Petabyte size clusters. Our data must always be the fastest, most high fidelity data as it is both billable, critical to checking the heartbeat of a campaign and changes to made to the campaigns while in flight, and part of the measure of success for a campaign.
We are looking for an experienced software engineer that can combine open source technologies such as Hadoop, Hive, Spark and Presto, as well as AWS services like EMR, Redshift, Kinesis and DynamoDB to build the next generation of our Traffic Ingestion and Application platform. You will be responsible for designing and developing software products that will provide measurement and reporting to a wide set of users across all of Amazon's advertising suite across display, search, native, and video on all devices.
Candidates for this position should have strong software engineering fundamentals as well as real-world experience. You will be able to demonstrate a variety of architectural approaches and design patterns and have a demonstrated competence in designing maintainable and scalable software written in a high-level language. You will show your ability to adapt to changing technical environments and devise creative solutions to vexing software problems. Candidates must have the ability to communicate effectively, both in writing and orally, to engineers and executives.

Join the fast growing Amazon Ads business! Amazon is leveraging its highly unique data to change the way marketers purchase, track, measure, and optimize their advertising spend. You will encounter some of the toughest and most inspiring technical challenges of your career as you build petabyte-scale services, invent new big data paradigms, and scale for extreme growth.

Master's degree in Computer Science
Experience in big data technologies (Hadoop, Hbase, Pig, Spark).
Experience building complex software systems that have been successfully delivered to customers
Knowledge of professional software engineering practices & best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Ability to take a project from scoping requirements through actual launch of the project
Experience in communicating with users, other technical teams, and management to collect requirements, describe software product features, and technical designs.
Experience improving web latency in complex large scale deployments.
Experience in databases, analytics, big data systems or business intelligence products
Experience mentoring and training other engineers
Amazon is an Equal Opportunity-Affirmative Action Employer - Minority/Female/Disability/Veteran/Gender Identity/Sexual Orientation

Amazon is committed to providing employment accommodation in accordance with the Ontario Human Rights Code and the Accessibility for Ontarians with Disabilities Act. If contacted for an employment opportunity, please advise Human Resources if you require accommodation.

#YYZBDSDE3
#YYZBDSDE3ADS

#PASDE3"
Senior Big Data Engineer,"Montréal, QC",DMD Marketing,None,Organic,"The Senior Big Data Developer will join the Data Lake team with a focus on designing a comprehensive data model and developing the pipelines to support the DMD’s growing business and enable it to scale its operations.
A successful candidate must be a self-starter, fast learner and able to be part of a cross-functional, SCRUM software development team. The candidate is a strong technology professional and software developer with significant experience with big data technologies and must also possess excellent analytical skills, with a focus on detail, quality, and accuracy. The position will include working with distributed business and technical resources, as well as external vendors to implement the components of the architecture.
Essential Job Responsibilities
Ability to grasp complex problems and develop a simple yet comprehensive solution that meets or exceeds the client’s expectations.
Ability to explain difficult or complex problems and solutions to varying audiences which allows them to understand and works to build consensus.
Proactively shares experiences and knowledge to help the team improve its development processes and practices.
Actively seeks challenging assignments, is genuinely excited by a challenge
Collaboration with Product Owner, Scrum Master, DevOps, developers and testers in an Agile team environment
Develop deep expertise in the product and be passionate about creating the absolute best customer experience for the product
Using first-hand customer information to improve products and services according to technical and product roadmap
Being flexible and responsive to the client’s and products changing needs
Continually evaluates emerging technologies to identify opportunities, trends and best practices
Required Skills & Experience
3+ years of development experience with big data technologies
Strong experience building data pipeline using the Hadoop ecosystem (HDFS, Hive & Spark)
Advanced knowledge in programming languages such as Scala or Java
Strong analytic skills related to working with unstructured datasets
Exposure to Cloud Service Providers (AWS)
Experience with Software Development Life Cycle
Eager to learn, adapt and perfect your work; you seek out help and put it to good use
Experience in designing and building large scale enterprise data solutions
Creative, attention to detail, multitasking and organizational capacity
Impeccable written and verbal communication skills.
Strong technical, process and problem-solving proficiency
Demonstrated business acumen, and cross-collaboration capabilities
Ability to set and deliver on priorities and deal with a degree of ambiguity
Exceptional interpersonal and communications capabilities
Display organizational and emotional intelligence
Able to hit the ground running with a can-do attitude
Additional Role Desirables
Big Data Certification(s)
Certified Scrum Master or Certified Product Owner
Experience with JIRA and Confluence Team Collaboration Software
Exposure to Medical / Pharmaceutical digital marketing industry
DMD Marketing is committed to creating a workplace where inclusion is not only valued, but prioritized. We’re proud to be an equal opportunity employer, seeking to create a welcoming and diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, veteran status, or any other non-merit based or legally protected grounds.
DMD Marketing is committed to providing reasonable accommodations to qualified individuals with disabilities in the employment application process. To request an accommodation, please email hr@dmdconnects.com at least one week in advance of your interview."
Sr. Big Data Engineer (Apache Druid),"Vancouver, BC",Capgemini,None,Organic,"Duration: FT
Requirement:

Purpose of Job:
Performs research, design, implementation and support tasks as a member of the team. Works in accordance with project guidelines, quality standards and code conventions.
Responsible for area/areas within the team area of responsibility (AOR). One of the current team AOR is migrating petabytes of data from HBase to Druid and building Data access services on top of Druid to be used by other applications of highly loaded social platform.
Investigate, create, and implement the solutions for existing technical challenges, including building/enhancing the frameworks and tools used by other development teams.

Responsibilities:
Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all stakeholders.
Ensures that assigned area/areas are delivered within set deadlines and required quality objectives.
Provides estimations, agrees task duration with the manager and contributes to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on his/her experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond his/her AOR.
Responsible for resolving crisis situations within his/her AOR.
Initiates and conducts code reviews, creates code standards, conventions and guidelines.
Suggests technical and functional improvements to add value to the product;
Constantly improves his/her professional level.
Supervises and coaches newcomers and more junior team members.
Collaborates with other teams.
If required, make yourselves available for the visits to the client location.
Must have:
University degree in Computer Related Sciences or similar
5+ years of commercial development experience including Java
Hands on experience with Apache Druid
Experience of work with Big Data technologies like Hive, Fllnk, HBase
Rigor in high code quality, automated testing, and other engineering best practices
Strong OOP skills
Strong communication, collaboration and interpersonal skills
Result oriented approach
Good English (oral & written) and communication skills in general

Would be a plus:
Development experience in Python

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
Data Engineer,"Montréal, QC",Hivestack,None,Organic,"We're Hivestack, a global, full stack, marketing technology company that powers the buy and sell-side of programmatic digital out-of-home (DOOH) advertising.
Reporting to the Director of Engineering, we are looking for a talented Data Engineer to join our Data team. We have a wide variety of complex problems related to building audience concentration for all our screens as well as advanced analytics on the bidding decision process.
How You'll Make an Impact:
You will deliver code that is clear and tested.
You will own the complete lifecycle from analysis to production deployment
You will mentor others of best practices regarding data processing and analytics.
What You are looking for :
You are passionate about a culture of learning and teaching. You love challenging yourself to constantly improve, and sharing your knowledge to empower others
You like to take risks when looking for novel solutions to complex problems. If faced with roadblocks, you continue to reach higher to make greatness happen
You care about solving big, systemic problems. You look beyond the surface to understand root causes so that you can build long-term solutions
What We'll Bring to the Table:
An opportunity to grow with a dynamic company with personal and professional growth
Small collaborative teams where you can impact both product and culture
Work for a growing company located in the thriving tech hub of Montreal
A great benefit package and a team that focuses on your well-being
What You'll Bring to the Table:
2+ years of experience in a related field
Have a Bachelor's Degree in Computer Science, Computer Engineering or equivalent
Excellent Computer Science fundamentals with regards to data structures, algorithms, time complexity, etc.
Thorough understanding and work experience with Python, Java / Scala
Strong experience in Spark
Experience with data analytics in Python (pandas, scikit, etc)
Strong understanding of the AWS cloud platform and its related services, such as EMR, RDS, Kinesis Streams
Strong database skills and experience working with large data sets
Ability to work independently and make use of your time effectively
Experience practicing advanced optimization techniques
Want to know more about our techstack, have a look at this link: https://stackshare.io/hivestack/hivestack
Feel this is the right opportunity for you? We would love to hear from you"
Data Engineer,"Vancouver, BC",Intersog,None,Organic,"Intersog® is a Chicago-based provider of ROI-driven custom web and mobile development specializing in the delivery of full-service, end-to-end solutions, and project resources to Fortune 500 companies, SMEs and startups. We help our clients attack their ambitious business goals, solve skills shortage issues and become innovative by building Dedicated Software Development Teams in Vancouver, BC, Canada, and/or providing on-demand IT project resources to complete required skills on their in-house teams.
Our primary goal in partnering with our clients is to exceed their expectations and foster an ongoing relationship that envelopes innovation, industry leadership and business strategy while delivering products that bring exceptional user experience, brand elevation and market dominance.
As a Data Engineer you will be responsible for expanding and optimizing our data and data pipeline architecture of a e-health platform, as well as optimizing data flow and collection for cross functional teams. As the ideal candidate you are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will collaborate with and support other engineers and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout the system. You are comfortable supporting the data needs of multiple teams, systems and products. As the right candidate you will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
What you’ll do
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Enhance the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Amazon Redshift, Airflow, and other ‘big data’ technologies.
Help building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
What we’re looking for
5+ years of industry experience in data engineering in a fast-paced environment, ideally a high growth company
Strong applied analytics - you love data, gleaning insights from data, and making decisions based on data
Advanced working experience with MySQL, PostgreSQL (or similar)
Advanced experience working with Python
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores like Kafka.
Excellent communication and presentation skills - effectively presenting insights is as important as the insights themselves
Experience working cross-functionally, particularly with engineering, product and growth teams
Team-player: we rely on each other to be candid, supportive, hardworking and genuine
We offer
Competitive compensation based on your skills, experience, and customer satisfaction;
Opportunity to work on challenging and exciting international projects;
Flexible working hours and the possibility to work remotely when needed;
Extended medical and life insurance;
Casual, friendly and family work environment;"
Data Engineer,"Toronto, ON",OANDA,None,Organic,"We have come a long way since our first currency feed 23 years ago. We are an award-winning global company offering leading currency solutions for both retail and corporate clients, from a tech start-up to a global corporation. Founded in 1996, we became the first company to share exchange rate information on the internet free of charge and in 2001, we launched a trading platform that helped pioneer the development of online-based trading around the world, enabling forex and CFD investors the ability to trade the financial markets. Our vision is to transform how our clients can meet all of their currency needs with innovative and award-winning solutions. Under new ownership with significant ambitions to grow the business on the global stage, we are looking for highly motivated, passionate individuals who want to make a mark in a dynamic environment.
We are looking for an exceptional data engineer to help us build our next generation data platform on top of Google Cloud, driving its growth and integration within the company. This platform will support our internal analytics and personalization needs, while enabling future business opportunities.

This role may be for you if :
You view the present world as a pool of data and believe it is your job to create the tools and platforms needed to process it all.
You believe in a future where every business is data driven, making insightful decisions based on the data available, rather than being lost and dragged down by it.
When you hear the words ETL you are curious about the past we are living in since you process everything in real-time, using the latest technology.
You wonder if you will ever use the challenging algorithms you've learned in your Computer Science background on a day to day basis, or if it was just all for theory.
You thrive in a fast-paced, highly collaborative team of 3 to 5 engineers.

During a typical week you might :
Build a data pipelines using DataFlow.
Explore a new managed service for better perfomance and simplification of a data pipeline.
Participate in meetings to collect reporting requirements for an application that will need to access information from the Cloud externally.
Suggest a design to better ogranize the tables inside the data warehouse.
Develop a data quality validation framework used for testing and real time production data.
Discuss the computationally expensive problems raised by the data science team.
Design tools enabling easy-to-use workflows for internal teams using the data platform

We are looking for :
5+ years of experience designing and implementing large scale software.
2+ years of experience working with Big Data technologies like DataFlow, BigQuery, Kafka, Airflow and Spark.
Experience designing and deploying a real-time Big Data platform.
Experience with data warehouse design.
Strong coding ability in an object oriented language (preferably Scala, Python, Java).
Excellent team player with strong communication skills (verbal and written).
Enthusiastic about collaborative problem solving.
Bachelor’s degree or better in Computer Science.

Extra points if you have :
Experience with Streamsets.
Using Looker, Fivetran or Mulesoft.
Google BigQuery.
Apache NiFi.
Strong SQL and data modelling skills.
Experience with other pipeline monitoring tools.

OANDA Global Corporation is a diverse and global team with offices around the world. We value the unique skills and experiences each individual brings to OANDA. We are committed to creating and sustaining a collegial work environment in which all individuals are treated with dignity and respect and one which reflects the diversity of the community in which we operate. We provide an inclusive and accessible environment for everyone. Candidates selected for an interview will be contacted directly. If you require accommodation during the recruitment and selection process, please let us know. We will work with you to provide as seamless a recruitment experience as possible."
Software Engineer - Data Backend (Intent Understanding & Rec...,Remote,Yelp,None,Organic,"On the Intent Understanding & Recommendation team, our vision is for all of Yelp to recognize each user’s unique needs and interests. Our platform enables personalization and contextualization of features across all of Yelp’s apps and websites. To do this, first our backend systems recommend relevant searches, filters, businesses, and places in the world, making it easy for users to specify what they want. Then, when users interact with these recommendations or anything else, we process their actions into semantic representations, for example mapping the text they’ve searched for to the most relevant business categories. Finally, we share this enriched data with a variety of other backend systems, enabling the features they power to perfectly match what the user wants. We’re scaling these systems into a platform that benefits more and more aspects of Yelp every day.

Are you intrigued by the challenge of building real-time recommender systems at scale? Do you want to work with diverse data types including text, geospatial, and concept taxonomies? And are you excited to impact both Yelp users and company-wide internal clients of a growing platform? If this sounds like you, join the team that makes tens of millions of people each month think, “Wow, Yelp understands exactly what I’m looking for!”
Where You Come In
Design, build, and deploy systems that run 24/7 at ever increasing scale
Collaborate with other engineering teams across
Yelp, bringing them onto our platform and enabling them to build features that deeply understand user intent
Create new ways of delighting users, from idea brainstorming to experiment analysis (and, of course, all the backend development and data mining in between!)

What it Takes to Succeed
Based in Canada
Prior hands-on experience in internet-scale systems design, and an understanding of operational and reliability trade-offs
An ability to translate ambiguous data from our users and clients into actionable engineering milestones
A craving for new knowledge and challenges
Experience in Java and/or Python
BA/BS degree or higher in Computer Science, Math, or related field

At Yelp, we believe that diversity is an expression of all the unique characteristics that make us human: race, age, sexual orientation, gender identity, religion, disability, and education — and those are just a few. We recognize that diverse backgrounds and perspectives strengthen our teams and our product. The foundation of our diversity efforts are closely tied to our core values, which include “Playing Well With Others” and “Authenticity.”

We’re proud to be an equal opportunity employer and consider qualified applicants without regard to race, color, religion, sex, national origin, ancestry, age, genetic information, sexual orientation, gender identity, marital or family status, veteran status, medical condition or disability.

We are committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-recruiting@yelp.com or 415-969-8488.

Note: Yelp does not accept agency resumes. Please do not forward resumes to any recruiting alias or employee. Yelp is not responsible for any fees related to unsolicited resumes."
"Senior Associate, Financial Advisory, M&A Advisory, Analytic...","Toronto, ON",Deloitte,None,Organic,"Job Type: Permanent
Primary Location: Toronto, Ontario, Canada
All Available Locations: Toronto

Partner with clients to solve their most complex problems
Be expected to share your ideas and to make them a reality.
Play a leading role in solving complex finance business challenges through data exploration and analytics


Our Mission is to Lead the Canadian market in delivering data driven insights and analytic solutions to help clients increase stakeholder value in their defining moments.

Are you a team-player who likes to collaborate and contribute to the mission and success of a broader like-minded team? Is the prospect of furthering a culture of inspiring and empowering our people, fixating on client value, and innovating toward a data/etch enable future exciting for you? Then this job is perfect for you!
What will your typical day look like?


As a Senior Associate you will work in a fast growing and challenging environment with like-minded people who are eminent in their respective technical fields such as Data Mining, Machine Learning, Data Visualization, and much more. You will experience the growth of a practice totally dedicated to the art and science of using data to help clients drive business decisions related to mergers and acquisitions as well as value creation / business improvement initiatives. You will continue your professional development to expand your chosen career path while working with high profile clients who need analytics support for their top priority initiatives and strategies.
You will develop advanced analytical solutions within our analytics team, across service lines at Deloitte, and with clients in multiple industries.
About the team

Deloitte’s Analytic Insights team transforms data into actionable insights - to support strategic decisions that amplify stakeholder value. The team focuses on three business areas to drive value for clients: i) Leverage analytics to generate deeper data-driven insights and optimize decisions across the M&A cycle, ii) Analytics for operational efficiency, to drive productivity and cost control, and iii) Advanced analytics to identify opportunities for profitable revenue growth. Our team is comprised of a mix of deep technical expertise in the statistical and analytics fields and business professionals with the experience to leverage insights to drive business decisions.
Enough about us, let’s talk about you

Completed undergraduate degree in a quantitative discipline (Math, Commerce, Economics, Data Science, Business, Computer Science, Engineering, etc);
2 + years of work experience in an analytical occupation (Business Analysis, Data Analytics, Data Engineering, Data Science, etc.) developing reporting and analytic solutions;
Sound understanding of data architecture and advanced SQL knowledge with an expertise in creating and maintaining data pipeline, integrating large complex data sets from various sources to meet business requirements
Superior verbal and written communication skills with experience communicating the results of your analysis to business executives;
Objective, flexible and curious - focused on solving business problems and delivering data-driven analytical solutions on our client’s objectives.

Why Deloitte?
Launch your career with The One Firm where you can make an impact that matters in a way that you never thought possible. With endless opportunities at every turn, and a culture built to support and develop our people to be the very best they can be, Deloitte is The One Firm for you to learn, grow, create, connect, and lead. We do this by making three commitments to you:
You will lead at every level: We grow the world’s best leaders so you can achieve the impact you seek, faster.
You can work your way: We give you the means to be flexible in how you need and want to work, and we have innovative spaces, arrangements and the mindset to help you be wildly successful.
You will feel included and inspired: We create a deep sense of belonging where you can bring your whole self to work.

The next step is yours
Sound like The One Firm. For You?
At Deloitte we are all about doing business inclusively – that starts with having diverse colleagues of all abilities! We encourage you to connect with us at accessiblecareers@deloitte.ca if you require an accommodation in the recruitment process, or need this job posting in an alternative format. We’d love to hear from you!
By applying to this job you will be assessed against the Deloitte Global Talent Standards. We’ve designed these standards to provide our clients with a consistent and exceptional Deloitte experience globally."
"Data Engineer, Game","Oakville, ON",Prodigy Game,None,Organic,"Prodigy Education connects students, parents, teachers and school districts with resources that promote a lifelong love of learning. Anyone with an internet connection is welcome to create a free account and try the most effective and engaging K-8 math platform in the world. Our business model helps us connect with youth around the world, with over 100 million students now voluntarily practicing math every single day — and enjoying it!

At Prodigy Education, we have an incredible team that works tirelessly to turn our goal of making education a human right into a reality. We are immensely grateful for our amazing team and all they do every day. We welcome people who share our passion to join our team, where you will have the opportunity to help an entire generation of students to LOVE learning.

Our passion is our mission - to help every student in the world love learning!

Please note: During the Covid-19 pandemic, in order to keep all our candidates safe, Prodigy is hiring and on-boarding 100% remotely for the time being.

Overview:
Here at Prodigy, we are working hard to achieve our mission of helping every child in the world to LOVE learning. Our Data team is scaling rapidly as we continue to hit our product and growth milestones, and it’s an exciting time for the company! Our data team is responsible for all aspects of data ingestion, storage, transformation, and analyzation using modern tools and environments such as Spark, Airflow, Snowflake, Periscope, Kinesis, AWS. You will be working alongside our development and data science teams to help build and manage all of our data pipelines.
Your Impact:
Create and manage key business data pipelines
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data acquisition, data-related technical issues and other analytics needs.
Work cross-functionally to explore and propose solutions to business problems that can be addressed using insights from data
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Support the development of new solutions for batch, real-time data, and analytics use cases that align with business requirements
Maintain and troubleshoot the infrastructure built for optimal extraction, transformation, and loading of data from a wide variety of data sources
Identify, design, and implement process improvements: automate manual processes, optimize data delivery, improve data reliability, efficiency, and quality, etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into student learning, customer acquisition, operational efficiency, and other key metrics.
Who You Are:
Working familiarity with a variety of different storage mechanisms including SQL & NoSQL databases, Data Warehouses, and Data Lakes.
Experience working with AWS Cloud platforms and related systems
Experience building and optimizing data pipelines, architectures, and data sets.
Experience with big data tools: Databricks, Spark, Kafka, etc.
Experience with data pipeline and workflow management tools, such as Airflow
Experience with real-time data processing and stream-processing systems: Kinesis, Spark-Streaming, etc.
Experience in requirements analysis, design, implementation, and testing of software solutions, especially data related, using Python, Scala, and/or other programming languages
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience implementing backend Data APIs using NodeJS, Python, or other programming languages
Strong project management, organizational and communication skills.
University degree in Engineering, Computer Science, Stats, Mathematics. Graduate degree in Data Science related discipline is a strong asset
Our Core Technologies:
Data tools: Kinesis, Lambda, Kubernetes, ECS
Storage: S3, Snowflake, Delta, Postgres Aurora, DynamoDB
Coding World & Libraries: Python, Javascript (Node.JS), Spark
Platforms: AWS, Persicope, Databricks.
What We Offer:
The opportunity to build a career of value and witness first hand the impact Prodigy has as the most engaging math platform in the world!
A culture of transparency, where team members are involved in important conversations
No micromanaging here! We believe in our employees skills and abilities, we encourage you to bring new innovative ideas to your team
Full health benefits from day one (1) for you and your family, fully covered! Nothing is ever taken off your pay cheque
We are a profitable company, with eligibility to participate in stock options for all employees
Late or early riser? We understand! We offer flexible working hours that allow you to schedule your 8 hour day with a bit more flexibility. We do have core office hours, to ensure team members can be present for important meetings and department needs. Our core office hours are 10:30am - 3:00pm

While we operate 100% remotely for the time being, our company culture and values remain at the core of everything we do. We are offering a number of remote-friendly activities such as:
Virtual trivia
Zoom meditation, stretching, and fitness sessions
Daily remote challenges via Slack
Virtual “Prodigy Prepares” cooking series featuring our very own employees!
Remote work employee budget for things like home office equipment etc.

When this pandemic comes to an end and it is safe to be back in the office. Here's a glimpse into what you can expect:
Feel like a different work environment? Work from home 1x a week!
Team building events that not only include you as a Prodigy employee, but your significant other and children as well
Company pizza lunch every second week for ‘All Hands’, where we discuss important Prodigy milestones
Do you need some fun to help break up your day? We have that covered! Join in some Ping Pong games, Smash Bros competitions or board games!
Come as you are

We believe the power of our collective potential will transform education. We are building towards a diverse, inclusive, and equitable workplace to empower and create access and opportunity for all. We welcome applications from people from all underrepresented groups, including (but not limited to) people of any gender, age, or religion, members of the LGBTQIA2+ community, BIPOC and other underrepresented races and nationalities, people with disabilities, veterans, and anyone who may contribute to the further diversification of Prodigy Education.
If you feel like you don’t have all the qualifications for this position, and are willing to use your initiative to learn the rest, we’d still love for you to apply!

We are an equal opportunity employer and are committed to providing employment accommodation in accordance with the Ontario Human Rights Code and the Accessibility for Ontarians with Disabilities Act, 2005 (AODA). Prodigy Game will provide accommodations to job applicants with disabilities throughout the recruitment process. If you require an accommodation, please notify us and we will work with you to meet your needs."
Backend Data Engineer (Machine Learning),Remote,REPLICANT,None,Organic,"About Replicant
Replicant is a Conversational AI technology that works out of the box to solve customer problems over the phone. We craft great conversations by combining Machine Learning, Artificial Intelligence, and linguistic conversational design into the fastest, smartest, and most expressive Thinking Machines you’ve ever spoken with. Our platform lets companies offer high-touch customer service without hiring, offshoring, or adding complex technical infrastructure to your call center. Replicant’s autonomous contact center completely handles many call flows so agents can focus on what matters to customers, rather than answering mundane questions. Regardless if you're an expert in customer service or not, we're building something much bigger here at Replicant by redefining human to machine conversations for the enterprise. We're a small team (~18 people) tackling a big industry with many eyes on it, using powerful technology. Our team comes from a diverse background of industry and the arts. Our stack includes TypeScript, JavaScript, Python (3.x), and Pytorch.
About the Role
Replicant is an ecosystem for crafting conversations, with in-house deep-learning algorithms we’ve built from scratch that allow our Thinking Machines to listen, think, and interact with the world. We have also built a suite of tools for conversation design and development, monitoring, testing, continuous learning, and constant improvement.

That's where you, dear candidate, come in. We’re looking for a Data Engineer with an ability to dive into backend systems, implement clean communication between various theoretical and applied systems, solve tricky problems, and help innovate on our cutting-edge voice technologies. This person will work closely with the ML team to productionize models, collect and route data properly, and improve our ML infrastructure

#LI-Remote
What You'll Do

Ownership: We know that we don't know everything and will need you to help us find the best answers. You’ll help define:
The application of cutting-edge deep-learning models to real-world problems.
Management and analysis of historical data including call audio, metadata, transcripts, and supervised/unsupervised feedback on semantic understanding.
Processes around the low-level details and the big picture of how Replicant’s systems fit together. This includes not just the data pipelines but how to mesh the real-time processing that happens during a call, the internal and external tools that are used to manually review and correct the data, and feedback from various automated tests.
What You'll Bring
You have experience with Node/Typescript & Python.
You like to build scalable software but also like to understand how it is used.
You have a keen eye for data flow and building understandable, testable systems.
You have excellent communication skills and a vivid imagination.
You have a penchant for numbers or are interested in analytics.
You are an independent thinker and like to own and solve complex problems.
You are interested in exploring the nuance and aesthetic of conversations.
Why Terminal?
At Terminal, we identify emerging tech hubs around the globe, and connect the top engineers with the most compelling companies. We provide complete operations and services to give companies all the benefits of a new office without any of the hassle. We are focused on building a diverse and inclusive workforce. Terminal is an Equal Opportunity Employer and considers applicants for employment without regard to race, colour, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, provincial, or local law."
Senior Data Engineer,"Kitchener, ON",Faire,None,Organic,"Faire is an online wholesale marketplace built on the belief that the future is local — there are over 1M independent retailers in the U.S. and Canada doing more than twice the revenue of Walmart and Amazon combined. At Faire, we're using the power of tech, data, and machine learning to connect a thriving community of over 100,000 brands and local retailers around the world. Picture your favorite boutique in town — we help them discover the best products to sell in their stores. With the right tools and insights, we believe that we can level the playing field so that small businesses can compete with these big box and ecommerce giants. We're looking for smart, resourceful and passionate people to join us as we power the shop local movement.
Job Description
The Data Engineering team is the backbone of all data-related processes and enables the Data Science team to develop and deploy a wide variety of algorithms and models that power the marketplace. Our infrastructure is used by the whole company for analytics, reporting, forecasting and research. We care about having a reliable infrastructure with quality data and building machine learning models that help our customers thrive.
As a Senior Data Engineer you'll be responsible for developing and automating large scale, high-performance data storage and processing systems.
Our team already includes experienced Data Scientists and Engineers from Airbnb, Facebook, Quora, Square, Uber, TripAdvisor, and Overstock. Faire will soon be known as a top destination for data science and machine learning, and you will help take us there!
What you will be doing:
Develop our machine learning infrastructure to help us scale for where we're going over the next several years
Manage our data infrastructure and ETL platform
What it takes:
4+ years experience in a Data Engineering role
Strong skills in Python, Git, Docker, SQL, Airflow, ETL pipelines
Familiarity with at least one of: Hive, Presto, Snowflake, AWS Redshift, BigQuery,
A passion for programming and solving problems with code
A bachelor's degree in Computer Science/Software Engineering or equivalent industry experience
A love for technology, and an insatiable curiosity for new tools to tackle real problems
Why you'll love working at Faire:
We are entrepreneurs: We believe entrepreneurship is a calling and our mission is to empower entrepreneurs to chase their dreams. Every member of our team is an owner of the business and taking part in the founding process.
We are using tech and machine learning to level the playing field: We are using the power of technology and data to connect brands and boutiques from all over the world, building a thriving community of over 100,000 small business owners.
We build products our customers love: Everything we do is ultimately in the service of helping our customers grow their business because our goal is to grow the pie - not steal a piece from it. Running a small business is hard work, but using Faire makes it easy.
We are curious and resourceful: We always find a way to get the job done and come up with creative solutions to whatever problems are standing in our way. People at Faire are insatiably curious. We lead with curiosity and data in our decision making and reason from a first principles mindset.

Faire was founded in 2017 by a team of early product and engineering leads from Square. We're backed by some of the top investors in retail and tech including: Y Combinator, Lightspeed Venture Partners, Forerunner Ventures, Khosla Ventures, Sequoia Capital, Founders Fund, and DST Global. We have offices in San Francisco, Kitchener-Waterloo, and Salt Lake City. We're looking for smart, resourceful and passionate people to join us as we power the shop local movement. To learn more about Faire and our customers, you can read more on our blog.
Faire is being built for entrepreneurs, by entrepreneurs.
Additional Information
Faire provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity or gender expression."
Data Engineer,"Toronto, ON",WAYPOINT,None,Organic,"About the Role
The software engineers on the data engineering team help build and maintain the transformation and cleaning steps of our ETL (Extract, Transform, Load) pipeline before it can be stored and accessed by our customers in a standardized fashion. As a software engineer on this team, you’ll be building components within the ETL pipeline that automate these cleaning and transformation steps. You’ll also assist the rest of the team to execute the manual steps to load the data of our clients.

Our Data Engineering team primarily works with Python, PHP, SQL and JavaScript. The pipeline leverages Snowflake, Airflow, DBT and Docker. More than checking the boxes on specific technologies, we're looking for driven engineers with real technical depth and a desire to deliver end products that deliver joy to users. We want people who are passionate and care deeply about the success of the whole company.
What You'll Do
Write code and design pipeline architecture.
Build pipelines that support the ingestion, analysis, and enrichment of financial data.
Improve the existing pipeline to increase the throughput and accuracy of data.
Identify areas of automation opportunities and implement improvements.
Work with the Product Manager to map general accounting concepts to our internal data models in a repeatable and precise fashion.
Understand data models and schemas, and work with other engineering staff to recommend extensions and changes.
Use investigative tools and database queries to automatically flag irreconcilable data within our production datasets.
Proven ability to collaborate with and mentor other engineers
What You'll Bring
A computer science degree or equivalent experience
5+ years of relevant experience
Proficient in at least one programming language such as PHP, Python or Ruby
Proficient with SQL and database concepts
Understanding of data structures and algorithms
Interest in data modeling, visualization, and ETL pipelines
Kind and intellectually mature
Comfortable with ambiguity; you enjoy figuring out what needs to be done—and then doing it
Experience with real estate (MRI, Yardi, RealPage) or accounting systems (SAP, Lawson) is encouraged but not necessary
Why Terminal?
At Terminal, we identify emerging tech hubs around the globe, and connect the top engineers with the most compelling companies. We provide complete operations and services to give companies all the benefits of a new office without any of the hassle. We are focused on building a diverse and inclusive workforce. Terminal is an Equal Opportunity Employer and considers applicants for employment without regard to race, colour, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, provincial, or local law."
Data Engineer,Remote,Rackspace,None,Organic,"Our Data Engineers are experienced technologists with technical depth and breadth, along with strong interpersonal skills. In this role, you will work directly with customers and our team to help enable innovation through continuous, hands-on, deployment across technology stacks. You will work to build data pipelines be involved in the complete end-to-end Data Engineering efforts, including code development, integration, troubleshooting and testing.
If you get a thrill working with cutting-edge technology and love to help solve customers’ problems, we’d love to hear from you. It’s time to rethink the possible. Are you ready?
What You’ll Be Doing:
Build complex ETL code
Work on Data and Analytics Tools in the Cloud
Develop code using Python, Scala, R languages
Work with technologies such as Spark, Hadoop, Kafka, etc.
Build complex Data Engineering workflows
Create complex data solutions and build data pipelines
Establish credibility and build impactful relationships with our customers to enable them to be cloud advocates
Capture and share industry best practices amongst the community
Attend and present valuable information at Industry Events
Drive the engagements with customers from the architectural pillar, from design to delivery, create runbooks etc.
Qualifications & Experience:
15+ Years of Data-warehouse and Analytic system development and deployment experience
10+ years of experience in database architectures and data pipeline development
8+ years of experience in modern data ware housing platform using cloud native technologies
5+ years of experience in delivering Azure/GCP/AWS Data Solutions.
Demonstrated knowledge of software development tools and methodologies
Presentation skills with a high degree of comfort speaking with executives, IT management, and developers
Excellent communication skills with an ability to right level conversations
Demonstrated ability to adapt to new technologies and learn quickly
Experience with Google Cloud Services such as Streaming + Batch, BigQuery, BigTable, DataStudio, DataPrep, Pub/Sub , Cloud Storage, Cloud Dataflow, Data Proc, DataFlow, DFunc, Big Query & Big Table
knowledge and proven use of contemporary data mining, cloud computing and data management tools including but not limited to Microsoft Azure, AWS Cloud, Google Cloud, hadoop, HDFS, MapR and spark.
Design and configuration of data movement, streaming and transformation (ETL) technologies such as Informatica, Nifi, Kafka, Storm, Sqoop, SSIS, Alteryx, Pentaho, Alooma, Airflow.
Creation of descriptive, predictive and prescriptive analytics solutions using Azure Stream Analytics, Azure Analysis Services, Data Lake Analytics, HDInsight, HDP, Spark, Databricks, MapReduce, Pig, Hive, Tez, SSAS.
Design and configuration of data movement, streaming and transformation (ETL) technologies such as Azure Data Factory, HDF, Nifi, Kafka, Storm, Sqoop, SSIS, LogicApps, Signiant, Aspera, Alteryx, Pentaho, Alooma, Airflow.
Large scale design, implementation and operations of OLTP, OLAP, DW and NoSQL data storage technologies such as SQL Server, Azure SQL, Azure SQL DW, PostgreSQL, CosmosDB, RedisCache, Azure Data Lake Store, Hadoop, Hive, MySQL, Neo4j, Cassandra, HBase
Experience working within an agile development process (Scrum, Kanban, etc)
Expertise in data estate workloads like HDInsight, Hadoop, Cloudera, Spark, Python.
Familiarity with CI/CD concepts
Strong verbal and written communications skills are a must, as well as the ability to work effectively across internal and external organizations and virtual teams.
Knowledge or hands-on experience with data visualization and/or data sciences.
Must have's:
Hands on experience with Azure/GCP projects.
Cloud certifications such as GCP Professional Data Engineer or Microsoft Data / AI certifications.
Technical degree required; Computer Science or Math background desired
Location:
This is a virtual role
The candidate needs to be based in US or Canada
#LI-Remote

About Rackspace Technology
We are the multicloud solutions experts. We combine our expertise with the world’s leading technologies — across applications, data and security — to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.

More on Rackspace Technology
Though we’re all different, Rackers thrive through our connection to a central goal: to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know."
Software Engineer Web/Data,"Vancouver, BC",Arista Networks,None,Organic,"Company Description
Arista Networks was founded to pioneer and deliver software driven cloud networking solutions for large datacenter storage and computing environments. Arista’s award-winning platforms, ranging in Ethernet speeds from 10 to 400 gigabits per second, redefine scalability, agility and resilience. Arista has shipped more than 20 million cloud networking ports worldwide with CloudVision and EOS, an advanced network operating system. Committed to open standards, Arista is a founding member of the 25/50GbE consortium. Arista Networks products are available worldwide directly and through partners.

Additional information and resources can be found at:
www.arista.com
www.twitter.com/aristanetworks
www.facebook.com/AristaNW
www.youtube.com/user/AristaNetworks

Job Description
Arista Networks is looking for world-class software engineers to join our team to help us design and develop a next-generation, web-based provisioning, monitoring, analytics and visualization interface to allow network operators to better configure, monitor, and control their networks. You will be part of a fast paced, high caliber team building the software for products used to build the industry's largest data center networks.
At Arista, you will own your projects from definition to deployment, and you will be responsible for the quality of everything you deliver. You will also have the opportunity to do full-stack development, including server-side technologies like Go, HBase, Kafka, Hadoop.
This role demands a strong and broad software engineering background. Your role will not be limited to any single aspect of software development at Arista, but will cover all aspects of software development spanning the addition of new features, debugging problems and fine tuning code for scalability and performance.
Responsibilities:
Architect, design, and develop features and solutions. Develop tests for all code to ensure quality. Fix bugs and refactor code as needed.
Review and contribute to the specifications and implementations written by another team members
Provide technical leadership across features, projects and tools
Work with Customer Support Engineers to analyze problems in customer networks and provide fixes for those problems when needed in the form of new software releases or software patches
Work with the System Test Engineers to analyze problems found in their tests and provide fixes for those problems
Mentor new and junior engineers to bring them up to speed in Arista's software development environment

Qualifications
At least BS Computer Science +3 years’ experience, or MS Computer Science +2 years’ experience, Ph.D. in Computer Science
Knowledge of one or more of Javascript, Go, Python, C, C++
Able to bend the idiosyncrasies of JavaScript, HTML, and CSS to your will in a cross-browser-compatible way
Passion for developing scalable backend systems in Go
Have developed and deployed several applications, and understand the problems that can occur and how to solve them
Familiar with, or have a strong desire to learn, the latest web tools and technologies, including JavaScript ES6/7, React, Redux, Webpack, D3, Kubernetes, Kafka, HBase, and more
Experience with UI/UX design, Network Monitoring, machine learning or data analytics is a plus

Additional Information
All your information will be kept confidential according to EEO guidelines."
Tech - Data Engineer,"Toronto, ON",Exiger,None,Organic,"Location: Toronto, Ontario

Exiger Tech is an experienced team of software professionals with a wide range of specialties and interests. We make use of big data and natural language processing technologies as well as dealing with day-to-day application development and scaling issues. We utilize agile methodologies, iterate quickly to find solutions, and work together to ensure that the system evolves in a maintainable and performant fashion.
This is a full-time opportunity located in Toronto, Ontario.

We are looking for an independent and client-oriented individual with a keen eye for detail to join our new Data Services function, Data Science & Client Integration team within Exiger Tech. The individual will be assisting with building system integration between client systems and our products, as well as providing data insights for internal product improvements and fine tuning for client implementations. A big part of this role involves working with external clients and internal teams to understand the incoming request and writing the necessary reusable, testable, and efficient code to satisfy requirements. Since this is client facing role, it is very important to have the ability to adapt to fluctuating requirements and priorities across a globally distributed client base.
Key Responsibilities
Understand the use cases and the requirements from clients and/or client services and provide technical solutions.
Write effective, scalable python code with proper error handling and logging.
Test and debug programs, and improve functionality of existing systems.
Provide estimates and communicate project plan to stakeholders.
Continually look for process improvements and more efficient use of technology.
Apply data privacy oversight throughout the project life cycle.
Continually look for process improvements and more efficient use of technology.
Knowledge & Skills
Strong practical knowledge of Python coding skills and its data-science related libraries.
Knowledge of application, data, and infrastructure architecture disciplines.
Hands-on database experience, specifically experience querying, designing, and maintaining SQL.
Experience working with RESTful API’s using JSON.
Experience with Git as well as bash scripting.
Familiarity with Docker and Kubernetes.
Strong data analysis skills & experience with data wrangling with Python.
Demonstrated experience operating in both Linux and windows environments.
Exposure to (or desire to learn) Java.
Client Consultancy and Delivery.
Professional Skills Required
Bachelors in Computer Science or Management Information Systems, or equivalent professional experience.
4 years of professional experience in Python scripting languages.
At least 3 years of professional experience with SQL-based databases.
3+ years of experience with ELT/ ETL tools such as Talend, Informatica, Excel, etc.
Proven project management skills.
Superior communication skills.
Proven ability to quickly learn and adapt to a dynamic environment.
Previous experience within the financial services sector is beneficial.
About Exiger
At Exiger we work everyday to make the world a safer place to do business in. Our experts and technology help clients prevent breaches, respond to risk, remediate issues and monitor activities. We are searching for people who think creatively to solve complex problems related to governance, risk and compliance thus delivering first class solutions for our corporate and government partners.
Exiger’s core values are courage, excellence, expertise, innovation, integrity, teamwork and trust.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law."
Data Scientist / Engineer,"Toronto, ON","LotusFlare, Inc.",None,Organic,"Mobile data connectivity drives economic growth and brings vast social benefits to the world, but two-thirds of the world's population is unable to access this valuable resource. Our mission is to make affordable mobile communications available to every human on earth.
We are founded by early Product and Growth team executives from Facebook, and backed by world-leading VCs including Google Ventures, Social Capital, SV Angels, Macquarie Capital, and Compound.
We partner with some of the greatest institutions in the world including Linkedin, Supercell, Twitter, Microsoft, Verizon Wireless, Singtel, and T-Mobile.
SUMMARY
As a Data Scientist / Engineer at LotusFlare, you will play a vital role in redefining connectivity on a global scale. You will work on products bringing connectivity to more than 10 million people across the globe. You will advocate data-fueled products that help our customers to make data-driven decisions. You will provide insight using leading analytics practice and ultimately produce new and creative analytic solutions that will become part of our core products including experimentation platform (Velocity), analytics platform (Periscope), and data platform. You are a strategic thinker who can form hypotheses, synthesize disparate information to validate those hypotheses, and provide actionable insights for the product team to scale user base.
RESPONSIBILITIES
Participate in building models that will scale products with millions of users globally
Partner with cross-functional teams including engineering, UX/UI, sales, marketing, and customer success to build growth strategies and manage complex, cross-functional projects
Analyze diverse sources of data to devise actionable insights
Deep understanding of the B2C consumer markets and core metrics in the markets
Work with Product Managers to develop, execute, and test different growth experiments that have a significant impact on conversion across all funnels (acquisition, activation, retention, engagement, and monetization)
REQUIREMENTS
Undergraduate degree in quantitative fields including Engineering, Math, Statistics from top tier institute or relevant field (MS or PHD is preferred)
Candidate must have the ability to independently build data pipelines, develop data models, and recommend growth strategies to product and executive teams
3 - 5 years of previous experience in building ETLs, analyzing consumer insights, creating metrics
Experience with SQL and/or NoSQL database
Experience with data visualization, dashboards, and reports
Experience with scripting languages such as Python
Candidate must be able to effectively synthesize disparate quantitative and qualitative data sets to make data-driven decisions
Eager to learn new programming languages and tools when needed
Strong desire to work in a fast-paced startup environment
Obsessive around moving critical business metrics and products
Strong communication skills, attention to detail, and ability to manage multiple projects and stakeholders
Great oral and written communication skills in English
Powered by JazzHR
PHHhZpv4wM"
Full Stack Engineer - Data Services,"Ottawa, ON","Bornio, Inc","$71,737 - $135,968 a year",Organic,"Full Stack Engineer - Data Services
Toronto, ON, Montreal ON, or Ottawa, Quebec
ABOUT THE ROLE
*
Bornio is the next generation Sensitive Data Management product that empowers organizations to use relevant & insightful personal information to enable AI & Analytics while preserving the privacy of its consumers, partners, employees, and citizens.
With the growing complexity of privacy regulations like GDPR, CCPA, PIPEDA, and tighter restrictions on personal data use, every organization is challenged in how to continue to gain insight, improve decision making, achieve efficiencies, and competitive advantages.
Bornio’s Cloud-native sensitive data framework offers pre-approved data sharing & de-identification templates that speed up the internal and regulatory approval process. Bornio runtime offers the widest range of de-identification techniques and algorithms to ensure the highest data utility and clear the path to compliance use of personal data.
We are looking for a Full-stack engineer to round out our core engineering team. This is a highly challenging role as it requires a combination of skills for Cloud Native Big Data Engineering, Cloud-Scale Applications Development, and advanced full-stack Java development.
For qualified candidates, it is a unique opportunity to apply their knowledge and expertise to build the most innovative next-generation data privacy solutions.
*
RESPONSIBILITIES
Hands-on expertise with data integration, data pipeline, cloud-native data management, and tools such as S3, AWS Glue, BigQuery, etc.
Comprehensive understanding of the state-of-the-art of big data analytics and data engineering.
Extensive hands-on experience as a full-stack developer of Cloud-native high-performance low-latency applications.
Well versed with at least one of AWS, GCP, or Azure, and experience with Kubernetes/Docker required.
Deep knowledge of Java (3+ years) for server-side components.
Experience with Spring framework, Spring Boot, and Spring Cloud, agile development
Ability to learn quickly and work independently and collaborate with the team
BENEFITS
Competitive compensation and benefits as well as early-stage equity with potential for greater upside.
Be a part of the core engineering team for an exciting venture-backed startup.
Opportunity to hone full product life cycle -- design, implementation, scale.
Job Type: Full-time
Pay: $71,737.00-$135,968.00 per year
Schedule:
8 hour shift
Experience:
javascript: 7 years (Preferred)
Work remotely:
Yes"
"Senior Data Engineer, Marketing Technology","Vancouver, BC",Lululemon,None,Organic,"Who we are

lululemon is a yoga-inspired technical apparel company up to big things. The practice and philosophy of yoga informs our overall purpose to elevate the world through the power of practice. We are proud to be a growing global company with locations all around the world, from Vancouver to Shanghai, and places in between. We owe our success to our innovative product, our emphasis on our stores, our commitment to our people, and the incredible connections we get to make in every community we are in.

About this team

The lululemon Global Marketing Technology Team oversees the technical aspects of lululemon's marketing technology stack across international (APAC and EMEA). One of our goals is to become the CRM leader that best understands and serves our guests through analytics and insights. The production support, maintenance and further enhancements of the full marketing technology stack, starting with the Customer Data Platforms in EMEA and APAC, are key elements to the success of this mission.

Job Summary

As a Senior Engineer for Global Marketing Technology, reporting into the Technology Manager for MarTech International, you will be responsible for the development, support, configuration, and automation of the data infrastructure that supports our global marketing strategy with a special focus on CDP. The solutions you build will involve large datasets and system integrations across both on-premise and cloud environments.

We are looking for a role model and inspiring leader that cares about cross-functional application development and can nurture a relationship with our CDP vendors to deliver a high-quality application which addresses our guest and business needs. This role requires leadership by example and will have you making regular individual contributions such as building out features in the backend for CDP, for example, new APIs, maintenance and refactoring, carrying out code and design reviews, data QA, data reconciliation and testing through unit tests. You are truly the single point of contact and expert on all things related to CDP technology for International.

Typical Duties and Responsibilities

Design, build, operate and optimize the data pipelines, data mapping and data models that run our marketing platforms with a special focus on the Customer Data Platforms. Augment CDP capabilities with new data sets and new data integrations to enhance omnichannel view of guest and omni purchase history.

*

Enhance and maintain existing features by improving and scaling supporting code, ensuring ongoing production support.
*

Implement new technology tools and frameworks where/when needed.
*

Interface with global CDP vendors and internal teams.
*

Oversee, participate in and manage production deployments.
*

Coordinate recovery from incidents utilizing well defined operational procedures, tools, and efficient communication across internal and external stakeholders.

Qualifications

Ability to: Design and communicate important concepts and decision points; communicate technical/complex information both verbally and in writing; analyze and problem solve a variety of issues; carry out strategic planning activities. Mentor upcoming junior engineering talent.

*

5+ years of experience in customer analytics or customer data management, implementation, production support and enhancement - end to end - knowledge and experience on ARM Treasure Data and Acquia/AgilOne CDPs or any other similar Customer Data Platform is a plus.
*

Strong technical understanding of data model, data mapping and data integration with a cross-functional mindset
*

2+ years of experience building data pipelines using MSFT Azure tools and services (Azure Data Factory, SnowFlake, Azure Blob, Azure Databricks, Azure SQL Server, Azure Data Warehouse, Azure Batch..)
*

2+ years of experience with Python and/or PySpark
*

Degree with strong analytical focus (Computer Science, Mathematics, Engineering, Statistics etc.)
*

Proficient in SQL and familiar with Presto / Hive is a plus.
*

Familiarity with agile software delivery methodologies
*

Proficient with DevOps tools and environments like Azure DevOps, Jenkins, Git, Ansible, Terraform.
*

Knowledge of Digital Marketing (Email, Search-Engine Marketing, Social, Affiliate)
*

Great problem solving and analytical skills combined with the ability to explain concepts to both technical and non-technical audiences
*

High energy in partnering with internal team and business teams across the globe to identify new opportunities within CDP to better understand and serve our guests
*

Individual contributor and true team player inspired by single view of customer and motivated by taking CDP and MarTech to the next level

Must haves:
*

Acknowledges the presence of choice in every moment and takes personal responsibility for their life.
*

Possesses an entrepreneurial spirit and continuously innovates to achieve great results.
*

Communicates with honesty and kindness and creates the space for others to do the same.
*

Leads with courage, knowing the possibility of greatness is bigger than the fear of failure.
*

Fosters connection by putting people first and building trusting relationships.
*

Integrates fun and joy as a way of being and working, aka doesn’t take themselves too seriously.

NOTE: Only those applicants under consideration will be contacted. Please accept our utmost appreciation for your interest. lululemon is an Equal Employment Opportunity employer. Employment decisions are based on merit and business needs, and not on race, color, creed, age, sex, gender, sexual orientation, national origin, religion, marital status, medical condition, physical or mental disability, military service, pregnancy, childbirth and related medical conditions or any other classification protected by federal, state or provincial and local laws and ordinances. Reasonable accommodation is available for qualified individuals with disabilities, upon request. This Equal Employment Opportunity policy applies to all practices relating to recruitment and hiring, compensation, benefits, discipline, transfer, termination and all other terms and conditions of employment. While management is primarily responsible for seeing that lululemon equal employment opportunity policies are implemented, you share in the responsibility for assuring that, by your personal actions, the policies are effective.
#LI-NA
#LI-SSC"
Sr Big Data Engineer,"Vancouver, BC","AMZN CAN Fulfillment Svcs, ULC",None,Organic,"Bachelors degree in Computer Science, Electrical Engineering, Information Systems, Mathematics, or a related field
Experience building large-scale distributed applications and services
6+ years experience in developing end to end BI solutions
6+ years experience in Oracle, Redshift, SQL, PL/SQL, and OLAP
6+ years experience in working with large databases
experience in data modeling and/or dimensional modeling
Expertise in SQL, SQL tuning, and ETL development
Proficiency with Linux/Unix systems

Are you interested in building intelligence systems from the ground up?

Are you interested in working with Applied Scientists, Big Data Engineers and Software Development Engineers to build large-scale data intelligence systems?

Are you interested to work in a team that positively impacts different key pillars of Amazon like Pricing, Promotions, Advertising, Auto inventory purchasing, Auto inventory removal, Inventory placement?

Are you interested in working for a team that builds cool systems yet has great work-life balance?

Profit intelligence systems measures, predicts true profit(/loss) for each item as a result of a specific shipment to an Amazon customer.

Profit Intelligence is all about providing intelligent ways for Amazon to understand profitability across retail business. What are the hidden factors driving the growth or profitability across millions of shipments each day?

Profit Intelligence provides these answers.

We compute the profitability of each and every shipment that gets shipped out of Amazon. Guess what, we predict the profitability of future possible shipments too. We are a team of agile, can-do engineers, who believe that not only are moon shots possible but that they can be done before lunch. All it takes is finding new ideas that challenge our preconceived notions of how things should be done. Process and procedure matter less than ideas and the practical work of getting stuff done. This is a place for exploring the new and taking risks.

We push the envelope in using cloud services in AWS as well as the latest in distributed systems, forecasting algorithms, and data mining.

We are looking for a solid Big Data Engineer who can work with 100s of TB of incoming data to build and deploy solutions together with a highly multi-disciplinary team of Applied scientist, software development engineers, strategic partners, product managers and subject domain experts. As a Big Data Engineer on this team, you will play a pivotal role in shaping the definition, vision, design, roadmap, and development of this set of product features from beginning to end.

Responsibilities
Work with a team of product and program managers, engineering leaders, and business leaders to build data architectures and platforms to support business
Design, develop, and operate high-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms
Recognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentation
Keep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture

To learn more about Financial Intelligence Systems, visit our page at http://bit.ly/AmazonFIS

Experience with full software development life cycle, including coding standards, code reviews, source control management, build processes, and testing
Experience with AWS technologies such as EMR, Dynamo, RDS, Redshift, S3, etc.
Experience with programming languages such as Java, C++, Spark/Scala, Python, etc.
Experience with open source big data processing systems and infrastructure such as Spark, Hive, PrestoDB, and etc.
Experience with business intelligence tools such as MicroStrategy, Tableau, Pentaho, etc."
SSENSE - Senior Data Engineer,"Quebec City, QC",Quebec en Mexico,None,Organic,"Job description
SSENSE is looking for a Senior Data Engineer to join our rapidly growing technology team. The Senior Data Engineer will take complex features of the product roadmap, break them down into their required technical components, and develop them independently. They own at least one component of the SSENSE technical stack and hold accountability for its SLAs. The ideal candidate will actively contribute to knowledge dissemination within the organization, participate in the recruiting and onboarding of new employees, and mentor Junior Developers on the team.
RESPONSIBILITIES
Product delivery:
Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets into a readable and accessible format for end-user facing reports, data sciences and ad-hoc analyses
Develop a deep understanding of the product roadmap for the squad, including future features to be developed
Contribute to high-level estimation and participate in laying out the development sequences, challenging the product roadmap and identifying areas where technical debt can be reduced or avoided
Complete independently complex development tasks and actively contribute to pushing code to production
Write testable, efficient, and reusable code suitable for continuous integration and deployment, respecting best practices and SSENSE development standards
Review Unified Modeling Language (UML) diagrams and technical documentation
Ownership and accountability
Be accountable for code quality by conducting adequate testing
Be accountable for performance, reliability, scalability and resilience of at least one technical component owned by the squad through SLAs and monitoring
Solve complex technical problems and mentor/support other technical staff on data modeling and ETL related issues
Contribute to cross-squad initiatives, acting as a change agent amongst peers to foster adoption of new processes or technical solutions
Knowledge sharing and coaching
Review Pull Requests with the objective to guide and upskill other Data Engineers on various technical topics
Actively contribute to SSENSE University (the internal peer learning platform) to promote continuous learning
Participate in the onboarding of new Data Engineers
Architecture
Contribute to solution designs, challenging other members on technical decisions and explaining the technical design to junior developers so they can write documentation for the rest of the team
Recruiting
Participate in HR recruiting events, helping to identify and recruit top developers
Qualifications
REQUIREMENTS
Bachelor’s degree in Computer Science, Engineering, or a related technical field, Master’s degree an asset
A minimum of 5 years of Functional Programming and/or Object Oriented Programming (OOP) experience
A minimum of 3 years experience writing and optimizing SQL queries
A minimum of 3 years experience with Apache Spark for big data processing
Extensive knowledge of Python programming language and its data manipulation libraries (Pandas and Numpy)
Expertise in data modeling and an advanced understanding of data architecture
Expertise with RDBMS and NoSQL databases at scale
Experience with Apache Airflow or other similar data pipelining and workflow scheduling framework (Luigi, Azkaban)
Ability to use containers, orchestration frameworks, and other DevOps tools (Kubernetes, Terraform, Giant Swarm, etc.)
Proficiency with cloud resources (AWS/Google Cloud/Azure) with the ability to operate them for the components owned, Certification is an asset.
Knowledge of the AWS services (Redshift, Glue, Athena, S3, etc.) an asset
Knowledge of big data technology (Databricks, Hadoop, Hive, Pig, Presto) an asset
Familiarity with continuous integration and automated pipeline tools (Jenkins, Travis, etc.)
Proficiency in Git
Strong written and verbal communication skills in both English and French
SKILLS
Highly analytical and detail oriented
Ability to coach and mentor junior employees to achieve personal and professional goals
Team player with a high sense of accountability and ownership
Ability to influence and drive change
Solution-oriented mindset and can-do attitude to overcome challenges
Ability to thrive in a fast-paced environment and master frequently changing technologies and techniques
Required skills
Python 3 to 5 years
Spark 3 to 5 years
Hadoop 3 to 5 years
Data Warehousing 3 to 5 years
Data Lakes 3 to 5 years
ETL Pipelines 3 to 5 years
Data Engineering 3 to 5 years
Agile/Scrum Environment 3 to 5 years
Cloud (AWS, Azure, Ansible or Google Cloud Platform) 3 to 5 years
-

Description de l'emploi
SSENSE est à la recherche d’un Développeur de logiciel senior pour joindre notre équipe technique en pleine croissance. Le Développeur principal prend les caractéristiques complexes de la feuille de route produit, les subdivise en composantes techniques requises, et les développe indépendamment. Il/Elle est responsable d’au moins une composante de la pile technique SSENSE ainsi que des niveaux de service associés. Le candidat idéal participera activement à la diffusion des connaissances à travers l’ensemble de l’entreprise, participera au recrutement et à l’intégration des nouveaux employés et agira à titre de mentor auprès des développeurs juniors de l’équipe.

RESPONSABILITÉS

Livraison de produit
Acquérir une compréhension totale de la feuille de route produit pour l’équipe assignée, incluant les futures fonctionnalités à développer
Contribuer à l’estimation de haut niveau et participer à l’élaboration des séquences de développement, questionnant la feuille de route produit et identifiant les dettes techniques
Réaliser des tâches de développement complexes indépendamment et contribuer activement à pousser le code en production
Écrire des programmes testables, efficaces et réutilisables, qui se prêtent à l’intégration et au déploiement continus et qui respectent les meilleures pratiques et les standards de développement SSENSE
Effectuer la revue des diagrammes UML et de la documentation technique
Appropriation de l'exécution et responsabilisation
Être responsable de la qualité du code, avec l’aide de l’Analyste en assurance qualité, en conduisant les tests adéquats
Être responsable du rendement, de la fiabilité, de l’évolutivité et de la résilience des éléments techniques complexes et essentiels au moyen de surveillance et d’ententes sur les niveaux de service
Contribuer aux initiatives inter-équipes, agissant à titre d’agent de changement auprès des pairs afin de favoriser l’adoption des nouveaux processus ou solutions techniques

Coaching et partage des connaissances
Réviser les Pull Requests afin de guider et contribuer au perfectionnement des développeurs juniors sur divers sujets techniques
Contribuer activement à SSENSE University, notre plateforme d’apprentissage entre pairs, en vue de promouvoir l’apprentissage continu
Participer à l’intégration des nouveaux développeurs

Architecture
Contribuer au design de solutions, questionner les autres membres sur les décisions techniques et expliquer le design technique aux développeurs juniors afin qu’ils puissent écrire de la documentation pour le reste de l’équipe

Recrutement
Participer aux activités de recrutement des ressources humaines et aider à la recherche et l’identification des meilleurs développeurs
Qualifications
EXIGENCES

Baccalauréat en informatique, ingénierie ou un domaine technique connexe; Maîtrise un atout
Un minimum de 5 années d’expérience en programmation orientée objet
Expertise en architecture de microservices
Forte connaissance des technologies de mise en cache (Fastly, Redis, Varnish) avec l’aptitude à mesurer, fragmenter et améliorer la résilience en privilégiant les notions de mise en cache avancées
Maîtrise des systèmes de gestion de base de données relationnelle (SGBDR) et des bases de données NoSQL à grande échelle
Connaissance des ressources informatiques en nuage (AWS, Google Cloud, Azure) et habileté à les utiliser pour les composantes possédées
Capacité à utiliser les conteneurs et les logiciels d’orchestration intégrés (Kubernetes, Giant Swarm, etc.)
Maîtrise de Git
Maîtrise de l’anglais et du français, tant à l’oral qu’à l’écrit

COMPÉTENCES

Esprit d’analyse et souci du détail
Aptitudes en coaching et en mentorat du personnel junior pour atteindre les objectifs individuels et professionnels
Esprit d’équipe et grand sens de la responsabilité
Capacité à influencer et à conduire le changement
Mentalité positive axée vers la recherche de solutions pour surmonter les obstacles
Habileté à prospérer dans les milieux dynamiques et à maîtriser les technologies et les techniques web en évolution fréquente
Compétences requises
Python 3 à 5 ans
Spark 3 à 5 ans
Hadoop 3 à 5 ans
Entreposage de données 3 à 5 ans
Lacs de données 3 à 5 ans
Pipelines ETL 3 à 5 ans
Ingénierie de Données 3 à 5 ans
Environnement Agile/Scrum 3 à 5 ans
Cloud (AWS, Azure, Ansible ou Google Cloud Platform) 3 à 5 ans"
Solution BI - Data engineer,"Quebec City, QC",Quebec en Mexico,None,Organic,"Description de l'emploi
Conseiller – Data Ingénieur
Nous sommes présentement à la recherche d’un conseiller/data ingénieur pour rejoindre notre équipe. Il sera en charge de développer, intégrer et valider des flux de données dans un environnement cloud pour le compte des clients de solution BI.
Responsabilités principales
Mettre au point des outils d’analyse qui utilisent le pipeline de données pour fournir des perspectives applicables en matière d’acquisition de clients, d’efficacité opérationnelle et d’autres mesures clés du rendement opérationnel ;
Créer et maintenir une architecture de pipeline de données optimale et évolutive;
Concevoir l’infrastructure requise pour l’extraction, la transformation et le chargement de données à partir de diverses sources de données ;
Définir, concevoir et mettre en œuvre des améliorations de processus internes : l’automatisation des processus manuels, l’optimisation de la transmission de données, la nouvelle conception de l’infrastructure pour une plus grande évolutivité, etc. ;
Maintenir les données séparées et en sécurité à travers les frontières nationales par l’entremise de plusieurs centres de données ;

Exigences
Détenir au minimum un baccalauréat en informatique ou équivalence, la maîtrise est un atout ;
Posséder au moins 3 ans d’expérience (Python/Java/Scala/SQL/JS/Bash) ;
Avoir au moins 3 ans d’expérience en BIG DATA / Cloud Datawarehouse;
Détenir de solides connaissances des principes fondamentaux du soutien à la clientèle en matière d’algorithmes et de structures de données ;
Posséder de l’expérience en utilisation de services infonuagiques (Microsoft Azure, Databricks), de systèmes de traitement de flux (Storm, Spark-Streaming) ainsi que de pipelines de données automatisés et d’outils de gestion de flux de travail (DevOps, Data Factory, Airflow);
Expertise requise en solutions de bases de données relationnelles (SQL, NoSQL…) comme SQL Datawarehouse, CosmoDB et/ou Snowflake;
Faire preuve d’initiative, d’innovation et de bienveillance ;
Aime travailler dans un milieu collaboratif et dynamique ;
Bilinguisme écrit et parlé (français et anglais).
Le masculin a été utilisé pour alléger le texte.

Compétences requises
Diplôme Informatique 3 à 5 ans
Python 3 à 5 ans
BIG DATA 3 à 5 ans
Azure 3 à 5 ans
Storm 3 à 5 ans
DevOps 3 à 5 ans
SQL 3 à 5 ans
SQL Datawarehouse 3 à 5 ans
Java 3 à 5 ans
Scala 3 à 5 ans
JS 3 à 5 ans
Bash 3 à 5 ans
Cloud Datawarehouse 3 à 5 ans
Databricks 3 à 5 ans
Spark Streaming 3 à 5 ans
Data Factory 3 à 5 ans
Airflow 3 à 5 ans
NoSQL 3 à 5 ans
CosmoDB 3 à 5 ans
Snowflake 3 à 5 ans
-

Job description
Advisor - Data Engineer
We are currently looking for an advisor / data engineer to join our team of experts. He will be in charge of developing, integrating and validating data flows in a cloud environment on behalf of BI solution customers.

Main responsibilities
Develop analytical tools that use the data pipeline to provide applicable insights into customer acquisition, operational efficiency and other key measures of operational performance;
Create and maintain an optimal and scalable data pipeline architecture;
Design the infrastructure required for extracting, transforming and loading data from various data sources;
Define, design and implement internal process improvements: automating manual processes, optimizing data transmission, redesigning infrastructure for greater scalability, etc. ;
Keep data separate and secure across national borders through multiple data centers;

Requirements
Hold at least a bachelor's degree in computer science or equivalent, a master's degree is an asset;
Have at least 3 years of experience (Python / Java / Scala / SQL / JS / Bash);
Have at least 3 years of experience in BIG DATA / Cloud Datawarehouse;
Have a solid knowledge of the fundamentals of customer support in terms of algorithms and data structures;
Have experience using cloud computing services (Microsoft Azure, Databricks), stream processing systems (Storm, Spark-Streaming) as well as automated data pipelines and workflow management tools (DevOps, Data Factory, Airflow);
Expertise required in relational database solutions (SQL, NoSQL…) such as SQL Datawarehouse, CosmoDB and / or Snowflake;
Demonstrate initiative, innovation and kindness;
Enjoys working in a collaborative and dynamic environment;
Bilingual (French and English, spoken and written).
The masculine has been used to lighten the text.
Required skills
Computer Science Degree 3 to 5 years
Python 3 to 5 years
BIG DATA 3 to 5 years
Azure 3 to 5 years
Storm 3 to 5 years
DevOps 3 to 5 years
SQL 3 to 5 years
SQL Datawarehouse 3 to 5 years
Java 3 to 5 years
Scala 3 to 5 years
JS 3 to 5 years
Bash 3 to 5 years
Cloud Datawarehouse 3 to 5 years
Databricks 3 to 5 years
Spark Streaming 3 to 5 years
Data Factory 3 to 5 years
Airflow 3 to 5 years
NoSQL 3 to 5 years
CosmoDB 3 to 5 years
Snowflake 3 to 5 years"
Croesus - Data Engineer,"Quebec City, QC",Quebec en Mexico,None,Organic,"Job description


Want to participate in the evolution of one of the most recognized Fintech software in Canada?
Want to be part of an experienced and dynamic development team?
Are you stimulated by the idea of helping to build a complex infrastructure that generates more than 20 million reports annually?
We want to know more about you!
The data engineer is responsible for the analysis, development and delivery of high quality Croesus environments.
Your daily life:
Design innovative technological solutions and proofs of concept that address specific internal issues.
Support internal and external teams (for example, students) within the framework of Croesus Lab projects.
Keep abreast of technological and scientific advances in areas related to the company’s sectors of activity.
Develop and maintain the laboratory infrastructure.
Document the company’s technological choices and participate in knowledge transfer and training of other internal teams.
Participate in the drafting of reports for the Scientific Research and Experimental Development Tax Incentive Program (SR&ED), patents, and scientific publications in collaboration with the persons concerned.
Participate in various internal and external innovation initiatives.
Participate in conferences and events related to technology or finance.
Requirements
Experience in development: C #, Java or other;
Mastering scripting languages such as SQL / Bash / Shell / Perl / Python on Linux;
Experience in databases;
Experience in data processing;
Rigor and thoroughness
Ability to solve problems
Bilingualism (French and English);
Ability to work in a multidisciplinary team.
Assets:
Knowledge in DataScience, Data Lake;
RedShitf, Snowflake;
ETL tools.
Required skills
C # 3 to 5 years
Bash 3 to 5 years
Power BI 3 to 5 years
Datascience 3 to 5 years
RedShitf 3 to 5 years
Java 1 to 2 years
Qlik 1 to 2 years
Snowflake 1 to 2 years
Shell 3 to 5 years
SQL 3 to 5 years
-

Description de l'emploi
Envie de participer à l’évolution d’un des logiciels Fintech les plus reconnus au Canada ?
Envie de faire partie d’une équipe de développement expérimentée et dynamique ?
Tu es stimulé par l’idée de participer à la réalisation d’une infrastructure complexe permettant de générer plus de 20 millions de rapports annuellement ?
Nous voulons en savoir plus sur toi !
L'Ingénieur de données est responsable de l’analyse, du développement et de la livraison de solutions pour le traitement et la présentation des données Croesus, selon les plans et les échéanciers établis.
Ton quotidien :
Mettre en place et maintenir des programmes/scripts/outils pour l’extraction, la transformation et le chargement de données à partir d’une variété de sources de données;
Concevoir des solutions innovantes de traitement de données volumineuses répondant aux besoins des clients.
Élaborer les plans et les processus de mise en place des solutions retenues;
Déployer et entretenir les environnements pendant le cycle de vie des projets;
Définir, concevoir et mettre en œuvre des améliorations de processus internes : l’automatisation des processus manuels, l’optimisation de la transmission de données;
Proposer des scénarios de tests de performance pour les environnements clients (application et bases de données);
Favoriser l’amélioration continue en proposant de nouvelles solutions et technologies;
Mettre en place des mécanismes permettant d’améliorer la qualité des données;
Participer à la mise en place des mécanismes de déploiement et de surveillance des modèles d’apprentissage machine;
Assister les autres secteurs de l’entreprise relativement aux solutions déployées, à la modélisation des données et aux requêtes SQL;
Faire partie d’une équipe agile dans un contexte DevOps et participer activement à toutes les cérémonies de l’équipe : affinage, scrums, planification, rétrospective et démonstrations.
Toutes autres tâches connexes.
Exigences
Expérience en développement : C#, Java ou autre
Au moins deux (2) ans d’expérience en ingénierie de données
Maîtrise des langages de scripts tels Bash / Shell sous Linux;
Expérience en bases de données relationnelles (Sybase, SQL Server, Postgres)
Expérience en traitement des données (SQL, Python, Talend)
Modélisation des entrepôts de données
Connaissance des outils BI (Power BI, Qlik)
Expérience en utilisation de services infonuagiques AWS (EC2, S3, EMR, etc.)
Expérience en utilisation de pipelines de données et d’outils de gestion du flux de travail (Luigi, Airflow, etc.)
Connaissance des principes CI/CD et les outils associés (Jenkins, TFS, Git, Docker, etc.)
Rigueur et minutie;
Habileté en résolution de problèmes;
Bilinguisme (français et anglais);
Habileté à travailler en équipe multidisciplinaire.
Atouts :
Connaissances en DataScience
Bases de données du cloud : RedShitf, Snowflake, etc.
Outils d’ETL
Compétences requises
C# 3 à 5 ans
Bash 3 à 5 ans
Power BI 3 à 5 ans
DataScience 3 à 5 ans
RedShitf 3 à 5 ans
Java 1 à 2 ans
Qlik 1 à 2 ans
Snowflake 1 à 2 ans
Shell 3 à 5 ans
SQL 3 à 5 ans"
Staff Data Engineer,"Toronto, ON",SADA,None,Organic,"Join SADA as a Staff Data Engineer!
Your Mission
As a Staff Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work, and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.
You will be expected to tackle all technical challenges on whole projects and to mentor less experienced Data Engineers. You should be able to work independently, but should be a major participant in team reviews. You should be recognized as having technical mastery within the practice with an established reputation with Google and our customers. You should have demonstrable experience with public facing activities such as blogs, presentations, webinars, and OSS contributions. You will ensure the best architecture and engineering approach is applied. You will be expected to repeatedly deliver complex projects, and will be the owner of the complete customer outcome, including complex technical components of the engagement. You will actively participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success
#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.
Your success starts by positively impacting the direction of a fast growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.
As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first week orientation at HQ followed by a 90 day onboarding schedule. Details of timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
A secondary Google Cloud certification in any other specialization.
Expert or Professional level certifications in either or both AWS and Azure.
Required Qualifications:
Mastery in at least 2-3 of the following domain areas:
Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ.
Data warehouse modernization: building complete data warehouse solutions on BigQuery, including technical architectures, star/snowflake schema designs, query optimization, ETL/ELT pipelines and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. May involve conversion between relational and NoSQL data stores, or vice versa.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
5+ years of experience writing software in at least two or more languages such as Python, Java, Scala, or Go
Experience in building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience with BI tool like Tableau, Looker etc.
Experience in technical consulting or other customer facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction - willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
About SADA
Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Work with the best: SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the 2018 Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.
Business Performance: SADA has been named to the INC 5000 Fastest Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
agileDSS - Lead Data Engineer,"Quebec City, QC",Quebec en Mexico,None,Organic,"Description de l'emploi

Nous sommes à la recherche d’un(e) Lead Data Engineer pour rejoindre notre équipe de conseillers en analytique avancé et travailler sur des projets innovants avec nos clients. Nos conseillers travaillent pour les plus grandes entreprises québécoises sur des projets à moyen ou long terme et chez nous hors de question de travailler en silo: on partage nos projets, nos compétences et notre savoir-faire au quotidien.
agileDSS, c’est quoi/qui ?
agileDSS est une entreprise de service-conseil spécialisée en analytique avancé. Depuis 15 ans, nous accompagnons les grandes entreprises québécoises à tirer le meilleur parti de leurs données par le biais d’une expertise de pointe en analytique avancé (Big Data, Intelligence d’Affaires, Visualisation de Données et Data Science).
Ce poste est pour toi si tu as :
Plus de 3 en ingénierie de données et/ou transformation de données.
Avoir une expérience pertinente en conception de solutions dans le domaine du traitement des données, de l’informatique décisionnelle et de l’analytique de données.
Maîtrise des techniques, cadres de référence et méthodologies propres à son domaine d’expertise (SAFe, Agile, Software Development Lifecycle , etc.)
Une solide expérience de travail de conception et d’optimisation de pipelines de données sur de grands volumes de données structurées et semi-structurées;
Bonne expérience de travail avec les plateformes de sciences de données tel que Azure Databricks, Spark, Hortonworks, etc. et avec les plateformes de transformations de données de type ETL comme Informatica, Talend, SSIS, etc;
Forte expérience avec les technologies d’intégration de données en flux tels que Apache Kafka;
Grande capacité d’identification et de résolution de problèmes.
Excellentes aptitudes à la coordination, à la négociation et à la prise de décision.
Forte expérience avec les langages de programmation et de gestion de base de données traditionnels, y compris SQL, PL/SQL, avec les langages de manipulation de données propres aux sciences des données tels que Python et R.
Excellent communicateur, structuré et vulgarisateur technique hors pair possédant un bon esprit de synthèse et sachant travailler efficacement en équipe.
Leader naturel avec une capacité d’influencer et d’orienter les autres, particulièrement au niveau des équipes de livraison de solutions et des collaborateurs en périphérie de son champ d’action.
Bilingue Français / Anglais
Mais les hard skills c’est pas tout !
Avant toute chose on cherche un nouveau collègue qui va s’épanouir dans notre environnement! Si tu es une personne qui aime partager ses connaissances, qui prône le self-management, qui est bienveillant, fun et gourmande, tu as de grandes chances de te plaire chez nous !
À quoi tu peux t’attendre dans ton rôle ?
Maintenir une vision globale de tous les pipelines/transformations de données et jeux de données disponibles dans les différents départements / unités d'affaires
Agir en tant que gardien des données de l’unité de rattachement et collaborer à la fois avec les affaires et les TI pour prescrire la saine gestion des données et les meilleures pratiques et idées en partage et consommation efficaces des données;
Encadrer et supporter les équipes de réalisation dans la mise en place de nouveaux pipelines de données et supporter les initiatives ad-hoc de création d’extractions ou jeux de données pour les affaires et les scientifiques de données;
Gérer la mise à jour des catalogues de données organisationnels pour les données publiées dans son unité d'affaires;
Créer des pipelines de données pour des besoins d’exploration analytique d’affaires.
Comprendre les différentes sources de données nécessaires pour supporter les besoins d’exploration analytique d’un secteur d’affaires
Comprendre les modèles développés par les scientifiques de données affaires afin de les opérationnaliser.
S’assurer de la conformité des jeux de données et pipelines de données en regard des règles de gouvernance de données établie.
Ce qu’on t’offre
Nous sommes avant tout une équipe de collaborateurs à taille humaine qui a à coeur le bien-être de ses employés, c’est pourquoi nous t’offrons:
Une rémunération annuelle fixe et une bonification annuelle selon l’atteinte de ta performance
Des objectifs individuels accessibles et réalistes pour garantir un environnement sans pression
Des assurances collectives Manuvie (dentaire, vision, soins paramédicaux…)
4 semaines de vacances
Remboursement des abonnements de sport et du transport
Horaires flexibles et télétravail
Plan de développement pour chaque employé et coaching avec un mentor
Environnement non hiérarchique favorisant l’intrapreneuriat
Activités mensuelles de team building
Abonnement au spa
Compétences requises
Ingénierie de données 3 à 5 ans
Spark / Scala 1 à 2 ans
Microsoft Azure Databricks 1 à 2 ans
Apache Kafka 1 à 2 ans
Gestion d'équipe 1 à 2 ans
Résolution de problem 1 à 2 ans
Relation client 1 à 2 ans
Attitude positive 1 à 2 ans
-
Job description

We are looking for a Lead Data Engineer to join our team of advanced analytics consultants and work on innovative projects with our clients. Our consultants work for the largest Quebec companies on medium- or long-term projects, and at agileDSS, we refuse to work in silos: we share our projects, skills and know-how on a daily basis.
Who is agileDSS?
agileDSS is a consulting firm specializing in advanced analytics. For 15 years, we have been helping Quebec’s largest companies to make the most of their data with our expertise in advanced analytics (Big Data, Business Intelligence, Data Visualization and Data Science).
This job is for you if you have:
More than 3 years in data engineering and / or data transformation.
Have relevant experience designing solutions in the field of data processing, business intelligence and data analytics.
Mastery of techniques, reference frameworks and methodologies specific to his field of expertise (SAFe, Agile, Software Development Lifecycle, etc.)
Solid experience in designing and optimizing data pipelines on large volumes of structured and semi-structured data;
Good experience working with data science platforms such as Azure Databricks, Spark, Hortonworks, etc. and with ETL-type data transformation platforms such as Informatica, Talend, SSIS, etc;
Strong experience with data flow integration technologies such as Apache Kafka;
Great ability to identify and solve problems.
Excellent coordination, negotiation and decision-making skills.
Strong experience with traditional programming and database management languagesincluding SQL, PL / SQL, with data manipulation languages specific to data science such as Python and R.
Excellent communicator, structured and unparalleled technical popularizer with a good spirit of synthesis and able to work effectively in a team.
A natural leader with the ability to influence and guide others, particularly at the level of solution delivery teams and collaborators at the edge of his field of action.
Bilingual French / English
But there is more to it than hard skills!
Most of all, we are looking for a new colleague who will grow in our environment! If you are a person who likes to share his/her knowledge, who advocates self-management, who is benevolent, fun and enjoys food, you will surely like it here at agileDSS!
What you can expect in your role:
Maintain a global view of all pipelines / data transformations and datasets available in different departments / business units
Act as ""Data Guardian"" of your department and collaborate with both business and IT to prescribe healthy data management and best practices and ideas for efficient data sharing and consumption;
Supervise and support the production teams in the implementation of new data pipelines and support ad-hoc initiatives to create data extracts or sets for business and data scientists;
Manage the updating of organizational data catalogs for data published in his business unit;
Create data pipelines for business analytical exploration needs.
Understand the different data sources necessary to support the analytical exploration needs of a business sector
Understand the models developed by business data scientists in order to operationalize them.
Ensure compliance of datasets and data pipelines with established data governance rules.
What we have to offer:
We are above all a human-sized company who takes the well-being of its employees at heart, which is why we offer you:
A fixed annual compensation and a performance-based annual bonus
Accessible and realistic individual goals to guarantee a pressure-free day-to-day environment
Manuvie group insurance (dental care, eye care, paramedical care, etc.)
4 weeks of paid vacation
Reimbursement of sporting and transportation fees
Flexible work schedules and telework
A development plan for every employee and mentoring
A non-hierarchical environment that promotes intrapreneurship
Monthly team building activities
A spa membership
Required skills
Data Engineering 3 to 5 years
Spark / Scala 1 to 2 years
Microsoft Azure Databricks 1 to 2 years
Apache Kafka 1 to 2 years
Team Lead 1 to 2 years
Problem Solver 1 to 2 years
Customer relationship 1 to 2 years
Positive attitude 1 to 2 years"
Data Engineer,"Montréal, QC",Thinking Capital,None,Organic,"Company Description
Available in Montreal, QC or Ottawa, ON.

Job Description
Where others see chaos, you see patterns and emerging structures. You have a passion for the collection, management and analysis of data to discover the patterns hidden within. You thrive on helping your teammates make sense of the data and enabling them to make effective use of the data.
As a data engineer you will be key member of our development team where your passion for data will help us design, develop and deploy our data platform, the foundation of our digital financing platform :
Data modeling, governance and stewardship
Data pipeline to support our BI, data science and machine learning activities
Maintaining data veracity and quality
Help develop and maintain ELT processes
Learn and adapt emerging data technologies and operational best practices to real problems

Qualifications
What You Bring
Extensive schema and data modelling experience
Development experience
Python, Pandas, Sci-Kit
Snowflake, Presto, Hive or other data warehousing technologies
Airflow, DBT
SQL
Working autonomously and being highly resourceful
Bachelors, MSC or PHD in computer science, engineering, or related field
Experience working on AWS
Other Valued Skills & Knowledge
Git or other version control systems
Security
Machine learning
Experience working with Kubernetes

Additional Information
Why join us:
Great Team:
Surround yourself with high-performing, energetic and passionate group of people dedicated to the Thinking Capital Mission;
FinTech Revolution:
Be part of a team that is revolutionizing the financial system and redefining how Canadian small businesses access capital;
Fast-Paced Environment:
Take on complex projects in a start-up like collaborative environment;
Amazing Culture:
Amazing work spaces, advanced technology tools and the flexibility to do your best work
Diversity of thought:
Join a team that values diversity and harmony."
Data Engineer,"Vancouver, BC",PlushCare,None,Organic,"Data Engineer
San Francisco, CA / Vancouver, BC
Employee: Full Time Employee
About Us:
PlushCare is a membership-based virtual primary care platform that is transforming the healthcare industry by making exceptional healthcare more accessible, convenient, and affordable. We connect thousands of patients with world-class physicians right from their phones—creating a seamless and enjoyable experience that eliminates many of the frustrations associated with doctors visits.
From urgent care to ongoing care and even mental health and beyond, we are committed to the whole-body wellness of our patients. As we continue to grow our services and expand our impact, we are looking for passionate and empathetic individuals to join our team.
Description
PlushCare is ruthlessly focused on data and has a complex two-sided marketplace with complex rules and limitations as well as a wealth of services and treatments that we offer to help our patients. As a data engineer you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. As the ideal candidate you are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will collaborate with and support other engineers and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout the system. You are comfortable supporting the data needs of multiple teams, systems and products. As the right candidate you will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.
What you'll do
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Enhance the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Amazon Redshift, Airflow, and other 'big data' technologies.
Help building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Proliferate a data-driven culture PlushCare
What we're looking for
5+ years of industry experience in data engineering in a fast-paced environment, ideally a high growth company
Strong applied analytics - you love data, gleaning insights from data, and making decisions based on data
Advanced working experience with MySQL, PostgreSQL (or similar)
Advanced experience working with Python
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores like Kafka.
Excellent communication and presentation skills - effectively presenting insights is as important as the insights themselves
Experience working cross-functionally, particularly with engineering, product and growth teams
Team-player: we rely on each other to be candid, supportive, hardworking and genuine

We are proud to embrace a diverse & inclusive workplace and are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status."
DevSecOps Engineer - Data Platform,"Vancouver, BC",Cisco Systems,None,Organic,"Why Cisco Cloud Security
We enable our customers to securely adopt the cloud and deliver security the way the world works today.
We protect users against threats anywhere they access the internet, and secure customer data and applications in the cloud. We use the cloud to deliver simplified security policy management and dynamic threat intelligence.
With Cisco Cloud Security, customers gain complete visibility into internet activity across cloud applications, all office locations, and roaming devices, plus faster threat detection and response.
Cisco Cloud Security provides an effective security platform that is open, automated, and simple to use. And it’s backed by industry-leading threat information delivered by the Cisco Talos security intelligence and research group. The Team
The Umbrella Data Platform Team builds the infrastructure that thousands of customers rely on to get visibility into the security of their networks and mobile devices. Our systems process billions of events every day, and we present it via powerful reports and analysis tools. Who are you
You are a self-starter and have a passion to learn technologies. Your strong interpersonal skills, critical thinking and problem solving capabilities coupled with your ability to stay focused while working under pressure make you a great fit for our team What you'll do
Deploy and maintain infrastructure in a 24x7 production environment.
Develop automation for systems deployment, patching of security vulnerabilities, and failure recovery.
Help us integrate security at all phases of the development process, so that we can achieve the highest level of security certifications.
Maintain production services by monitoring latency, availability, and overall health. Determine when we need to increase capacity and scale things up.
Debug system and performance issues. You enjoy diving into the difficult and obscure things that others can’t. Skills and required experience:
Bachelor's degree in Computer Science, a related technical field involving software/systems engineering, or equivalent practical experience.
Significant knowledge of security best practices.
Proven experience with AWS. You can build AMIs with Packer, deploy and manage resources with Terraform, and automate things with Ansible.
Experience with Linux. You know your way around the command-line and are close friends with lsof, strace, tcpdump, and their associates.
A solid understanding of network architecture, and experience building systems that span multiple geographic regions.
Proficiency in one or more scripting languages including Bash, Python, etc. Additional development experience would be an asset.
Experience with the software release process. You know how to get things deployed, and you also always have a rollback strategy in case things don’t go as planned.
Experience performing vulnerability assessments and threat modeling. You know the difference between CSRF and XSS and you love to share your knowledge with others.
Familiarity with Prometheus, Datadog, etc. for monitoring and alerting.
Experience with Big Data would be an asset. We Are Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool."
Data Engineer,"Mississauga, ON",Compass Digital Labs,None,Organic,"Hey there! Just in case you were wondering, this role will be remote for the remainder of 2020 and we will revisit the possibility of working back in our HQ office come January 2021. A member from our recruitment team can explain this in more detail if selected to move forward to the initial phone screen. We look forward to hearing from you!
WHO WE ARE:
Compass Digital Labs is an organization that drives innovation for Compass Group NA, an $18 billion food hospitality organization. Compass Group serves over 3 billion meals per year in award winning restaurants, corporate cafes, hospitals, schools, arenas, museums, and more. As the innovation branch of Compass Group, we’re custom-built for fast-paced transformation at the intersection of hospitality and technology. We’re focused on delivering the best experiences possible for our customers and consumers.
WHAT WE DO:
We are a team of high performing problem solvers with the same vision: to drive the digital future in hospitality. As digital experts we are focused on building a diverse set of products and solutions for our consumers. Our core products include mobile apps, self-serve kiosks, POS and delivery. We also invest in areas like AI, IoT and frictionless retail. Here we work with the “art of the possible” ideas, where we explore and develop the future of hospitality and technology.
We are looking for a Data Engineer who will work closely with senior developers to build out new and maintain existing ETL pipelines in Airflow. In addition this individual will be helping flush out our middle tier API. You will be part of our Data Technology team working with an all new technology stack, working closely in collaboration with internal and external stakeholders.
WHAT YOU'LL BE DOING:
Design, implement and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies
Build and maintain existing Airflow DAGs
Maintain and add to our middle tier API
Prototype new technology that supports our vision of making our consumer’s experiences better with our data
Provide support and insights to the business analytics and data science teams
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs
QUALIFICATIONS:
At least 2-3 years of relevant experience in a similar role or function
Bachelor’s degree or equivalent in Computer Science, Computer Engineering, Information Systems or similar
Programming skills with Python and Golang
Exposure to at least one cloud provider, AWS preferred
Knowledge of SQL, relational database concepts and SQL scripting
Some experience with Airflow highly preferred
Web Development skills with languages and frameworks like Angular, Node.js would be a plus
Bonus points for experience with Docker, Kafka or other data stream processing software platform
Data warehousing experience (RedShift) is a nice to have
ABOUT YOU:
Strong oral and written communication skills
Excellent people and management skills to interact with colleagues, cross-functional teams and stakeholders
A go getter attitude and a passion for your work!

Powered by JazzHR
umizwWgYhv"
Senior Engineer - Data Engineering,"Toronto, ON",Slalom Consulting,None,Organic,"About Slalom Build
Slalom Build is a highly-scalable, high-velocity Build as a Service firm. We work with clients to close the distance between dream and reality, imagined possibility and technical realization.
We do this by blending design, product engineering, analytics, and automation to build the modern technology products of tomorrow.
Nearly 1000 builders strong in eight Build Centers across North America, Slalom Build leverages a foundation of innovation inherited from Slalom Consulting. We’re intensely proud to partner with future-focused clients committed to disrupting their industries.
About Slalom
Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to over 6,000 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2017 and are regularly recognized by our employees as a best place to work. You can find us in 27 cities across the U.S., U.K., and Canada.
Job Title: Data Engineer
As a Data Engineer for Slalom Build, you’ll work in small teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools, Hadoop, Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you’ll be working with some of the most forward-thinking organizations in data and analytics.
Responsibilities:
Work as part of a team to develop Cloud Data and Analytics solutions
Participate in development of cloud data warehouses and business intelligence solutions
Data wrangling of heterogeneous data and explore and discover new insights
Gain hands-on experience with new data platforms and programming languages (e.g. Python, Hive, Spark)
Willingness to travel up to 25%, at peak times of projects
Qualifications:
0-3 years of related work experience in Data Engineering or Data Warehousing
Hands-on experience with leading commercial Cloud platforms, including AWS, Azure, and Google
Proven experience with data warehousing, data ingestion, and data profiling
Proficient in SQL
Strong aptitude for learning new technologies and analytics techniques
Highly self-motivated and able to work independently as well as in a team environment
Understanding of agile project approaches and methodologies
Proficient in a source code control system, such as Git
Proficient in the Linux shell, including utilities such as SSH
Preferred Experience:
Familiarity with implementing analytics solutions with one or more Hadoop distributions (Cloudera, Hortonworks, MapR, HDInsight, EMR)
Familiarity with streaming data ingestion
Proficient in Python and/or Java
Consulting experience
Familiarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)
Slalom Is An Inclusive Employer Dedicated To Building A Diverse Workforce. We Encourage Applications From All Qualified Candidates And Will Accommodate Applicants’ Needs Under The Respective Provincial Human Rights Codes Throughout All Stages Of The Recruitment And Selection Process. Please Advise The Talent Acquisition Team If You Require Accommodations During The Interview Process."
"Machine Learning Engineer, Content Intelligence Team (Data E...","Montréal, QC",LogMeIn,None,Organic,"Job Description
Overview
Here at LogMeIn, we offer a broad portfolio of SaaS products that unlock the potential of the modern remote, in-office and hybrid workforces. We help people do their best work – whenever, wherever and, however. With millions of users across the globe, our portfolio includes some of the most recognized and popular software products in the world today, (Inc. GoToMeeting, GoToWebinar, LastPass, GoToConnect, etc.).
Currently, we are searching for a Machine Learning Engineer ( developing/mid-level ) to join our Content Intelligence Team.
In this Montreal or Quebec City-based role, you will have the opportunity to utilize your Data Engineering technical skills, knowledge, educational and career experiences to help build the next generation of Machine Learning-driven products. Your efforts will have a direct, meaningful and lasting impact on the GoToMeeting user experience for years to come.
Responsibilities
Function as a productive, developing/mid-level Data Engineering-focused Machine Learning Engineer on our Content Intelligence Team
Work under the guidance of senior team members to help develop ETL/data pipelines, feature stores for training ML models, dashboards, POCs, etc.
Document system designs, specifications, work requirements, architectures, etc.
Learn and expand your professional skills and knowledge in order to help build, optimize, monitor and refine the performance of new and existing ML models and systems for training and inference
Collaborate with global teams of Data Scientists, Machine Learning Engineers, Product Managers, etc.
Communicate and translate results to peers, leaders and other business stakeholders
Qualifications
Development skills with Python and associated Python Web Frameworks, (e.g. Flask, FastAPI, Django-web, etc.) is required; experience with ML Libraries and tools such as NumPy, Scikit-learn, Spacy, DataFrame and Jupyter Notebooks is desired
Understanding and experience in Big Data Engineering concepts and processes, (e.g. data stores, structured and unstructured data, SQL Query, ETL/data pipelines, data cleansing, Hadoop framework, etc.)
Experience building products and/or services that require high-performance and scalability; especially, RESTful web services utilizing AWS or Google Cloud
General understanding of building Machine Learning-driven products, (Inc. ML Frameworks, Libraries and Tools such as PyTorch, TensorFlow, Caffe, NumPy, Scikit-learn, Spacy, DataFrame and Jupyter Notebooks; along with, Classification, Natural Language Process, data visualization etc.)
Knowledge of software engineering best practices; including, Agile, coding standards, code reviews, source management, build processes, testing and operations
A desire to learn and expand your professional Machine Learning skills and knowledge is essential; coachability is critical in this role
Ability to clearly communicate in English, (Inc. verbal and written language skills)
LogMeIn Product Portfolio : GoToMeeting, GoToWebinar, Grasshopper, Join.me, GoToTraining, OpenVoice, Jive, Rescue, Bold360, GoToAssist Seeit, GoToAssist, Rescue Lens, LastPass, Pro, Central, GoToMyPC, etc., ( https://www.logmeininc.com/products )
Keywords : Agile, Algorithm, AWS, Big Data, CI/CD, Cloud, Collaboration, Data, DataFrame, Development, Django, Engineering, ETL, FastAPI, Flask, Frameworks, Google, Hadoop, Jupyter, Language, Learning, Machine, ML, Models, Montreal, Natural, NumPy, Pipelines, Prototype, Python, Quebec, RESTful, RPJ, Scikit-learn, Services, Spacy, SQL, Visualization, Web"
Big Data Engineer,"Vancouver, BC",Semios,None,Organic,"Who we are:
Semios is a market leader in leveraging the internet-of-things (IoT) and big data to improve the sustainability and profitability of specialty crops. With 500 million data points being reported by our sensors every day, we leverage our big data analytics, such as in-depth pest and disease modeling, to empower tree fruit and tree nut growers with decision-making tools to minimize resources and risks.
Our innovative work has received several industry awards:
THRIVE - Top 50 Leading AgTech (2020) – recognized as exemplifying some of the best in agriculture technology around the globe.
Global CleanTech Top 100 (2020) – identified as one of the companies best positioned to solve tomorrow’s clean technology challenges.
Google Accelerator (2020) - Selected as 1 of 9 companies for the inaugural Google for Startups Accelerator Canada cohort, who are all using technology to solve complex challenges.
One of our partners produced this short video which shows what we do and our positive environmental impact.
We know our journey is only achievable by having a great team who shares ideas, tries new things and learns as we go.
Who you are:
You are looking to make a difference, you want to know your work with big data has real world benefits. Plus you are excited to make your mark by driving the future of Semios’ data engineering approaches and infrastructure.
What you will do:

This is a growing team, so you will have a significant impact in handling Semios' data by developing scalable infrastructure to process data and integrate data-driven models in partnership with our data science team. You will use your knowledge of the latest cloud technologies, including AWS and GCP, to architect and implement the systems and tools to drive our data engineering forward. In addition, you will work with the data scientists to determine the most effective methods to access data and run models within required time frames.
Requirements
We want you to succeed, so you will need:
In this rapidly developing field you have gained:
In-depth experience with understanding, evaluating, and evolving existing structures to achieve more efficient and effective collection, storage, and access to big data
At least two years experience in object oriented programming, and a fluency in Python and its data analytics ecosystem
Hands-on experience with provisioning and developing on cloud platforms, particularly GCP or AWS
A proven track record of mastering manual database-related tasks and building automation to create efficiencies
A solid understanding of benchmarking and performance tuning
Excellent troubleshooting skills to rapidly identify and resolve issues
The ability to quickly evaluate new technologies, determine their suitability and to integrate the chosen tools into the environment effectively
Excellent verbal and written communication skills with the talent to distill complex ideas to different audiences
The ability to work autonomously within a team environment
Nice to have:
Real world experience with container management system (e.g. Kubernetes) and SQL
Experience with Google BigQuery
Experience with dbt
Advanced education in Big Data
Benefits
Why this is the opportunity for you:
Sleep better knowing you're making the world a better place through more sustainable food production
Work with a team that values fun, laughter, and each other
Have a lasting impact as you help to build a company
Learn a lot along the way!"
"Data Engineer, Analytics and ETL","Toronto, ON",Fleet Complete,None,Organic,"Data Engineer, Analytics and ETL - Toronto/Waterloo

MAIN BUSINESS OBJECTIVES:
Reporting to the Global Head of Data Services, the Data Engineer is tasked with developing the capabilities and daily operations of various integration technologies around our Data platforms. This role is responsible for supporting workloads running on the data warehouse, data lake and associated ingestion technologies (Real-Time, Batch, Streaming) and supporting data quality initiatives. The ETL responsibilities are focused on batch integration technologies, Near Real-Time, and Streaming.

This position will be responsible for the implementation of ingestion frameworks to improve the efficacy and efficiency of data loading and data transport processes. As part of the team, the candidate will also include 24x7 production support rotation and will also take part in the DevOps rotation for making enhancements. The role will be involved in the R&D of emerging technologies with application administrators and technical architects.

ESSENTIAL DUTIES & RESPONSIBILITIES:
Evaluate tools and technologies in the context of the future state ingestion architecture, and evolving business requirements
Provide technology operational support as new systems/platforms are rolled out to the enterprise
Implement and Optimize Data Warehouse, Integration and Analytical technologies
Review solution and technical designs
Propose best practices/standards
Benchmark the performance in line with the non-functional requirements
Previous experience in developing and deploying operational procedures, tuning guides and best practices documentation
Capacity to provide technical guidance, including serving as a resource to project teams and the other teams by evaluating and proposing technical alternatives for resolving business and technology issues
Attention to detail to review project deliverables for completeness, quality, and compliance with established project standards
Working with data architects to ensure that data structures and models are efficiently designed to optimize loading, tagging, cleansing and reprocessing activities
Acts as a positive role model, co-workers and the business group to promote high-performance solutions
Develops strong relationships with the developers and managers to deliver effective services.
Promotes an environment of continuous learning and continuous improvements
Provides 2nd level and 3rd level support
Participate in the on-call support rotation
Keep abreast of software updates and vendor strategies
Perform bug fixes and minor enhancements to existing Hadoop application
Hours of Operations: 9:00 AM – 5:00 PM (Rotational on-call evenings and weekends)

QUALIFICATIONS:
All applicants must possess the following:
3+ years developing and supporting applications leveraging the Microsoft Data Stack (SQL Server, SSIS, Azure, Data Factory etc.)
3+ years of experience with data integration/ETL tools such as DataStage or alternatives
Previous experience developing ETL / Hadoop ingestion frameworks is a must
Previous experience with data quality tools (Atacama, Informatica)
Previous work experience in a lead and/or management capacity is an asset
SDLC knowledge in both waterfall and agile methodologies
Hands-on experience with source code management system (SVN, Git) and continuous integration tools (Jenkins)
Experience in following tools: SQL Server, SSIS, Hive, SQL, Nifi, Spark, Kafka, Flume, Sqoop, HBase, Pig, HDFS, R, NoSQL, Python
Experience in handling data processing, delivering distributed and highly scalable application
Experience with developing solutions which can satisfy 24x7 data availability requirements
Experience with large scale domain or enterprise solution analysis development, selection and implementation
Experience with high-volume, transaction processing software applications
Good understanding of workload management, schedulers, scalability and distributed platform architectures
Experience in software development and architecture experience using Java EE technologies (Application Server, Enterprise Service Bus, SOA, Messaging, Data Access Layers)
Experience in scripting languages & automation such as bash, PERL, and Python
Experience in data warehousing, analytics, and business intelligence/visualization/presentation
Experience using SQL against relational databases
Experience producing data quality reports and deploying data quality fixes
Excellent communication skills (both written and oral) combined with strong interpersonal skills
Strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem-solving environment
Attention to detail
Strong organizational & multi-tasking skills
COMPANY OVERVIEW:
Success stories like this, don’t happen every day. From humble beginnings as a courier industry solutions provider in Canada, Fleet Complete quickly grew to be one of the world’s leaders in telematics and connected mobility solutions for a wide variety of industries with fleets, assets and mobile workers.

Today, with 20 years in the industry, Fleet Complete is one of the fastest-growing IoT (Internet of Things) companies across the globe, operating in 17 countries with offices in Canada, Netherlands, Denmark, Belgium, Estonia, Latvia, Lithuania and Australia. Fleet Complete continues to win employer, innovation thanks to our relentless customer-centric approach and commitment to company values of Innovation, Quality, Customers, Productivity, People and Community.

Thanks to strong partnerships and sound investments, our trusted Fleet and Mobile workforce platform provides real-time insights, visibility, employee safety and overall operational efficiency. This helps organizations, municipalities and businesses of all sizes to modernize their operations with ease. Fleet Complete is known for hiring, growing and empowering talented people who develop innovative products, build powerful relationships and provide personalized support that is unparalleled in our industry. Join Fleet Complete on our next chapter and we can work together to ""help fleets thrive"".

Fleet Complete will provide support in its recruitment processes to applicants with disabilities, including accommodation that takes into account an applicant's accessibility needs. If you require accommodation during the interview process, please contact the Recruitment Team, recruitment@fleetcomplete.com, 866-649-7949.

Fleet Complete is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, national origin, age, disability, protected veterans status or any other legally-protected factors."
Data Engineer,"Toronto, ON",HomeStars,None,Organic,"WHO WE ARE:

Canadians spend over $70B annually on their homes - whether it’s major renovations, or small repairs and maintenance work. This journey often starts by finding the right contractor. HomeStars is the leading platform in Canada, helping homeowners with their home renovation needs by allowing them to search our database of companies and hundreds of thousands of reviews.

Every month over half a million homeowners visit HomeStars to research and connect with the best rated home professionals near them. Home Professionals advertise on HomeStars to tell their story, and grow their business with highly qualified connections. HomeStars has recently been acquired by ANGI Homeservices Inc., the #1 marketplace in the US for helping homeowners connect with the best home service providers.

WHY JOIN US:

HomeStars is looking to continue to build a positive, winning culture and that starts with hiring great people like you! If you are looking to work or be a part of an organization with an exceptional environment, we want to hear from you!

HomeStars embraces and celebrates the uniqueness within our work community. We believe our greatest ideas come from a diverse mix of mindsets, backgrounds, and experiences. The HomeStars organization is committed to cultivating an inclusive work environment where all of our employees feel welcome, comfortable, and have the opportunity to thrive.

Perks of working at HomeStars:
We are growing fast. HomeStars is part of the ANGI Homeservices Inc. family which is public and includes top home service brands like Home Advisor and Angie’s List.
Excellent Benefits Package. We want a healthy and productive team.
Company wide social events and team building activities.
December Break. The office is closed the week before New Year’s.
Personal Growth. We seek to provide employees with constructive feedback to foster their career growth.
Beautiful brick & beam office in the heart of the entertainment district in Toronto.
Rec room with lounge, ping pong, darts and foosball.
Requirements
ABOUT THE ROLE:
As a data engineer you will help us tame our multiple sources of raw data and present it in ways that assist us in making accurate decisions for our customers and our business.

WHAT WE ARE LOOKING FOR:
A strong background with relational and NoSQL data stores
Data Warehouse experience with schema design and creating ETL pipelines
Proficiency with a programming language, Python is a bonus
Someone who can contribute to building and maintaining a comprehensive data dictionary
An active voice in our technical architecture discussions to ensure that data is captured to deliver actionable insights
An interest in uncovering patterns and trends in our data that can inform how we build products


REQUIREMENTS:
3+ years of experience working in data architecture or data modeling
Excellent written and verbal communication and interpersonal skills, able to effectively collaborate with technical and business partners
Demonstrated ability to navigate between big-picture and implementation details


NICE TO HAVE:
Experience in creating dashboards in Tableau or an equivalent application
Knowledge of relational database performance tuning
Data cleaning and validation experience

WHAT YOU WOULD BE WORKING WITH:
Python, R
Informatica
Tableau
Mysql, Postgres, Snowflake
Google Analytics, Mixpanel


Our Engineering Values:
Seek to Understand and Be Understood
Make User Driven, Data Informed Decisions - Think Big, Ship Small, to Maximize Learning
Champion Belonging and Wellness for Everyone - Take Responsibility to Build Sustainably
Commit to Continuous Learning


Benefits
We are an equal opportunity employer and do not discriminate based upon race, religion, colour, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics."
Ingénieur de Données Senior / Senior Data Engineer,"Montréal, QC",Square Enix Montréal,None,Organic,"//English version will follow...//
Poste: Ingénieur de Données Senior
Relève de: Directeur BI

Fonctions:
Spécifier et résoudre les besoins en solutions BI de la division mobile de Square Enix West;
Concevoir, monitorer et améliorer des solutions Big Data en batch ou traitement de flux afin de maintenir notre stack analytique à la pointe de la technologie;
Soutenir le besoin d’ingestion des milliards d’events générés mensuellement provenant des jeux mobiles ou de Third Parties dans le data lake centralisé;
Optimiser la performance des pipelines de données en termes de temps et de coût;
Participer à mise en place de services data driven pour les opérations des jeux;
Rédiger et maintenir la documentation technique appropriée;
Participer au processus de qualité des données en collaboration avec l’équipe QA;
Coacher l’équipe BI sur les bonnes pratiques de développement;
Participer à l’amélioration continue des processus BI au sein de l’équipe centrale en collaboration étroite avec l’équipe DevOps du backend;
Effectuer toutes autres taches connexes.
Expérience et qualifications:
Au moins 7 à 10 ans d’expérience pertinente ou équivalente dans le domaine du jeu mobile ou domaine connexe;
Maitrise de plusieurs systèmes de base de données dans le cloud tel que Google Big Query, Snowflake, Redshift ou équivalent;
Maitrise de Java, SQL;
Bonne connaissance de Python;
Expérience sur le design d’architecture pour des solutions Big Data dans un environnement Cloud;
Expérience en développement CI/CD est un atout majeur (Team City, Terraform, Ansible, Gitlab etc.);
Expérience avec les tests fonctionnels et de performance;
Expérience avec la gestion d’events provenant d’un SDK;
Expérience avec l’environnement cloud de Google est un atout important (Dataflow, Dataproc etc.);
Expérience sur le traitement de données en flux est un plus;
Expérience sur l’application de la gouvernance et la sécurité des données (GDPR etc.);
Application concrète des concepts de DataOps;
Connaissance du C#, Scala, Spark, NodeJs.

Qualités interpersonnelles:
Capable de présenter les concepts clés reliés au BI à des audiences variées;
Capacité à résoudre des problèmes complexes;
Rigoureux et précis;
Axé sur l’impact opérationnel, débrouillard et capable de prendre de l’initiative;
Être capable de s’adapter dans un environnement où les priorités peuvent changer régulièrement;
Bonne capacité d’apprentissage;
Être capable de faire preuve d’humilité et d’aider à faire progresser l’équipe.
Motivation et intérêts:
De l’ambition et de la passion pour les jeux vidéo sont essentiels!

-

// ENGLISH VERSION //
Position: Senior Data Engineer
Reports to: BI Director

Duties:
Specify and solve the BI solution needs of Square Enix West’s mobile division;
Design, monitor and improve Big Data solutions in batch or flow processing, in order to keep our analytical stack at the cutting edge of technology;
Support the need to ingest the billions of events generated monthly from the mobile games or third parties into the centralized data lake;
Optimize the performance of data pipeline regarding time and cost;
Participate in the implementation of data driven services for gaming operations;
Write and maintain appropriate technical documentation;
Participate in the data quality process in collaboration with the QA team;
Coach the BI team on good development practices;
Participate in the continuous improvement of BI processes within the central team, in close collaboration with the backend DevOps team;
Perform all other related tasks.

Experience and qualifications:
At least 7 to 10 years of relevant or equivalent experience in mobile gaming or a related field;
Master the several database systems in the cloud such as Google Big Query, Snowflake, Redshift or equivalent;
Master Java, SQL;
Excellent knowledge of python;
Experience on architecture design for Big Data solutions in a Cloud environment;
Experience in CI/CD development is a major assett (Team City, Terraform, Ansible, Gitlab etc.);
Experience with functional and performance testing;
Experience with managing events coming from an SDK;
Experience with the Google cloud environment is an important asset (Dataflow, Dataproc etc.);
Experience in flow data processing is a plus;
Experience in the application of governance and data security (GDPR, etc.);
Concrete application of DataOps concepts;
Knowledge of C#, Scala, Spark, NodeJs.

Interpersonal qualities:
Ability to present key concepts related to BI to a variety of audiences;
Ability to solve complex problems;
Rigorous and precise;
Focused on the operational impact, resourceful and able to take initiative;
Ability to adapt in an environment where priorities may change on a regular basis;
Good learning ability;
Ability to show humility and help the team progress.
Motivation and interests:
Passion and ambition for videogames are essential!"
Senior Data Engineer,"Montréal, QC",Intact,None,Organic,"Hiring Manager: Ionut Calacean
Workplace: Montreal (2020, Blvd. Robert-Bourassa) or Toronto (700 University)
Data is the most important asset in today’s competitive world. We are the Data and Analytics department of the largest P&C insurer in Canada. Our mission is: “Accelerate our data advantage by bringing to user analytical data ready to be consumed, at a proper level of trust and security”.
Our team is using modern concepts, tools and technologies for data ingestion, acquisition and data consumption and work with Agile framework. We are structured into small, highly productive squads and working very close with the Data Lab team composed of Actuaries and Data Scientists.
You dream of exciting challenges at one of Canada’s top 100 places to work? Join our team as Senior Data Engineer and contribute to the planning and execution of data pipelines for various machine learning projects.
You will:
Design, implement, and maintain large-scale batch and real-time scalable data pipelines with complex data transformations;
Perform data wrangling to transform and map data from raw data forms into formats more appropriate and valuable for analytics;
Write and optimize complex queries on large data sets;
Assemble large, complex data sets that meet functional / non-functional business requirements;
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs;
Experience working with business teams to translate functional requirements into technical requirements;
Conduct business and functional requirements gathering and provide projects estimates;
Experience supporting and working with cross-functional teams in a dynamic environment;
Develops workflows and tools that automate data loading processes and help ensure data quality and integrity.



Your Skills


You are:
Motivated and autonomous, you love learning and applying new technologies to solve complex problems;
Eager to understand how the business works and how your work impacts the business;
Comfortable working in complex environments and multidisciplinary teams;
Fast learner, versatile and a good team player;
Excellent communicator with good analytical skills.
You have:
A bachelor’s degree in computer science, Software Engineering or equivalent;
Hands-on experience with leading commercial Cloud platforms, including AWS and Azure;
Excellent SQL, Python and PySpark skills;
Strong experience with Apache Spark(Databricks or similar), Kafka and NiFi;
Hands on experience with Hadoop distributions (Cloudera, Hortonworks) and Hive;
Strong experience with relational SQL and NoSQL databases like PostgreSQL, Oracle, Cassandra, Mongo DB;
Hands on experience with Snowflake or Redshift, or similar cloud storage technologies;
Experience with ElasticSearch and Kibana is a big plus;
Experience with Alation and/or other data catalog tools (Azure, Informatica) is a big plus;
Experience with Kubernetes, Docker is a big plus;
Experience with Agile development methodologies;
Strong presentation, facilitation, verbal and written communication skills, including interpersonal skills.
Here are a few reasons why others have joined our team:
An award-winning, inspiring workplace that supports its people and recognizes great work
Stimulating, challenging projects and development opportunities to help you grow your skills and career
Flexibility in how and where you work
A comprehensive financial rewards program that recognizes your success
An extensive, flexible benefits package
An industry leading Employee Share Purchase Plan where we match 50% of net shares purchased
A casual ‘dress for your day’ culture that encourages you to be yourself
A $350 annual wellness account that promotes an active lifestyle



Closing Statement


We are an Equal Opportunity Employer
At Intact, our value of Respect is founded on seeing diversity as a strength, being inclusive and fostering collaboration. We value diversity and strive to create an inclusive, accessible workplace where all individuals feel valued, respected and heard.
If we can provide a specific adjustment to make the recruitment process more accessible for you, please advise the Talent Acquisition partner who reaches out about the job opportunity and they will work with you to meet your needs.
Background Checks
As an employer and publicly traded financial services company, the best interests of our customers, employees and shareholders are important to us. We want Intact to be a great place to work! This means that internal and external candidates will be asked to consent to background checks so we can learn more about you. Please note that for positions with access to financial data or funds, your credit must be in good standing.
Internal Candidates
For internal candidates, you can apply for a posted position if you have been in your current position for at least 12 months and are performing at a satisfactory level. Please note we may have identified other internal candidates through our Employee Development Program, and that the selection process may also be opened to external applicants.
Eligibility to Work in Canada
It’s important that you are legally eligible to work in Canada at the time an offer of employment is made. You may be requested to provide proof of eligibility at that time.



Referral Bonus


This role is eligible for employee referral bonus. #myReferrals3000"
Big Data Engineer,"Toronto, ON",Enhance IT,"$90,000 - $100,000 a year",Organic,"Big Data Lead Developer
Our growing organization is seeking a skilled professional to serve as an internal resource for our specialized focus in the field of Big Data/Hadoop Development. We provide Fortune 500 clients throughout the United States with IT consultants in a wide-ranging technical sphere.
In order to fully maintain our incoming nationwide and international hires, we will be hiring a Big Data Lead Developer for our Atlanta, GA (US) headquarters to coach/mentor our incoming classes of consultants. If you have a strong passion for the various Big Data platforms and are looking to join a seasoned team of IT professionals, this could be an advantageous next step.
This is a full-time internal position that requires relocation to Atlanta. No travel required after relocation. The Big Data/Hadoop SME will take on the following responsibilities:
Interviewing potential consultants to ensure all onboarding employees will be successful in Big Data domains prior to each onboarding
The design, development and maintenance of our best-in-class Big Data/Hadoop development training materials
Training, guiding and mentoring junior to mid-level developers
Preparing mock interview situations to enhance the esteemed learning process provided by the company
Acting as a primary resource for individuals working on a variety of projects throughout the US
Interacting with our Executive and Sales team to ensure that projects and employees are appropriately matched
Prepping consultants for interviews for specific assignments involving development and implementation of Hadoop and other environments
The ideal candidate will not only possess a solid knowledge of Big Data infrastructures, but must also have a fluency in the following areas (allowing for fluid interactions with other team members scattered across the entry to senior level spectrum):
Hadoop development and implementation
Strong in Object Oriented Development in Scala/Java platform
Hands on experience in big data technologies including Scala or Spark, Hadoop, Hive, HDFS.
Strong SQL skills and experience
Designing, building, installing, configuring and supporting Big Data Clusters Spark/Kafka
Translate complex functional and technical requirements into detail design Implementing ETL process for integration of data from disparate sources
Cloud Experience is a plus
Skills Include:
5+ Years of professional experience in the IT Industry
Bachelor’s Degree in the Computer Science field
Good knowledge in back-end programming, specifically Java/Scala
Good knowledge of database structures, theories, principles and practices
Analytical and problem-solving skills
Proven understanding with Hadoop, Spark, Kafka, Hive, and HBase
Good aptitude in multi-threading and concurrency concepts
Able to work in Atlanta, Georgia
Job Types: Full-time, Contract, Permanent
Pay: $90,000.00-$100,000.00 per year
Experience:
Big Data: 5 years (Required)
Work remotely:
No"
Data Engineer,"Markham, ON",Xperigo,None,Organic,"Job Description:
Xperigo is an exciting place to work. For 28 years, Xperigo has provided products and services to the automotive sector in Canada. Working with some of the biggest and most prestigious automotive brands in the world, Xperigo has developed industry-leading solutions that have fueled rapid growth. With a keen focus on innovation, Xperigo has expanded its suite of services and solidified our place as a valued strategic partner to our many clients. Xperigo is fast becoming a global force in the automotive sector.
Proudly Canadian, Xperigo has provided B2B roadside assistance services in Canada and expanded into the U.S. market in 2017. We provide private label roadside services to top automobile manufacturers, rental car companies and other organizations with large vehicle fleets. We are dedicated to providing best in-class customer experiences to our clients and are continually looking for new and innovative ways to exceed our client’s expectations.
Currently our Technology & Innovation team is looking full-time Data Engineer who will be responsible for data engineering, design and architecture, as well as reporting and analytics. Additional responsibilities of the role include:
Design, build and operate database environments
Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.
Managing master data, including creation, updates, and deletion
Experience building and optimizing data pipelines, architectures, and data sets
Knowledge of message queueing, stream processing, and data stores/warehouses
Focus on data architecture, best practices, reliability, security, and compliance
Provide architectural guidance to improve database and systems operations and integration
Design and implement solutions for data monitoring, availability and reliability, performance, analytics, and security
Managing indexes, statistics, query plans, alerts, database activity, and overall performance activity
Strong advocate for documenting and communicating of our data models and data flows to increase data proficiency for our environments
Assure proactive monitoring, maintenance, and production support of a broad array of data transformation processes and database systems
Troubleshooting the reporting database environment and reports



Job Requirements:
5+ yeas of experience designing, building and administering database systems
5+ years hands-on advanced experience designing and developing BI Solutions and providing technical expertise
5+ years hands-on advanced experience using MS SQL
5+ years of experience with SSAS, SSIS and SSRS
5+ years hands-on advanced experience using Power BI or similar BI platforms
Experience using Cloud architecture
Experience using building data pipelines to integrate with unstructured data sources
Experience in designing and building unstructured data stores using Azure
Experience with Data Warehouse concepts, including the use of Extract, Transform, and Load (ETL) tools
Excellent analytical, troubleshooting, problem-solving and research skills
Must be able to multitask and have experience with interacting within a diverse user/customer base
Excellent written, verbal, and interpersonal communication skills"
Senior Data Engineer,"Kirkland, QC",IQVIA,None,Organic,"Job Purpose:
The SeniorData Engineer (Sr. Application Development Specialist) is a senior position in our development team who should be able to architect solutions, guide our developers, and also be willing and excited to write code. Most often you write the first prototype that the rest of the team goes on to build and implement.
Job Requirements:
Knowledge of academic field(s) or discipline(s):
Bachelor's degree in Computer Science or related discipline or knowledge acquired through equivalent experience.
Work experience: -
4 - 10 years of SDLC experience in database development and ETL.
Knowledge of technology:
Mandatory qualifications:
Strong design experience in DW/BI platforms.
Strong data modeling & data architecture experience.
Strong experience developing ETL/ELT solutions using SQL and stored procedures, preferably Oracle & PL/SQL but not mandatory.
Well versed with defining KPIs, building dashboards and reports preferably using Tableau.
Software development experience in Python or Scala, tools like Git, Airflow and platforms like Spark or Hadoop.
A drive to cut through requirements or the lack of it, learn and implement solutions as quickly as possible.
Guide our software development team members on best practices and help them resolve bottlenecks in all areas above.
Nice to have experience:
Experience developing on cloud platforms, preferably AWS.
Other specific knowledge/ key competencies:
Communicate timely and proactively. Work in a team environment amiably.
Take ownership of work and be autonomous.
Document all architectural artifacts
Be open to changes and new technologies.
At IQVIA, we believe in pushing the boundaries of human science and data science to make the biggest impact possible – to help our customers create a healthier world. The advanced analytics, technology solutions and contract research services we provide to the life sciences industry are made possible by our 67,000+ employees around the world who apply their insight, curiosity and intellectual courage every step of the way. Learn more at jobs.iqvia.com."
Big Data & Cloud DevOps Engineer,"Toronto, ON",Capgemini,None,Organic,"About Capgemini
Capgemini is a global leader in consulting, technology services and digital transformation, is at the forefront of innovation to address the entire breadth of clients opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and industry specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in over 40 countries.
Visit us at www.capgemini.com. People matter, results count.
Job Responsibilities
Role: Big Data & Cloud DevOps Engineer
Location: Toronto, ON.
Type: Full time Permanent with benefits

JOB SPECIFICATIONS AND QUALIFICATIONS:

5 years of experience in Spark Hadoop.
5 years of experience programming in Java Scala.
5 years of experience in Docker, Swarm, Kuberenetes.
5 years of experience with Python, Shell Scripting.
Proficient understanding of distributed computing principles.
Proficient understanding of networking principles.
Proficient understanding of performance computing principles.
Experience with NoSQL, Graph databases such as MongoDB Ignite and Druid.
Experience with Hadoop, Docker, Kubernetes and ELK Stack a plus.
Experience with Hive.

PRINCIPAL RESPONSIBILITIES:

Take ownership of components of the workflows supported within our data ecosystem.
Interact with CM development teams across and understand their application requirements data access patterns and assist them with expediting onboarding to various platforms from a engineering and development aspect.
Design and develop systems that meet our latency volume storage and scale expectations to enhance KPI metrics and monitoring capabilities for use across multiple teams.
Participate in meetings to help influence architectural engineering and development decisions.
Follow our Agile software development process with daily scrums and monthly Sprints.
Ability to work collaboratively on a cross functional team with a wide range of experience levels.
Define best practices for spark usage work with teams to influence their architecture.
Provide expertise in Spark Performance tuning.
Perform knowledge sharing, conduction, education workshops and train other employees is expected.
Keep pace with emerging technology by researching and evaluating products, AUTHORITIES IMPACT RISK.
Recognized Big Data Cloud DevOps Engineer who can helps build components, drive best practices, standards and provide guidance insight to partnering Architecture, Development, Engineering and Operations teams across QTS.
KEY RELATIONSHIPS - QTS Application Development Teams, QTS Architecture and Engineering Team, QTS Engineering and Operations Team."
Data Engineer / Data Analyst,"Montréal, QC",agileDSS,None,Organic,"Nous sommes à la recherche d’un(e) Data Engineer / Data Analyst pour rejoindre notre équipe de conseillers en analytique avancé et travailler sur des projets innovants avec nos clients. Nos conseillers travaillent pour les plus grandes entreprises québécoises sur des projets à moyen ou long terme et chez nous hors de question de travailler en silo : on partage nos projets, nos compétences et notre savoir-faire au quotidien.

agileDSS, c’est quoi/qui ?
agileDSS est une entreprise de service-conseil spécialisée en analytique avancé. Depuis 15 ans, nous accompagnons les grandes entreprises québécoises à tirer le meilleur parti de leurs données par le biais d’une expertise de pointe en analytique avancé (Big Data, Intelligence d’Affaires, Visualisation de Données et Data Science).

Ce poste est pour toi si tu as :
6 ans d'expérience globale
3 ans d'expérience comme Data Engineer
2 ans d'expérience comme Data Analyst
Expérience comme analyste ETL
Expérience dans les modèles de données
Capacité d'analyser de grands ensembles de données
Capacité à rédiger des rapports complets
Esprit analytique et problem solver
Capacité de présentation, bon relationnel
Connaissances Techniques : Spark / Python, Pyspark, Azure (Data Lake Gen2, Data Factory, Event Hub, Azure Data Warehouse), Databricks

Mais les hard skills c’est pas tout !
Avant toute chose on cherche un nouveau collègue qui va s’épanouir dans notre environnement! Si tu es une personne qui aime partager ses connaissances, qui prône le self-management, qui est bienveillant, fun et gourmande, tu as de grandes chances de te plaire chez nous !

À quoi tu peux t’attendre dans ton rôle:
Analyser des besoins auprès des clients
Collecter des données et analyse des résultats
Interpréter des données
Identifier les modèles et les tendances dans les ensembles de données
Définitir de nouveaux processus de collecte et d'analyse de données
Rédiger des rapports à destination des clients

Ce qu’on t’offre
Nous sommes avant tout une équipe de collaborateurs à taille humaine qui a à coeur le bien-être de ses employés, c’est pourquoi nous t’offrons:
Une rémunération annuelle fixe et une bonification annuelle selon l’atteinte de ta performance
Des objectifs individuels accessibles et réalistes pour garantir un environnement sans pression
Des assurances collectives Manuvie (dentaire, vision, soins paramédicaux…)
4 semaines de vacances
Remboursement des abonnements de sport et du transport
Horaires flexibles et télétravail
Plan de développement pour chaque employé et coaching avec un mentor
Environnement non hiérarchique favorisant l’intrapreneuriat
Activités mensuelles de team building
Abonnement au spa"
Customer Engineer - Data Platform,"Ottawa, ON",Microsoft,None,Organic,"Microsoft is on a mission to empower every person and every organization on the planet to achieve more. Our culture is centered on embracing a growth mindset, a theme of inspiring excellence, and encouraging teams and leaders to bring their best each day. In doing so, we create life-changing innovations that impact billions of lives around the world. You can help us to achieve our mission.

Customer Success: Microsoft aspires to help our customers achieve their own digital transformation, leveraging the power of Microsoft Cloud solutions and support offerings. To this end, Microsoft invests in a dedicated Customer Success team that will help Microsoft customers successfully realize their business outcomes.

Data Platform Customer Engineers (CEs) are deep technical advisors supporting Enterprise customers to deliver unique value by removing blockers to consumption within their given Solution Area. CEs provide technical support including risk assessments and tuning to operate and optimize a customer’s cloud or on-premise environment. The Customer Engineer provides support delivery services as well as technical readiness through a Customer’s Support contract via dispatch by the account team. Customer Engineer services can be delivered either remotely or on-premises.
Responsibilities
Support Management - 50%
Participate in proactive account management, spot performance issues, analyze problems, develop solutions to meet customer needs, represent them.
Provides the most effective method of service delivery by analyzing trends and common themes across customers.
Create deliverables to address common customer needs & support mobile-first, cloud-first strategy, share intellectual property with others.
Engages in strategic service delivery planning, in partnership with the virtual account team (VAT), to strengthen targeted customer relationships.
Gathers customer impressions of products and services and integrates this feedback into decision making.
Seeks information about the underlying needs of customers.
Allocates and aligns resources to optimize the customer experience.
Develops and communicates realistic performance goals and standards.
Builds plans that consider potential obstacles and immediate and long-term consequences.
Demonstrates expertise in a specific solution, or several products, feature functions, or services.
Provides stakeholder assistance throughout deployment to avoid/resolve technical issues.
Support Execution - Deliver Results through Teamwork & Optimizing Business Results - 25%
Seeks opportunities to drive Customer Success business results by collaborating with multiple team members.
Identifies opportunities to articulate business value and grow customer/partner relationships in alignment with Customer Success business priorities and stakeholder management principles.
Provides and drives actionable feedback across groups about the customer/partner experience and competitor threats.
Modifies existing intellectual property (IP) or, where applicable, creates new content.
Seeks opportunities to drive Customer Success business results by collaborating with multiple team members.
Leadership and Growing the Business - 25%
Consistently apply ""lessons learned"", model personal accountability & teamwork.
Demonstrates an understanding of his or her role and contribution to customer/partner change management and adoption initiatives.
Understands customer/partner requirements and can map the adoption and optimization of Microsoft technology solutions accordingly.
Drives and Supports innovation focusing on industry solutions and customer business outcomes on the Microsoft platform.
Contribute & participate with meetings to articulate Premier offerings to all customers; share knowledge thru communities, adapt for customers.
Cultivates relationships, credibility, and loyalty with customers and partners intentionally by sharing relevant business expertise.
Demonstrated Self Learner
Qualifications
Key Experiences, Skills and Knowledge:
Deep technical or architectural knowledge of at least 2 of the following areas:
Database administration (preferably with SQL, including Azure SQL) – performance tuning and optimization, trouble-shooting, high availability / disaster recovery, security
Database development (preferably with SQL, including Azure SQL) – designing and building database solutions (tables / stored procedures / forms / queries / etc.)
Business intelligence – combining knowledge of SSIS/SSAS/SSRS/AAS/Azure Data Factory/Power BI technologies with a deep understanding of data structure / data models to design, develop, and tune BI solutions and reports
Advanced data analytics – designing and building solutions using technologies such as Azure Data Factory, Azure Data Lake, HD Insights, Azure Synapse Analytics, Azure Stream Analytics, Azure Machine Learning Service, R server
Experience with the following technologies:
SQL Server 2019, 20v16, 2014, 2012, 2008 R2 and SQL Server 2005
Experience delivering and/or authoring technical training
Bachelor’s Degree (B.S./B.A.) or equivalent, preferably in Computer Science/Engineering or Information Systems or equivalent work experience
Ability to travel internationally and able to obtain a valid passport within 90 days of joining the team
Fluency in written and spoken English. Ability to speak French a plus
Ability to communicate with a variety of different audiences and strong presentation skills
Ability to match technical solutions with customer business requirements
Ability to lead and motivate technical communities
Ability to effectively recognize and adapt to change

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."
Data Architect / Principal Data Engineer,"Toronto, ON",Nomis,None,Organic,"About Us:
We are working on market-leading Pricing & Profitability Management solutions for the Banking & Financial Services industry. Our suite of Enterprise products combines cutting edge Behavioral Science with the “easy button”, all delivered via the cloud. And we are looking for a Data Architect to join our growing team in San Bruno, CA.
About You:
Nomis is developing a highly scalable, super-efficient, big data platform that feeds our predictive analytics and software products. You are an ideal candidate if you have hands-on experience implementing data management frameworks that power analytical applications using relational, unstructured AND big data technologies.
Job Responsibilities:
Design, develop, and maintain highly scalable infrastructure for data pipelines to ingest data from external sources, store in data stores that are scalable, secured, and accessible by other services in the organization
Work in a cross-functional team of software architects, modeling scientists, project managers and other key stake holders to drive overall architecture.
Work with professional services to operationalize this data platform for various customer implementations.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics

Qualifications:
Extensive experience in software development, serverless architecture, and design-driven development
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets
Experience building processes supporting data transformation, data structures, metadata, dependency and workload management
Experience with AWS cloud services: Lambda, EC2, EMR, RDS, Redshift, S3
Experience with big data tools: Hadoop, HIVE / PIG, Spark, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Mongo.
Experience with data pipeline and workflow management tools such as Airflow

Experience:
10+ years of experience in Data Architecture
4+ years of experience in Big Data Architecture
4+ years of experience on AWS
Strong background in relational databases
Strong background in MapReduce/Hadoop frameworks
Experience in ETL frameworks
Expertise in job scheduling (custom and open source), data quality, metadata management and monitoring systems
Experience handling retail bank/financial institution data a plus
BS in computer science, or other related discipline
Powered by JazzHR
GTbl1eYvWm"
"Data Engineer - Python, Airflow","Vancouver, BC",Affinity Staffing,None,Organic,"On behalf of our client located in the Lower Mainland, we're looking for a proven Senior Data Engineer (6-month contract, fully remote) to join its data analytics team consisting of a Cloud Solutions Architect, Principal Developer, Data Analysts, Data Engineers and Data Scientists and drive ETL, data pipeline, and API integration development using technologies like Python and Airflow. The project is focused on gaining a greater understanding of its customers, customers preferences, and interactions with them so they can better tailor their products to their customers needs.
Responsibilities:
Build solutions that are sustainable and scalable.
Design the short-term and long-term data strategy by developing the business data layer allowing business users to generate valuable insights.
Partner with other functional teams within data analytics.
Work with various business units to identify user and business needs that provide data powered solutions.
Design and implement database architectures, frameworks and Data Models.
Design, implement and optimize end-to-end data pipelines and ETL processes focused on 3rd party integrations.
Develop the data foundations to support information deep dives and scalable solutions.
Act as a partner to business leaders by driving data-informed decisions.
Define and advance best practices.
Coach and mentor junior team members.
Qualifications:
8+ years of experience in the analytics and data management space in a technical development role designing and implementing BI/analytics solutions.
Experience developing data pipelines / integrations for data abstraction layers to deliver data driven business insights.
Experience coaching and mentoring junior team members.
MUST have experience developing data pipelines using Python and Airflow.
Experience developing ETL processes.
Ability to communicate analytical insights and make recommendations to both technical and non-technical stakeholders resulting in actionable insights.
Experience working with relational, non relational and multidimensional databases, including NoSQL.
Experience developing business semantic layers.
Experience with a variety of data visualization tools such as Tableau, Locker or Mode.
Experience designing and implementing data analytics solutions for cloud platforms.
Knowledge of or experience in an AWS cloud environment considered an asset.
Must be located in British Columbia.
Must be legally eligible to work in Canada.

Experience : 5 - 10 Years"
Senior Data Engineer,"Ottawa, ON",Lixar I.T. Inc.,None,Organic,"Lixar I.T. is looking for a Senior Data Engineer to join our data team. As a Senior Data Engineer, you will be part of a team supporting and participating in the ongoing development of leading edge applications. The ideal candidate will be a senior developer who has a strong background in Big Data with a mix of general programming and some exposure to data visualization.
As an experienced Data Engineer, you have:
Post-secondary education in engineering or computer science or equivalent work experience
A proven track record using the Apache Hadoop ecosystem (Spark, Data Lake, Hive, HDFS, Impala) to tackle ""big data"" problems
A master of all things SQL (and NoSQL)
5+ years of programming experience in Python
Proven experience using RESTful Web Services & JSON
Good experience using Cloud based data solutions (AWS/Azure)
Experience working with production systems
Knowledge of ELT, ELT, Lambda and Kappa data architectures
Preferably, you also have:
Knowledge of Continuous Integration and Source Control systems (e.g. Gradle, Maven, Bamboo, TeamCity, Git)
Experience with DataBricks
Some Data Visualization experience in Power BI, Tableau, or similar
Exposure to data science, machine learning or statistics
Some experience using Docker
As the ideal candidate, you:
Have a sense of humour
Know that your routine is in fact, not routine
Communicate exceptionally well with management, peers, and clients
Have “Attention to detail” as your middle name
Don’t blame others for your mistakes
Get things done!"
Big Data Engineer,"Kitchener, ON",AstraNorth,None,Organic,"Role and Responsibilities
The candidate will be responsible for manage and maintain the Big Data environments, building high quality application solutions to design specifications, standards, and user requirements by developing and testing components in an iterative manner through an Agile process.
“MUST HAVE” skills and experience
Expertise with Hadoop distributed frameworks handling large amount of data using Spark and Hadoop ecosystems
Experience Hadoop technologies such as Oozie, Kafka, Druid, Hive, Storm, Ignite, Kudu, NiFi
Experience with data loading tools like Flume, Sqoop, as well as different layers of Hadoop Framework – Storage (HDFS, HBASE), Analysis (Hive/Kudu/Druid/Impala etc.), Map Reduce Jobs
Must have hands on Spark/Scala experience
Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels
Experience in the financial sector, and specifically the P&C Insurance industry is considered a strong asset
Firm understanding of database systems – data modelling, SQL and transactional processing
DevOps and Agile collaboration knowledge are necessary (i.e. security, testing, version control, continuous integration, testing, etc.)
“NICE TO HAVE”
Integration of multiple data sources and databases into one system.
Understanding fundamental design principles behind a scalable application
1+ years of software / application design and development experience in a true Agile / Scrum / XP environment with proven skill and effectiveness in the use of Agile SDLC methodologies
Job Types: Contract, Permanent
Schedule:
Monday to Friday
Work remotely:
Yes, temporarily due to COVID-19"
Data Engineer,"Vancouver, BC",Generac Power Systems,None,Organic,"Company Generac Power Systems

Name Data Engineer - Neurio

Req # 55199

Location TBD

Employment Type Full Time

Shift 1st

Are you interested in joining a high-growth company and software team in the Clean Energy industry? Do you want to work with massive amounts of real-time energy data, and the latest technologies in IoT, machine learning, big data, and SaaS?
Come join the Clean Energy Team in our mission to accelerate the adoption of renewable energy and create a more intelligent home!
Neurio, a subsidiary of Generac Power Systems, is looking for a Data Engineer to iterate quickly on all stages of data pipeline, including bringing data intelligence products to production.
This position in based in Vancouver, BC, Canada.
Our software combines intermittent energy sources like solar with residential storage systems in order to build a reliable, environmentally sustainable electricity grid. Our applications help homeowners control their appliances and renewable energy sources for energy savings, help them manage and understand their homes’ energy consumption, help installers manage large fleets of devices, and much more.

What will you do?

Analyze and extract relevant information from large amounts of streaming data to create an event driven data pipeline
Establish scalable, efficient, automated processes for large scale data analyses, model development, validation and implementation, specifically productizing ML applications
Propose and validate solutions for real time streaming solutions
Work closely with scientists and engineers to create and deploy new features

What will you be required to have?

Production experience with various real time streaming technologies (Flink, Spark, Samza, etc.)
3+ years of Production Experience with Java/Scala and Python
1 year of expertise in developing big data pipelines using technologies like Kafka or Kinesis
1 year of experience with large scale data warehousing, mining or analytic systems.
1 year of experience with various AWS services (EC2, S3, Lambda, Kinesis, EMR, Redshift etc.)
Experience building scalable infrastructure software or distributed systems for commercial online services
Strong knowledge of SQL and NoSQL data stores
Experience with YARN, Kubernetes
Experience with orchestration tools such as Apache Airflow
What else will you need to be successful?

Background in Machine Learning and productizing Data Intelligence pipelines
Strong communication skills and commitment to teamwork
Sharp analytical abilities and proven design skills
Strong sense of ownership, urgency, and drive
Proven leadership abilities in an engineering environment in driving operational excellence and best practices
Why work for us?

Comprehensive medical benefits
Employer RRSP contribution matching
Unlimited vacation (and we genuinely encourage you to use it)
Work-from-home flexibility
Spacious new office in the heart of downtown Vancouver
“We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law"""
⇨ Python Full Stack / Data Engineer,"Vancouver, BC",INTERSOG,"$60,000 - $120,000 a year",Organic,"Intersog® is a Chicago-based provider of ROI-driven custom web and mobile development specializing in the delivery of full-service, end-to-end solutions, and project resources to Fortune 500 companies, SMEs and startups. We help our clients attack their ambitious business goals, solve skills shortage issues and become innovative by building Dedicated Software Development Teams in Vancouver, BC, Canada, and/or providing on-demand IT project resources to complete required skills on their in-house teams.
Our primary goal in partnering with our clients is to exceed their expectations and foster an ongoing relationship that envelopes innovation, industry leadership and business strategy while delivering products that bring exceptional user experience, brand elevation and market dominance.
We are looking for an Engineer who will combine the best from Data Science expertise and Python FullStack development experience. The best candidates love to learn and seek out solutions or develop alternate ways to solve challenging problems.
Requirements:
Statistics background
Strong data validation knowledge ( numeric analysis and statistical analysis)
Data modelling and validation of different populations within a data set based on arbitrary conditions
PostgreSQL experience in a Python ecosystem using Django and/or SQLAlchemy.
RESTful API Python experience
Experience in Python/ Django
Nice to have:
Frontend skills (preferably React.js)
We offer
Competitive compensation based on your skills, experience, and customer satisfaction;
Opportunity to work on challenging and exciting international projects;
Flexible working hours and the possibility to work remotely when needed;
Extended medical and life insurance;
Casual, friendly and family work environment
Job Types: Full-time, Contract, Permanent
Salary: $60,000.00-$120,000.00 per year
Additional pay:
Commission pay
Benefits:
Dental care
Disability insurance
Extended health care
Life insurance
Paid time off
Vision care
Schedule:
Monday to Friday
Experience:
ReactJS: 1 year (Preferred)
data validation: 3 years (Preferred)
Django: 3 years (Preferred)
statistics background: 3 years (Preferred)
Python: 3 years (Preferred)
Location:
Vancouver, BC (Preferred)
Work remotely:
Yes, always"
Principal Data Engineer,"Vancouver, BC","Operating Company ACTIVISION PUBLISHING, INC",None,Organic,"The Activision Data Pipeline team is looking for a Principal Data Engineer to help lead the development of our near-realtime pipeline, processing tens of billions of messages per day, feeding our petabyte-scale Data Lake, high-volume stream customers, and API-based access tools.
Game development is a highly dynamic, fast paced environment with numerous challenges beyond boilerplate data pipeline work. Your work will help guide decisions throughout the company, improving all aspects of our players’ experiences.
Responsibilities:
Priorities can often change in a fast-paced environment like ours, so this role includes, but is not limited to, the following responsibilities:
Being the trusted expert charged with guiding the development of our Java-based pipeline, and keeping it up-to-date with the latest technologies that align with the ever-increasing demand for higher-volume, lower-latency data
Designing & developing novel approaches for near-realtime processing of tens of billions of messages daily, providing data to an ever-expanding set of customers in a cost-efficient manner
Collaborating with Activision’s Cloud Infrastructure team to ensure optimal configuration of our GCP-based platform
Representing the data perspective in technical discussions with game studios, other Central Technology groups, vendors, the open source community, and other key partners
Providing technical leadership and mentorship to data engineers of all levels
Documenting and presenting key learnings & best practices to various groups throughout the company and beyond
Ensuring we meet our SLAs/SLOs, including participation in our team support rotation
Player Profile
10+ years of relevant software engineering experience
5+ years of large-scale, highly-distributed cloud application development and operation
Expert-level knowledge of Java, Scala, Python, and/or other relevant languages
Expert-level knowledge of modern, near-realtime data pipeline / Data Lake technologies such as Kafka, Kafka Connect, Spark, Hive, or similar, and various Big Data storage formats and their tradeoffs
Real-world experience of working with Linux/Unix, version control, containerization, etc.
Proven verbal and written communication skills, including the ability to communicate complex technical ideas to end users
Proven ability to understand customer workflows, analyze architecturally significant requirements, and implement systems to match
Knowledgeable and passionate about software development best practices, including testing and build automation
Extra Points:
Demonstrated real-world knowledge of relational, non-relational, and/or other database types
Experience leading a small-to-midsize team of engineers working on complex systems and problems
Knowledge of the games industry, ideally from a live operations (Games-as-a-Service) perspective
Delivering breakthrough analytical projects resulting in measurable engagement or revenue impact
Basic project management knowledge, utilizing software such as JIRA
Our World
Activision Blizzard, Inc. (NASDAQ: ATVI), is one of the world's largest and most successful interactive entertainment companies and is at the intersection of media, technology and entertainment. We are home to some of the most beloved entertainment franchises including Call of Duty®, World of Warcraft®, Overwatch®, Diablo®, Candy Crush™ and Bubble Witch™. Our combined entertainment network delights hundreds of millions of monthly active users in 196 countries, making us the largest gaming network on the planet!
Our ability to build immersive and innovate worlds is only enhanced by diverse teams working in an inclusive environment. We aspire to have a culture where everyone can thrive in order to connect and engage the world through epic entertainment. We provide a suite of benefits that promote physical, emotional and financial well-being for ‘Every World’ - we’ve got our employees covered!
The videogame industry and therefore our business is fast-paced and will continue to evolve. As such, the duties and responsibilities of this role may be changed as directed by the Company at any time to promote and support our business and relationships with industry partners.
Activision is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law and will not be discriminated against on the basis of disability."
Principal Data Engineer,"Vancouver, BC",MacroHealth,None,Organic,"Are you interested in being part of a rapidly team tasked to build the next-generation industry-leading platform for healthcare markets? MacroHealth is looking for a smart, passionate, and highly skilled *Principal Data Engineer* with experience working in startups and larger companies in a high-tempo, dynamic environments. You will be responsible for driving the overall vision and development of our data platform.
This position requires you to be a self-starter with the ability to take ownership, work with tight timelines, and handle various tasks simultaneously while continuing to develop a positive work culture. You will join a world-renowned leadership team with a track record of leading the development of multiple successful companies and products.
*About us:  *
The U.S. health sector is complex and dynamic - a vast and diverse network of entities that impacts the population and economy like no other industry. At USD$3.3 Trillion, healthcare spending represents nearly 18% of U.S. GDP. It is complex and inefficient. Health service buying, selling, and settlements have seen little change in the past decade despite clear changes in available information, technology, regulation, and market consolidation.
Enter MacroHealth. MacroHealth is a mission-driven company. We are building the most advanced and trusted market platform for health service payers and providers. The company’s vision is to create Intelligent Health Markets by building technology, knowledge and relationships that enable payers and providers to optimize the buying and selling of health services.
We are expanding our team of passionate thought leaders as we pursue our goal of disrupting the U.S. health sector with our vision and technology. We are committed to developing leading edge solutions in a supportive environment, focused on customer success and employee fulfillment. We provide our employees an environment in which they can contribute from day one while also providing opportunities for learning and growth. In turn, these optimizations will help to lower the cost of health care services for consumers.
*What you’ll do: *
Design and own MacroHealth’s data platform architecture,
Design, build, and launch efficient and reliable data pipelines to move and transform data,
Articulate solutions and influence leadership,
Collaborate closely across teams with engineers, data analysts/scientists, product managers and business owners,
Build tools for monitoring data health and compliancy according to MH's data policy,
Create and implement data management policies, procedures and standards.
* What you’ll need: *
Fit in a people-first culture of inclusion, teamwork and respect,
5+ years of experience in data engineering and data architecture,
5+ years of experience in building automation of data management services,
5+ years of experience with Apache Spark,
5+ years of experience scripting programming languages, such as Python,
Experience writing complex SQL, Dataframe APIs and ETL processes,
Experience with DataOps,
Experience with databricks,
Experience with workflow management platforms, such as Apache Airflow,
Excellent documentation skills,
Mentorship experience is a huge plus,
Knowledge of Machine Learning is a plus.
Job Type: Full-time"
Data Engineer (Infrastructure),"Edmonton, AB",David Aplin Group,None,Organic,"David Aplin Group, one of Canada's Best Managed Companies, has partnered with our client to recruit for a Full-Time Data Engineer, based in Edmonton, AB.

Our Client is seeking a Data Engineer with demonstrated full stack experience in a fast-paced and creative environment.

The Role:
Establish and maintain a modern analytics ecosystem
Work with the IT department to implement modern data tools and processes
Work with the IT Director to accomplish IT strategy objectives
Coach the development team, and business power users, on best practices for accessing, interpreting and managing data
Work with business analysts and end-users to elicit and finalize requirements
Work with IT department, Business Staff and customers to ensure that solutions are tested and satisfy requirements
Understand business problems and then design and implement solutions using an MVP mindset
Implement data technology solutions in a collaborative and cross-functional team environment
The Ideal Candidate:
Strong interpersonal, communication, and leadership skills
Demonstrated commitment to constant learning and knowledge transfer
Out of the box, thinking and problem solving
Positive attitude with a willingness to work with internal and external stakeholders
Effective written communication
Demonstrated proficiency in modern data architectures (Microsoft Azure experience would be ideal)
Demonstrated capability to design, deploy and maintain data services
Familiarity with microservice architectures, end-user analytics, data catalog’s, data hub concepts, multiple database technologies
Familiarity with business systems (CRM, ERP, eCommerce)
Strong software engineering fundamentals
Hacking, reverse engineering, troubleshooting, googling solutions, copying and pasting from stack overflow, sharing memes and correct spelling
If you are interested in this position and meet the above criteria, please click the Apply button to send your resume in confidence directly to Deborah Dodd, Partner - Aplin Information Technology. We thank all applicants; however, only those selected for an interview will be contacted.

WE APPRECIATE YOUR INTEREST IN DAVID APLIN GROUP.

If this is your first introduction to us, we invite you to become one of our satisfied candidates. David Aplin Group has been Canadian owned since 1975. Our professional consultants are passionate about helping you find a fulfilling job or career and ensuring your complete satisfaction with our process. Our proven track record, nearly 4 decades long, is largely due to our team of highly skilled and successful specialists. Through superior service and a commitment to long-term relationships, we provide deep specialization in core areas for complete recruiting and HR solutions across Canada - all from one source. We look forward to exceeding your expectations!

Learn more about David Aplin Group and view all of our current job opportunities, career tips, and tools at www.aplin.com. #LI-EDM"
Sr Data Engineer,"Toronto, ON",Amazon Advertising Canada Inc.,None,Organic,"5+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
8+ years of experience in building large-scale data/software systems with high performance, great usability and highly satisfied customers.
Experience gathering business requirements, formulating metrics and building highly scalable pipelines.
Detailed knowledge of cloud-based data warehouses, architecture, infrastructure components, ETL and reporting/analytic tools and environments.
Solid understanding of source data, its strengths, weaknesses, semantics and formats.
Solid understanding of big data echo system
Excellent understanding of software development life cycle and/or agile development environment with emphasis on BI practices.
Ability to create collaborative relationships with partners, stakeholders and customers while managing expectations, managing concerns and risks, and communicating progress.
Work with business owners and partners to build analytical solutions that answer their specific business questions
Ability to plan roadmap, prioritize tasks, manage dependencies and deliver on time.
Strong organizational skills and planning prowess, with excellent attention to detail.
Excellent oral and written communication skills.
Experience hiring, retaining, coaching and managing a high-functioning team responsible for reports and visualization.
You should excel in the design, creation, and management of large datasets, and have the analytical skills to answer tough questions for the business.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Sr Data Engineer, Self-Service Performance Advertising

Amazon is investing heavily in building a world class advertising business and we are responsible for defining and delivering a collection of self-service performance advertising products that drive discovery and sales. Our products are strategically important to our Retail and Marketplace businesses driving long term growth. We deliver billions of ad impressions and millions of clicks daily and are breaking fresh ground to create world-class products. We are highly motivated, collaborative and fun-loving with an entrepreneurial spirit and bias for action. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities.

The Analytics and Data Management team for Performance advertising organization is looking for a Data Engineering Manager (Big Data) to play a key role in building its next generation analytics tools and solutions that enables and grows Performance Advertising business. Are you passionate about Big Data and highly scalable data platforms? Do you enjoy building petabyte scale data platforms? And if you have experience in building and maintaining highly scalable data engineering platforms delivering insights then we need you!!!

In this role you will part of Big Data Platform team delivering the analytics needs of the commercial organization by translating business needs into scalable solutions. You will manage a team of Data and BI Engineers and deliver highly visible enterprise level solutions. You will have the opportunity to innovate and drive your vision to build platforms that scale with the business. You will improve productivity of our customers and help various teams across Advertising organization in meeting their goals by enabling them with predictive analytics and business insights.

sspajobs

Experience with Big Data technologies e.g. Hadoop, Hive, Oozie, Presto, Hue, Spark, Scala and more!
Experience with Massively Parallel Processing (MPP) databases - Redshift, Teradata etc
Experience with distributed systems and NoSQL databases
Excellent oral and written communication skills including the ability to communicate effectively with both technical and non-technical stakeholders.
Proven ability to meet tight deadlines, multi-task, and prioritize workload
A work ethic based on a strong desire to exceed expectations.
Strong analytical skills

Amazon is committed to providing employment accommodation in accordance with the Ontario Human Rights Code and the Accessibility for Ontarians with Disabilities Act. If contacted for an employment opportunity, please advise Human Resources if you require accommodation."
"Senior Staff Software Engineer - Data, ETL, and Analytics","Ottawa, ON",Gigamon,None,Organic,"Gigamon Threat Insight is an enterprise network security product focused on providing capabilities that empower our customers to detect and track adversaries in real-time.
Our mission is to use the power of information to detect, track and dismantle hackers' means of attacking our customers. Our team has seen all sides of the equation, as attackers and defenders, in addition to the complex engineering required to solve these problems at scale. Our software helps security professionals get an unparalleled view into their networks, perform forensics on security incidents and build effective early warning systems.
We are looking for Software Engineers to help us solve complex search and pattern matching problems at petabyte scale. Our data collection needs to operate in near real-time, our data stores need to scale linearly with our datasets, our search needs to perform sub-second matches, our classifiers and behavior analytics need to operate over streaming data sets, and our pattern matching needs to support a variety of operators, window functions, and custom intersection semantics. We work with a number of data storage and processing systems, including MySQL, Postgres, Hadoop, HBase/Phoenix, Spark, Elasticsearch, a variety of AWS data services as well as some proprietary systems.
If you are passionate about building robust, high-scale systems, working with large data sets and protecting public and private organizations from today's ever increasing cyber threats, then Gigamon might be the place for you.
Core Job Role
Design, implement, build, test and deploy scalable systems to store, process and retrieve high-rate event streams.
Design and hands-on software development (writing new code) for ETL, enrichment, alerting, and indexing high-rate event streams.
Support the Operations team in capacity planning and performance tuning for large scale systems.
Implement processes for applying data-analysis algorithms to event-streams.
Job Requirements
4 or more years of hands-on coding experience in Java (mandatory).
Knowledge and experience with the Hadoop ecosystem: Hadoop, HBase, Spark
Solid knowledge and understanding of object-oriented programming, data structures, algorithms, software design. Rigor in high code quality, automated testing, and other engineering best practices.
Knowledge of monitoring, benchmarking, diagnostic and performance optimization tools.
Experience in developing high performance and scalable systems
Experience with Amazon Web Services is a plus.
Employment at Gigamon is contingent on meeting eligibility requirements which may include additional security screening on being able to obtain a government sponsored security clearance
About us
Flexible work and vacation schedule
Competitive pay and benefits
We put great emphasis building well tested and stable solutions

#LI-LN1"
Data Engineer,"Toronto, ON",Dean Group,None,Organic,"Data Engineer



Job Description:

If working with billions of events, petabytes of data and optimizing for last millisecond is something that excites you then read on! We are looking for Data Engineers who have seen their fair share of messy data sets and have been able to structure them for building useful AI products.

You will be working on writing frameworks building for real time and batch pipelines to ingest and transform events(108 scale) from 100’ s of applications every day. Our ML and Software engineers consume these for building data products like personalization and fraud detection. You will also help optimize the feature pipelines for fast execution and work with software engineers to build event driven microservices.

You will get to put cutting edge tech in production and freedom to experiment with new frameworks, try new ways to optimize and resources to build next big thing in fintech using data!

Requirements:
You have previously worked on building serious data pipelines ingesting and transforming > 10 ^6 events per minute and terabytes of data per day.
You are passionate about producing clean, maintainable and testable code part of real-time data pipeline.
You understand how microservices work and are familiar with concepts of data modelling.
You can connect different services and processes together even if you have not worked with them before and follow the flow of data through various pipelines to debug data issues.
You have worked with Spark and Kafka before and have experimented or heard about Flink/Druid/Ignite/Presto/Athena and understand when to use one over the other.
On a bad day maintaining zookeeper and bringing up cluster doesn’ t bother you.
You may not be a networking expert but you understand issues with ingesting data from applications in multiple data centres across geographies, on-premise and cloud and will find a way to solve them.
Proficient in Java/Scala/Python/Spark
Notice for Job Applicants
Following the advice of Canadian health authorities, to mitigate the risk of potential spread of COVID-19 and support social distancing, all recruiting activities including interviews and new hire onboarding will be conducted remotely.

Please submit resume to rkohli@deangroup.ca

#DGT2"
Data Engineer,"Moncton, NB",Fiddlehead,None,Organic,"What you'll do
Responsibilities
Use tools and custom code to implement automated data pipelines
Provide data analysis
Develop and advocate data governance best-practices
Design and implement API endpoints to be consumed by data end-users
Design, document and implement data models
What you'll need
Qualifications

Required

BS or MS degree in Computer Science or relevant experience
Strong software development background preferably in Python or Java
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Experience designing, building, and maintaining data processing systems
Strong background in Microsoft Azure, particularly Azure data analytics offerings like Azure Data Factory, Azure Managed SQL, Azure Databricks, Azure Blob storage
Experience developing analytics driven solutions in Microsoft PowerBI

Nice to have

Past experience with Apache Airflow or similar workflow management system
Exposure to Yandex Clickhouse or other column-oriented databases
We're looking for
Core Skills
Airflow Data Modeling CRISP DM Automation Java Python Data Science Data Analysis Databases"
Principal Engineer - Data Applications,"Toronto, ON",Xero,None,Organic,"Xero is a beautiful, easy-to-use platform that helps small businesses and their accounting and bookkeeping advisors grow and thrive.

At Xero, our purpose is to make life better for people in small business, their advisors, and communities around the world. This purpose sits at the centre of everything we do. We support our people to do the best work of their lives so that they can help small businesses succeed through better tools, information and connections. Because when they succeed they make a difference, and when millions of small businesses are making a difference, the world is a more beautiful place.

How you’ll make an impact

Xero views the application of intelligent, data-driven products and services at scale as a key competitive advantage for our business, now and into the future.
As a Principal Engineer in the Data Applications team you will lead the design and build of the data pipelines and applications that will enable Xero to bring smart, data-driven, personalized services to millions of customers around the world. As a professional, commercially experienced software engineer you will provide technical leadership for a group of talented and enthusiastic engineers. You will set technical standards, coach, mentor and develop individuals to create a high calibre delivery capability with a specialization in the design, build, operation and refinement of machine learning training and inference pipelines and frameworks at scale. You will lift the AI/ML capability of teams around Xero, working closely with the relevant Architecture and Engineering Practice teams to establish data processing ecosystems and development patterns that maximize the reach and impact of your teams and programs of work.
What you’ll do
Hands on coding including troubleshooting, proofs of concept and core project contributions
Form and lead cross-functional teams to conceive, design, build and operate web-scale, data driven solutions to internal and customer facing Xero business challenges
Across multiple teams, drive adoption of software-engineering best practice adapted to modern data-driven products including peer-review, data testing, documentation, reproducibility and agile processes
Technical and architectural collaboration over the Data domain in general, and the ML/AI domain specifically, to align teams and ensure consistency across the systems they build
Keep a watching brief on adjacent roadmaps and upcoming technology challenges, guiding the team to navigate them successfully and integrate seamlessly with other pods
Seek out and quantify cross-cutting problems that hinder development teams' ability to deliver ML solutions within Xero
Mentor and develop members of your team, increasing our capability to build and operate intelligent, data-driven, products and services in our core customer offerings
Contribute to the organizational design of the Data team as a whole including designing the capability matrix, talent development plan and leading hiring efforts
Act as an educator and evangelist globally across the Xero business helping to raise the data and AI literacy of the Xero workforce with a particular focus on those in product and engineering roles
Work closely with the Engineering Practice and Architecture teams to uplift the ML/AI knowledge and capability of pods across Xero
Stay current with emerging practices, techniques and frameworks in the fields of applied machine learning and artificial intelligence at scale
Champion the ethical development of data processing systems which include ML/AI components and ensure your teams are always working with the best interests of our users in mindFrom ideation to production enable the development of easy-to-use infrastructure, tooling and monitoring for data applications and reproducible data science workflows
Represent Xero externally by speaking at Meetups and on panels to raise awareness of the work we do and the career opportunities we offer
Cultivate a fun, rewarding, agile and results-driven culture that lives and breathes our Xero values
Success looks like
We are delivering intelligent products and services to market and improving the user experience across our product portfolio
Your team is continually learning and growing and view Xero as a fantastic place to develop their career for the long term
Teams developing ‘ML inside’ systems at Xero have a faster cycle time delivering new products from conception to production deployment
Xero’s in engineering and product management roles are increasingly confident in their ability to identify and evaluate problems that can be solved by the application of analytical data processing and machine learning and are actively considering product ideas that capitalize on our growing capability to execute ML projects quickly and sustainably
Our analytical data stores & processing pipelines have entered into a virtuous cycle with teams across the business committed to improving the quality of the data flowing through our systems
Xero is becoming a destination of choice for software engineers who build and operate machine learning pipelines at scale in our hubs around the world

Critical competencies
Track record of innovating and delivering technology at scale
Thorough understanding of the practices and components required to support machine learning pipelines in production
Strong communication skills, both written and oral and the ability to translate between business and technical audiences
Strong leadership skills with the ability to articulate & advocate the technical vision and direction for your programs of work
The ability to exercise effective judgment, sensitivity and creativity to changing needs and situations within the business
Can successfully build trust and credibility with stakeholders and working relationships across all levels of the business
Excellent interpersonal skills with the ability to contribute meaningful and accurate advice
Experience
Extensive experience coding in production
Expert level hands on practitioner skills in a good selection of the following areas and the appetite to learn those which are unfamiliar:data architecture, solutions architecture, CI/CD, distributed computing, streaming data, machine learning pipelines, deep learning frameworks
Comfortable at the ‘nix command line and with AWS, Hive, Kinesis, Kafka, Kubernetes and an expert programmer in Python/Scala/Java/.Net/Demonstrable experience in developing both training and inference systems for machine learning pipelines at scale
Good understanding and experience measuring and designing for non-functionals, like reliability, availability, security and performance
Proven experience in operating at a senior technical level including leading and mentoring teams
Extensive experience working on software development projects at scale at all stages of the SDLC
Experience mentoring, leading and developing high performing teams within an agile delivery environment
Why Xero?

At Xero, we are empowered to bring our ‘whole self’ to work. Our collaborative and inclusive culture is one we’re immensely proud of. We know that a diverse workforce is a strength that enables businesses, including ours, to better understand and serve customers, attract top talent and innovate. We care about learning together and celebrate our teams’ continuous improvement and career development.

We offer a great remuneration package, including compelling benefits and perks, like Xero shares. We also support flexible working arrangements that allow you to balance your work, your life and your passions. Our Canadian Xero family includes Hubdoc, an automated data capture platform and we have offices in Toronto, Calgary, and Vancouver. From the moment you step through our doors, you’ll feel welcome and supported to do the best work of your life."
Data Engineer,"Montréal, QC",Mistplay,None,Organic,"Mistplay is the first Loyalty Program for mobile gamers. Players use our platform to play games, connect with friends, and earn awesome rewards; such as Amazon gift cards and prepaid Visas.

We leverage a wealth of in-game data and Machine Learning to recommend the best games to our users and coach developers of all sizes to help them build games. We use our marketing expertise and platforms to make sure our studio partners' games reach millions of players around the world.

With a growth of over 10 million users in under 3 years, Mistplay is one of the fastest growing companies in North America. Join us as we continue to level up!

We're a fast-growing tech start-up, which means you can jump right into the action and deep dive in open AI challenges we are facing. Join our skilled team of Data Scientists & Engineers and build Machine Learning products with real-time impact on our business. You'll have a unique opportunity to wear different hats so don't be afraid to roll up your sleeves and think of creative solutions to further develop and optimize our existing AI-solutions.

We're looking for people that are passionate about AI and our product. Technicality is important, but so is being a positive addition to our culture. A can-do attitude will take you a long way with Mistplay!
What You'll Be Doing:
Designing and developing scalable data warehouses and ETL processes for AI and Analytics purposes on our cloud service (AWS)
Working with cross functionally with other departments (Sales, Marketing and Community) to improve quality and reliability of data (data warehouses and data pipelines) for ML, analytics and business purposes
Building monitoring tools for live production of ML experiments
Supporting the orchestration of end-to-end ML projects
What We're Looking For:
Bachelors degree in Computer Science
2+ years in Python
2+ years experience in ETL processes and data warehousing
1+ years working with production AWS
Experience in Big Data and distributed systems (Hadoop, Spark...)
Experience with streaming technologies (Flink, Kafka)
Experience with Kubernetes, Docker
A love for gaming is always a plus!
We work hard to make our office atmosphere as inviting and fun as possible! Working at Mistplay is coupled with a whole array of perks: company Macbooks, automatic standing desk, unlimited free-trade organic coffee, frequent team outings, Friday happy hours, and so much more!

Our culture is deeply rooted in growth and upheld by a team of smart, dynamic, and enthusiastic people. We utilize data to constantly learn, improve, and adapt. We foster an environment where everyone is encouraged to share their ideas, push boundaries, take calculated risks, and witness their visions come to life.

Think you have what it takes? We'd love to meet you!"
Embedded Software Engineer - Data Plane - Multiple Roles (OT...,"Ottawa, ON",Ciena,None,Organic,"Since 1992, Ciena has been driven by a relentless pursuit of network innovation. We believe in a network that grows smarter, more agile, and more responsive every day. This means that when you digitally interact in your world - picking up the phone, streaming video, texting a friend or loved one – your interactions are being enabled by Ciena technologies. Ciena makes your social / entertainment / business existence REAL.
What will you do at Ciena?
Ciena is a network strategy and technology company known for its commitment to customer success. With nearly 25 years of industry leadership, we support more than 1,300 of the world’s largest, most reliable networks. Our technology is complemented with a high-touch consultative business model. We’re committed to developing and applying technologies that facilitate openness, virtualization, automation, collaboration, and a common experience. Technologies that offer the greatest degree of choice deliver the most rewarding customer experiences and business outcomes.
JOB DESCRIPTION & RESPONSIBILITIES
Optical networks continue to aggressively move towards Layer 0/1/2/3 converged infrastructures. Ciena’s Packet Optical Transport Switching (POTS) portfolio of Layer 2, MPLS and IP networking products provide carrier grade packet networking services in metro networks.
The POTS Datapath software group is seeking multiple embedded software engineers with experience in designing and developing embedded carrier-grade software on custom hardware platforms. As a member of the team, the successful candidate will participate in all states of the software development life cycle, including:
Design and develop software written in C for Layer 2, MPLS and IP Datapath applications and the hardware abstraction layer. This includes enhancements of existing software as well as development of new networking features.
Write software which runs on an embedded Linux/VxWorks platform.
Participate in backlog defect reduction.
Troubleshoot issues and work with hardware, software and system engineers to identify the root cause.
Help investigate and collect information to resolve process or design issues found in the codebase.
Identify software performance improvements through test driven development, automated unit testing, and comprehensive integration testing.
WHAT YOU MUST HAVE
Bachelor’s degree in Electrical/Computer Engineering or Computer Science.
5 + years’ work experience (Dependent on level) focused on embedded software development.
Extensive C programming language experience.
Experience with real-time, embedded software development for some or all of the following data plane technologies:
Fastpath hardware (ASICs, NPUs, FPGAs, TCAMs) Layer 2 and Layer 3 forwarding engines
End-to-end slow path forwarding and packet exception handling
MPLS, BGP and Segment Routing centric solutions
IPv4, IPv6, E-VPN and IP-VPN
Embedded fast protection mechanisms such as BFD, BGP-PIC and FRR
Other data-plane services including traffic management, metering, ACLs
Experience with Broadcom DNX/XGS packet processing chipsets is a strong asset
Experience with datapath programming using DPDK is an asset
Ability to work in a lab environment and integrate software onto custom and off-the-shelf data plane hardware components such as ASICs, NPUs, FPGAs and TCAMs and associated SDKs.
Familiarity with developing software on multi-processor, highly concurrent systems
Good knowledge of mutual exclusion, synchronization, interrupt handling, inter-process communication, etc.
Ability to seek out answers and work independently.
Positive work attitude, highly motivated and a willingness to learn.
Strong verbal and technical writing skills.
Collaborates well in a team environment.
Independent self-starter and commitment to delivering on aggressive deadlines.
LI-EP
About Ciena
Ciena is a network strategy and technology company with a passion to provide an experience, to you and our customers that is as rewarding as the outcome. We attract the best and brightest– those with outstanding talent, motivation, and the right attitude to contribute to our success. Our culture balances our openness and informality with professionalism and trust and is built on the foundation of our core values: Customer First, Integrity, Velocity, Innovation, and Outstanding People.
Ciena enables everyone to have a voice and a network that supports them while on the journey to discovering their passion and purpose. We trust each individual to do what they can to reach their full potential and make an impact on the business, whenever, wherever they are in the world. With Ciena’s highly innovative, forward-thinking business practices, we reward people for pushing the boundaries. Unlock your potential at Ciena!

Being You @ Ciena
As part of our commitment to diversity and inclusion, we want to foster an environment that values and respects all individual’s strengths, perspectives, ideas, and ability to meet the needs of our customers globally. Ciena values the diversity of its workforce and respects its employees as individuals, regardless of race, ethnicity, religion, gender, age, national origin, disability, sexual orientation, veteran or marital status or any other category protected by applicable law. We do not tolerate any form of discrimination.
Ciena is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
If contacted in relation to a job opportunity, you should advise Ciena in a timely fashion of the specific accommodation measures required for you to be assessed in a fair and equitable manner."
Senior Data Engineer,"Montréal, QC",SSENSE,None,Organic,"Company Description
SSENSE, pronounced [es-uhns], is a Montreal-based fashion platform with global reach. Founded in 2003, SSENSE is pacing the vanguard of directional retail with a mix of luxury, streetwear, and avant-garde labels. We produce industry-leading original content and take pride in building our own technology solutions and systems from scratch. Our field of focus has grown beyond that of a typical e-commerce entity as we explore the nexus of content, commerce, and culture. Currently serving 150 countries, generating an average of 76 million monthly page views, and achieving high double digit annual growth since inception, SSENSE is becoming a cultural protagonist in its own right.

Job Description
SSENSE is looking for a Senior Data Engineer to join our rapidly growing technology team. The Senior Data Engineer will take complex features of the product roadmap, break them down into their required technical components, and develop them independently. They own at least one component of the SSENSE technical stack and hold accountability for its SLAs. The ideal candidate will actively contribute to knowledge dissemination within the organization, participate in the recruiting and onboarding of new employees, and mentor Junior Developers on the team.
RESPONSIBILITIES
Product delivery:
Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets into a readable and accessible format for end-user facing reports, data sciences and ad-hoc analyses
Develop a deep understanding of the product roadmap for the squad, including future features to be developed
Contribute to high-level estimation and participate in laying out the development sequences, challenging the product roadmap and identifying areas where technical debt can be reduced or avoided
Complete independently complex development tasks and actively contribute to pushing code to production
Write testable, efficient, and reusable code suitable for continuous integration and deployment, respecting best practices and SSENSE development standards
Review Unified Modeling Language (UML) diagrams and technical documentation
Ownership and accountability
Be accountable for code quality by conducting adequate testing
Be accountable for performance, reliability, scalability and resilience of at least one technical component owned by the squad through SLAs and monitoring
Solve complex technical problems and mentor/support other technical staff on data modeling and ETL related issues
Contribute to cross-squad initiatives, acting as a change agent amongst peers to foster adoption of new processes or technical solutions
Knowledge sharing and coaching
Review Pull Requests with the objective to guide and upskill other Data Engineers on various technical topics
Actively contribute to SSENSE University (the internal peer learning platform) to promote continuous learning
Participate in the onboarding of new Data Engineers
Architecture
Contribute to solution designs, challenging other members on technical decisions and explaining the technical design to junior developers so they can write documentation for the rest of the team
Recruiting
Participate in HR recruiting events, helping to identify and recruit top developers

Qualifications
REQUIREMENTS
Bachelor’s degree in Computer Science, Engineering, or a related technical field, Master’s degree an asset
A minimum of 5 years of Functional Programming and/or Object Oriented Programming (OOP) experience
A minimum of 3 years experience writing and optimizing SQL queries
A minimum of 3 years experience with Apache Spark for big data processing
Extensive knowledge of Python programming language and its data manipulation libraries (Pandas and Numpy)
Expertise in data modeling and an advanced understanding of data architecture
Expertise with RDBMS and NoSQL databases at scale
Experience with Apache Airflow or other similar data pipelining and workflow scheduling framework (Luigi, Azkaban)
Ability to use containers, orchestration frameworks, and other DevOps tools (Kubernetes, Terraform, Giant Swarm, etc.)
Proficiency with cloud resources (AWS/Google Cloud/Azure) with the ability to operate them for the components owned, Certification is an asset.
Knowledge of the AWS services (Redshift, Glue, Athena, S3, etc.) an asset
Knowledge of big data technology (Databricks, Hadoop, Hive, Pig, Presto) an asset
Familiarity with continuous integration and automated pipeline tools (Jenkins, Travis, etc.)
Proficiency in Git
Strong written and verbal communication skills in both English and French
SKILLS
Highly analytical and detail oriented
Ability to coach and mentor junior employees to achieve personal and professional goals
Team player with a high sense of accountability and ownership
Ability to influence and drive change
Solution-oriented mindset and can-do attitude to overcome challenges
Ability to thrive in a fast-paced environment and master frequently changing technologies and techniques

Additional Information

null"
Data Engineer - Level 3,"Montréal, QC",SSENSE,None,Organic,"Company Description
SSENSE, pronounced [es-uhns], is a Montreal-based fashion platform with global reach. Founded in 2003, SSENSE is pacing the vanguard of directional retail with a mix of luxury, streetwear, and avant-garde labels. We produce industry-leading original content and take pride in building our own technology solutions and systems from scratch. Our field of focus has grown beyond that of a typical e-commerce entity as we explore the nexus of content, commerce, and culture. Currently serving 150 countries, generating an average of 76 million monthly page views, and achieving high double digit annual growth since inception, SSENSE is becoming a cultural protagonist in its own right.

Job Description
SSENSE is looking for a Data Engineer to join our rapidly growing technology team. The level 3 Data Engineer will join a squad and deepen their knowledge of software development and data pipelines. They will break down, with minimal guidance, large tasks into smaller, manageable steps to deliver complex tasks required for well-defined features of the Product roadmap. The ideal candidate will contribute to knowledge dissemination within the organization and participate in the recruiting and onboarding of new employees.
RESPONSIBILITIES
Product Delivery
Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets into a readable and accessible format for end-user facing reports, data sciences and ad-hoc analyses
Understand the high-level product roadmap and immediate features to be developed, contributing to high-level estimation and layout of the development sequences
Complete complex development tasks with minimal guidance
Constantly and actively contribute to pushing code to production with the objective of becoming a main contributor
Review Pull Requests
Write testable, efficient, and reusable code suitable for continuous integration and deployment, that respect best practices and SSENSE development standards
Review Unified Modeling Language (UML) diagrams and technical documentation, ensuring its quality
Ownership and accountability:
Be accountable for code quality and conduct adequate testing
Review and contribute to technical documentation
Knowledge sharing and coaching
Join SSENSE University (the internal peer learning platform) sessions to ramp up on various technologies and host at least two sessions per year
Lead the onboarding of new data engineers
Architecture:
Contribute actively to the design of the solution, challenging other members on technical decisions
Help more junior Data Engineers understand the technical design so they can write documentation for the rest of the team
Recruiting:
Participate in HR recruiting events, helping to identify and recruit top tech talent

Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field
A minimum of 3 years of Object Oriented Programming (OOP) and/or functional programming experience
Knowledge of Apache Spark for big data processing
Knowledge of Python programming language
Knowledge of the data modelling concepts and ability to define the architecture with minimal guidance to develop a complex microservice
Familiar with various database systems and able to write complex queries independently
Knowledge of cloud concepts and the ability to follow instructions to use them with minimal guidance
Knowledge of the AWS services (Glue, Athena, S3, Databricks, etc.) and Apache Airflow, an asset
Proficiency in Git
Strong English written and verbal communication skills, French is an asset
SKILLS
Fast learner and detail oriented
Solution-oriented mindset and can-do attitude to overcome challenges
Team player with a high sense of accountability and ownership
Ability to thrive in a fast-paced environment and master frequently changing technologies and techniques

Additional Information

null"
Senior Scala Engineer - Data,"Winnipeg, MB",SkipTheDishes,None,Organic,"Description:
SkipTheDishes, an arm of Just Eat Takeaway, is searching for a Senior Data Engineer to join the Data Systems - Core platform team. You’ll have the opportunity to work with big data technologies, building scalable and reliable solutions to support real-time analytics, advanced data science and critical operational projects powered by data.
What We Do
The Data Systems team’s role is to build a transformational data platform in order to democratize data in Just Eat. Our team is built on the following ideals:
Open Data: We ingest all data produced across Just Eat using batch and real-time pipelines and make it available to every employee in Just Eat. This data is then used to drive analytics, business intelligence, data science and critical business operations.
Self Service: We build tools, frameworks and processes to support self-service modelling and activation of data. Our goal is to empower our users to find, process and consume our data without barriers.
Single Truth: We build services that host all metadata about Just Eat’s data in a single store and promote governance, data culture and Single Source of Truth.
Intelligent Personalisation: We build and maintain a machine learning platform that supports data scientists in developing and deploying ML models at the production scale. This allows us to deliver insights, personalization and predictions to our customers at scale.

How We Do It
Our team is built on the following tenets:
Innovate: We are always learning, growing, inquisitive and keen on new technologies and open source tooling. We love like-minded engineers with a passion to keep our code-base and infrastructure best in class.
Build for Scale: All our tools and components are built for scale and we use Kubernetes and other tools to help us scale automatically.
Cloud-based: We use serverless technologies where possible to simplify our estate, technologies like BigQuery, PubSub, Dataflow and Cloud functions allow us to move quickly. In addition, we run a Kubernetes cluster on GKE with many workloads including instances of Apache Airflow.
DevOps culture: Everyone in the team contributes to infrastructure, we have a CI/CD pipeline and we define our infrastructure as code. Our stack includes Terraform, Jenkins and Helm. Teams monitor their applications using Prometheus, Grafana and alert manager.
Collaboration & Ownership: All code is owned by the team and we have multiple avenues for collaboration - rotation, pairing and technical showcases. We also encourage team members to own their own code and promote self-governance.
We’re looking for enthusiastic engineers to join our team in Data Systems
Core Systems
Our team’s mission is to build base functionalities that power a leading data platform. Our portfolio includes a variety of highly critical tools including (but not limited to) Real-time Ingestion, GDPR, Metadata Catalogues, and Access Controls. Here are snippets of some of our portfolio
Real-time Ingestion - A generic data ingestion pipeline currently running up to 10k events per second from 2.5k topics.
Metadata Catalogues - Infer and store context around data (about 1 PB in 500 datasets) and keep it secure and searchable.
Our stack is primarily Scala using libraries like ZIO, Cats, Akka streams and Apache beam. Python is also used in lots of areas.

You should apply if
You are confident in a functional programming language like Scala both in and outside of the data domain.
You love writing well tested, readable and performant code, capable of processing large volumes of data.
You love working with Cloud technologies and have experience in working with AWS, Azure or Google Cloud. We use Google Cloud with a mix of services - Kubernetes, Dataflow, PubSub etc.
You can contribute to architecture discussions and influence peers and stakeholders to make better decisions.
You have the inclination to collaborate and the ability to communicate technical ideas clearly.
You understand the entire product development lifecycle, from coding to deployments, to monitoring, alerting etc... Our teams maintain all aspects of our product lifecycle, but we don’t expect everyone to be an expert in all of it.
You understand the fundamentals of computing and distributed systems.
Why Work At Skip?
Picture this: you, dressed in your fave casual attire, amongst a team of friendly and passionate colleagues. You feel pride knowing your input and uniqueness are not only embraced but make an impact on a major Canadian company and its satisfied customers. As the company grows, so do you — you meet and surpass new challenges every day.

That’s just a small taste of what it’s like to work at one of Canada’s leading tech companies. If you’re hungry for opportunity, growth, and something meaningful in a dynamic, yet casual environment, we’d love to hear from you.

Skip The Dishes is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We are committed to accommodating applicants with disabilities throughout the recruitment activities and will work with all applicants requesting accommodation at any stage of the hiring process.

Note: All employees will be asked to sign a Consent for Disclosure of Personal Information in order to complete a background check. Job offers will be conditional upon results that the Company determines to be satisfactory."
Senior Data Engineer,"Burnaby, BC",Infoblox,None,Organic,"Infoblox is looking for a Senior Data Engineer to augment our Cyber Security Data Science Team. This growing team supports the Infoblox mission to thwart cybersecurity threats in our customer’s networks. This is an opportunity to work closely with data scientists and threat analysts to curate the data that makes this mission possible.
The ideal candidate is a savvy software engineer with experience in data engineering and a solid background in Spark and Python. Preferably you know that countMinSketch is not a children’s game. You are comfortable wearing several hats in a small organization with a wide range of responsibilities and have worked in a cloud environment, such as Amazon EMR. You know that Big Data is both a blessing and a curse; without good data engineering, it loses its potential. You are passionate about the nexus between data and computer science-driven to figure out how best to represent and summarize data in a way that informs good decisions and drives new products. When someone says, “my Spark job failed”, your first question is “what’s the skew?”.

Come join our growing Cyber Threat Intelligence team and help us build world-class solutions!
Responsibilities:
Curate very large-scale data from a multitude of sources into appropriate sets for research and development for data science, threat analysts, and developers across the company
Design, test, and implement storage solutions for various consumers of the data
Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods
Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics
Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines
Collaborate on design, implementation, and deployment of applications with the rest of software engineering
Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data
Build and maintain tools for automation, deployment, monitoring, and operations
Create test plans, test cases, and run tests with automated tools
Requirements:
5+ years of experience with Python3, and 2+ years experience with Spark. Scala experience is helpful
5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments
3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL
3+ years of experience with developing ETL pipelines and data manipulation scripts
Proficient in Object-Oriented Design and S.O.L.I.D principles
Strong emphasis on unit testing and code quality
Proficient with AWS products (EMR S3, L.ambda, VPC, EC2, API Gateway, etc)
Preferred Experience:
Very strong Python and PySpark experience
Very strong back end development experience
Strong experience with cloud deployments and CI/CD
Experience with virtualization, containers, and orchestration (Docker, Kubernetes, XEN)
Experience with NoSQL Non-Relational databases (AWS DynamoDB)
Education:
MS or BS in Computer Science or a related field, or equivalent work experience required
Perks:
Work with a world-class technology team in a rapidly growing company
A career path with opportunities to grow
Boutique office space with state of the art amenities, located in the heart of Metro Vancouver area; steps from SkyTrain and Metrotown Mall
Cross-functional break room stocked with complimentary snacks and beverages
And many, many more perks!
It’s an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies—and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team—our future looks bright, and so will yours. To check out what it’s like to be a Bloxer, click here.

#LI-AB1"
Senior Software Engineer - Data Pipeline,"Toronto, ON",BenchSci,None,Organic,"BenchSci's vision is to bring medicine to patients 50% faster by 2025. We're doing this by empowering scientists with the world’s most advanced biomedical artificial intelligence to run more successful experiments. Backed by F-Prime, Gradient Ventures (Google’s AI fund), and Inovia Capital, our platform accelerates science at 15 top 20 pharmaceutical companies and over 4,300 leading research centers worldwide. We're a CIX Top 10 Growth company, certified Great Place to Work®, and top-ranked company on Glassdoor.

We are currently seeking a Senior Software Engineer to join our Data Pipeline Team. Reporting into the Data Engineering Manager, you will work on evolving our data models in several styles of datastores, improve internal tooling to allow data self-service, and operationalize production-grade data pipelines.
You Will:
Scale data pipelines to allow data to go from research to platform as fast as possible
Develop data access mechanisms for downstream applications consumption
Manage sources which contain both semi-structured as well as unstructured data
Develop and apply suitable frameworks to detect data drift, and then calibrate and redeploy them to production seamlessly
Collaborate closely with other engineers to solve interesting and challenging data problems
You Have:
5+ years' experience working as a professional developer
Expertise in Python
Expertise with SQL
Expertise in Spark 2.x, Dataset/DataFrame API and performance tuning
Experience with cloud reference architectures and developing specialized stacks on cloud services
Experience with Pandas
Nice to haves, but not mandatory qualifications:
Background in Life Science
Experience with Airflow or other workflow management systems in a distributed setup
Experience with graph data modelling and scaling graph databases
Experience with Kubernetes in production
Experience with technical design and applying architectural patterns
Our benefits and perks:
A compensation package that includes equity options in the company
An annual Executive Health Assessment at Medcan: All employees get the “executive treatment”
Effectiveness coaching for managers: Onsite, personalized coaching from a trained clinical psychologist
Mental health tools and support: Optional mindfulness sessions and a free Headspace account
Complimentary genome sequencing from 23andMe: Find out what your DNA says about your health, traits, and ancestry
Three weeks of vacation, plus another week: Get 15 days to use anytime, and we’re closed Dec 25-Jan 1
Additional days off: Company summer day, your birthday, and earn +1 vacation day annually
Work from anywhere flexibility: Every day right now, and up to 4 days per week once we return to the office
An onsite gym: Keep fit, conveniently, with a Peloton and other great equipment
A great benefits package: Including health and dental

Here at BenchSci, these are our core values:
Focused: We focus on what will drive the greatest impact at all times.
Advancement: We believe in continuous growth, and discovering new ways to do things better. This applies to our product and business, but also to ourselves.
Speed: We recognize that without a sense of urgency, our team, our product and our mission lose their value.
Tenacity: What we’re trying to do isn’t easy, but we hire the best people, and give them the autonomy, tools, and resources to succeed. The hard work is up to them.
Transparency: We believe that sharing diverse ideas and information creates strong teams. Our success stems from research, collaboration, feedback, and trust.
BenchSci is an equal opportunity employer. We value diversity and are committed to fostering an inclusive environment. All four of our cofounders are immigrants to Canada, as are many of our employees. We welcome your fresh perspectives and ideas."
Senior Data Engineer,"Toronto, ON",Myant,None,Organic,"About us:
At Myant, we are creating the world’s first textile computing platform, integrating technology directly into the only thing we’ve been wearing our entire life – clothing. SKIIN is our first consumer facing brand, and SKIIN’s vision is to enhance human ability through connected clothing - think Ironman’s suit, but comfortable. The sensors and actuators embedded within our apparel create your Digital Identity, which will be consumed by those who matter to you - your family members, doctors, coaches, other IoT devices - without you consciously having to think about it. Imagine a world where you walk into your house and the temperature automatically adjusts to your optimal body temperature, the lights adjust to your mood, you can monitor and adjust your everyday lifestyle based on your vital signs, or your doctor is aware of the onset of a disease before you even visit. The line between the digital and physical world is becoming increasingly blurry, and we believe textile is the next medium to bridge that gap.
We’re looking for people who believe in our mission to make wearable technology truly ubiquitous and convenient, so that everyone can benefit from it. We are a cross-functional team solving big challenges at the intersection of fashion, electronics, software, and data science.
Responsibilities:
Test the performance of the algorithms developed by the Data Science team
Leverage native APIs for integration of AWS platforms
Take ownership of all your deliverables and communicate your results to timely project delivery
Prepare reports and some technical documentations

Qualifications Required:
Bachelor’s Degree in Computer Science, Computer Engineering, or equivalent work experience
Proficiency with JavaScript and Python language
Basic knowledge of machine learning algorithm and libraries like PyTorch, Keras, TensorFlow, sklearn
Experience in building RESTful APIs following Micro-Services Architecture
Experienced in NodeJS, PostgreSQL, and GraphQL.
Significant experience in building microservices leveraging various AWS features (AWS Lambda, IAM, SQS, DynamoDB, Kinesis, Redshift, Aurora, EC2, S3, API Gateway, etc.)
Experience in Biomedical signal processing and data mining related to physiological patient data is a bonus
Powered by JazzHR
8SQJftU9nl"
Data Engineer,"Ottawa, ON",Veem,None,Organic,"About Veem
Veem empowers businesses who spend too much time and money dealing with inefficient financial payment systems. Our transparent, relationship-based payments model makes it easy to build trust with your vendors, contractors and customers by providing a quick and seamless payable and receivable process. We make the process even easier for these clients by supporting integration with all major accounting software including QuickBooks, Netsuite, and Xero. Backed by top investors such as Goldman Sachs, Kleiner Perkins and Google Ventures, Veem is a fast-growing financial technology company that is changing the way companies pay and get paid.
Veem is looking for an experienced data engineer to be a key member of our Data Team. The position will be based full-time in Ottawa. You are passionate about leading initiatives to deliver core insights that are rigorous and reliable. Your work will be leveraged throughout the company including product, sales, marketing, and leadership. You have deep technical skills and are excited about building a green field data platform to help make data-informed decisions.
Guiding Principles and Expected Responsibilities
The Data Team's objective is to answer central questions that are foundational for our business: What influences customer behavior? How do these influences impact our business?
Define and develop the program and architecture for data collection, modeling, metrics creation, data validation, model training, and reporting of intelligence.
Create pipelines (data processing, data analysis, optimization, implementation, validation)
Define data schemas and services, focussed on accessibility/use-case for the consuming process.
Understand customer behavior to develop predictive models to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.
What We Believe Will Lead To Your Success
We're looking for someone with 5-10 years of experience in data engineering or platform engineering with an emphasis working with creating data architectures to collect and analyze diverse datasets (e.g. large and small, structured and unstructured, behavioral and self-reported).
Highly effective with SQL and understand how to write and tune complex queries.
Hands-on experience with languages like R, Python or Java for data manipulation.
Working knowledge of relational & analytical databases and distributed systems with tech stack that can include - MySQL, Snowflake, Redshift, Hive/Hadoop, AWS S3 services, GraphDB, Spark, etc.
Experience analyzing data from 3rd party providers like: Google Analytics, Adwords, Segment, Salesforce, Pardot, Mandrill, etc.
Nice to have: Data analysis using tools like: Looker, Tableau, D3, ggplot, etc.
Nice to have: Familiarity with machine learning and statistical approaches (Clustering, Decision Trees, Bayesian, GLM/Regression, Random Forest, Neural Networks, etc.)
BS/BA or greater in Mathematics, Physics, Statistics, Computer Science, Engineering, or another quantitative field.
Perks:
Competitive salary
Comprehensive benefits package (Health, Dental, Medical, Vision)
Group RRSP Plan (after 3 months)
3 weeks vacation
Friday afternoon unwind"
Sr. Data Engineer,"Mississauga, ON",SOTI Inc.,None,Organic,"SOTI is committed to providing its employees with endless possibilities; learning new things, working with the latest technologies and making a difference in the world.
Job Title:
Senior Data Engineer
Location:
Mississauga
Who We Are
At SOTI, we are committed to delivering best in class mobile and IoT device management solutions. We are looking for out of the box thinkers that appreciate the art of creating great software. To us, being visionary is more important than doing things the way they’ve always been done.
What’s in it for you?
The People - From our humble origins in our founder’s basement, to our industry leading position today, SOTI has worked hard to foster a company culture that we can all believe in. A culture that emphasizes personal growth, continuous innovation and fun.
The Growth - Our environment fosters new ideas, fresh perspectives, and the ability to take them over the goal line. SOTI is a fast-paced environment with a global reach that encourages you to make your mark and be part of something big!
The Technology - You’ll get the chance to work with leading edge technologies and take on complex and interesting projects, as part of highly collaborative and agile teams. You will work alongside SOTI’s partners which include leading tech giants that will keep you on the cusp of emerging technologies.
What You’ll Do
Ability to translate and document business requirements into technical documentation, supporting document management and knowledge sharing
Ensure assigned deliverables are within business / audit control requirements
Take ownership of end to end design and all aspects related to development and ensure design and development standards and followed
Create project documentation (Detailed design, Source-to-target mappings, Implementation plans, etc.)
Develop data and database-oriented solutions in order to solve complex business problems leading to data driven decision making
Develop data integration processes to integrate disparate data sets into a cohesive data model in support of BI and analytical requirements
Works closely with data architecture to ensure proper adherence to architectural guidelines and principles
Experience You’ll Bring:
7+ years of hands-on advanced experience designing and developing BI Solutions and providing technical expertise
7+ years hands-on advanced experience using MS SQL
7+ years of experience with MS SQL Business Intelligence Stack (SSAS, SSIS, and SSRS)
5+ years hands-on advanced experience using Power BI or similar BI platforms
Experience using Cloud architecture, NoSQL databases and R/Python
Experience using building data pipelines to integrate with unstructured data sources
Experience in designing and building unstructured data stores using Azure or AWS technologies
Experience with Data Warehouse concepts, including the use of Extract, Transform, and Load (ETL) tools
Excellent analytical, troubleshooting, problem-solving and research skills
Must be able to multitask and have experience with interacting within a diverse user/customer base
Excellent written, verbal, and interpersonal communication skills
About SOTI
SOTI is the world's most trusted provider of mobile and IoT management solutions, with more than 17,000 enterprise customers and millions of devices managed worldwide. SOTI's innovative portfolio of solutions and services provide the tools organizations need to truly mobilize their operations and optimize their mobility investments. SOTI extends secure mobility management to provide a total, flexible solution for comprehensive management and security of all mobile devices and connected peripherals deployed in an organization.
At SOTI, we celebrate the uniqueness of our global teams and are proud to be an equal opportunity workplace. We are curious problem solvers who are committed to bringing the best mobile and IoT management solutions to market. We offer careers with #EndlessPossibilities.
What are you waiting for? Apply today: https://www.soti.net/careers
If you want to bring your ideas to life, apply at SOTI today.
We are committed to providing accessible employment practices that are in compliance with the requirements under the Human Rights Code and the Accessibility for Ontarians with Disabilities Act (AODA). If you require accommodation during any stage of the recruitment process, please notify People & Culture at careers@soti.net .
Please note that SOTI does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed Services Agreement with agency/recruiter, SOTI will not consider or agree to payment of any referral compensation or recruiter fee."
Data Support Engineer,"Toronto, ON",Capgemini,None,Organic,"Duration: 5+ months

Requirement:
Primary Skill:
Talend Data Warehouse Engineer

Talend Data Catalog

Talend Metadata Management

Snowflake Data Warehouse

Secondary Skill:
DevOps

Microsoft Azure

AWS

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
(Senior) Principal Data Engineer - Platform,"Vancouver, BC",Absolute Software,None,Organic,"Description
Do you want to be part of a team that is on the leading edge of innovation in endpoint visibility and control? Absolute is seeking an experienced technical leader with a passion for data, architecture and design. As a senior member of the platform team you will have the opportunity to drive the technical direction of Absolute’s next generation big data platform and champion the use of architecture standards and best practices.
Responsibilities:
Manage the definition of data architecture to ensure that software solutions are built within a consistent framework
Contribute to the evolution of the data and technology architecture, combining technology selection and operational expertise
Partner with teams in building a highly scalable, reliable, resilient and available big data platform
Hands-on development creating proof of concepts for technological innovations and leading their adoption
Participate in technical reviews
Evangelize use of data architecture standards and best practices; mentor and coach
What You Will Need:
BSc degree in Computer Science is minimum, M.S. in Computer Science is preferred
7+ years of Java EE development, working with Hibernate, Spring or similar JPA frameworks
10+ years of experience with a variety of persistence technologies (Relational and NoSQL) optimized for big data processing, querying and analysis. Those would include document data stores (MongoDB), time series (InfluxDB), and full text search engines (Elastic).
Experience building high-throughput, low-latency multi-threaded systems with parallel pipeline processing.
Experience taking a lead role in designing and building sophisticated fault-tolerant distributed systems that have been successfully delivered to customers
Knowledge of cloud IaaS/PaaS and container orchestration tools (Kubernetes, Docker)
Outstanding problem solving and organizational skills
Communication is crucial – so excellent verbal and written communication skills are a must
Continuously looks for ways to improve and sets a very high bar in terms of quality
Why Work For Us:
Headquartered in Vancouver, Canada with international offices in San Jose - CA, Boulder - CO, Ankeny –IA, Austin - TX, Reading - UK and Ho Chi Minh City - Vietnam, Absolute serves as the benchmark for Endpoint Resilience, ensuring connectivity, visibility and control, independent of the operating system – embedded in more than a billion endpoints, we empower devices to recover automatically from any state to a secure operational state without user intervention. Our unique value supports our aspirational journey - to become the World’s Most Trusted Security Company. Nothing short of bold, and nothing less than achievable for this team. Whether it’s our commitment to the cybersecurity industry, our customers, or to one another, we are relentless about protecting people’s devices and the sensitive information found on them. And those common goals foster a work environment where collaboration, big ideas and world-class execution are rewarded with success through our mantra of One Team | One Number. At Absolute, we incorporate the ideals of Resilience in all we do to safeguard our customers’ data and information, so they can focus on saving lives, fighting fraud, moving markets and protecting passengers, to name a few. Our innovation journey has blossomed from within, so we foster that mindset by investing in our employees – fueling our employee’s creative expression, and resulting in our own cyber capabilities. Our momentum is palpable – Forbes noticed too and recognized Absolute as one of the top-10 cybersecurity companies to watch in 2019 and 2020. The New Reality of Remote Work and Distance Learning has further connected our teams and our passion to drive to solve our customers challenges. We pride ourselves on our agile, high energy culture that rewards exceptional achievements and the contributions of those passionate about our collective growth and success. We also respect the need for downtime and believe in a sound work / life balance, reflected in our ‘Take What You Need’ vacation policy and our annual employee retreat where it’s all about friends and family. To learn more about Absolute, visit our website at www.absolute.com or visit our YouTube channel.
Absolute is an equal opportunity employer."
(Senior) Principal Data Engineer - Platform,"Vancouver, BC",Absolute,None,Organic,"Do you want to be part of a team that is on the leading edge of innovation in endpoint visibility and control? Absolute is seeking an experienced technical leader with a passion for data, architecture and design. As a senior member of the platform team you will have the opportunity to drive the technical direction of Absolute’s next generation big data platform and champion the use of architecture standards and best practices.
Responsibilities:
Manage the definition of data architecture to ensure that software solutions are built within a consistent framework
Contribute to the evolution of the data and technology architecture, combining technology selection and operational expertise
Partner with teams in building a highly scalable, reliable, resilient and available big data platform
Hands-on development creating proof of concepts for technological innovations and leading their adoption
Participate in technical reviews
Evangelize use of data architecture standards and best practices; mentor and coach
What You Will Need:
BSc degree in Computer Science is minimum, M.S. in Computer Science is preferred
7+ years of Java EE development, working with Hibernate, Spring or similar JPA frameworks
10+ years of experience with a variety of persistence technologies (Relational and NoSQL) optimized for big data processing, querying and analysis. Those would include document data stores (MongoDB), time series (InfluxDB), and full text search engines (Elastic).
Experience building high-throughput, low-latency multi-threaded systems with parallel pipeline processing.
Experience taking a lead role in designing and building sophisticated fault-tolerant distributed systems that have been successfully delivered to customers
Knowledge of cloud IaaS/PaaS and container orchestration tools (Kubernetes, Docker)
Outstanding problem solving and organizational skills
Communication is crucial – so excellent verbal and written communication skills are a must
Continuously looks for ways to improve and sets a very high bar in terms of quality
Why Work For Us:
Headquartered in Vancouver, Canada with international offices in San Jose - CA, Boulder - CO, Ankeny –IA, Austin - TX, Reading - UK and Ho Chi Minh City - Vietnam, Absolute serves as the benchmark for Endpoint Resilience, ensuring connectivity, visibility and control, independent of the operating system – embedded in more than a billion endpoints, we empower devices to recover automatically from any state to a secure operational state without user intervention. Our unique value supports our aspirational journey - to become the World’s Most Trusted Security Company. Nothing short of bold, and nothing less than achievable for this team. Whether it’s our commitment to the cybersecurity industry, our customers, or to one another, we are relentless about protecting people’s devices and the sensitive information found on them. And those common goals foster a work environment where collaboration, big ideas and world-class execution are rewarded with success through our mantra of One Team | One Number. At Absolute, we incorporate the ideals of Resilience in all we do to safeguard our customers’ data and information, so they can focus on saving lives, fighting fraud, moving markets and protecting passengers, to name a few. Our innovation journey has blossomed from within, so we foster that mindset by investing in our employees – fueling our employee’s creative expression, and resulting in our own cyber capabilities. Our momentum is palpable – Forbes noticed too and recognized Absolute as one of the top-10 cybersecurity companies to watch in 2019 and 2020. The New Reality of Remote Work and Distance Learning has further connected our teams and our passion to drive to solve our customers challenges. We pride ourselves on our agile, high energy culture that rewards exceptional achievements and the contributions of those passionate about our collective growth and success. We also respect the need for downtime and believe in a sound work / life balance, reflected in our ‘Take What You Need’ vacation policy and our annual employee retreat where it’s all about friends and family. To learn more about Absolute, visit our website at www.absolute.com or visit our YouTube channel.
Absolute is an equal opportunity employer."
Data Engineer,"Mississauga, ON",Easton Diamond Sports,None,Organic,"Do you have what it takes to win?
Like a championship team, a leading global sports brand is built with a solid foundation of players at all levels who have an unending desire and dedication not only to succeed, but also to win. At Peak Achievement Athletics, our championship team is deeply committed to developing the most innovative sports equipment in the industry and we are always looking to strengthen our roster with talented players.
Want to join our team as a Data Engineer?
The Data Engineer is responsible for developing and delivering a large-scale database platform that can efficiently enable our analysts to transform data into intelligence. The system design will dramatically reduce the time spent on data preparation tasks and have the inherent ability to scale with business growth and complexity. The Data Engineer will become intimately familiar with the architecture of Bauer Hockey systems & enterprise data structures, and be responsible for diving deep into code while simultaneously developing UI solutions for the Sales Operations super users. The Data Engineer will liaise with multiple technical teams and business teams across international geographies. The database management system will operate on a global scale driving automation and scaling for the wider organization. The role of the Data Engineer will include incorporating data management best practices into the scoping, design and development of the database. The Data Engineer will also be responsible for effectively organizing testing, implementation, support, and the development of user and technical documentation such as guidelines or instructions as necessary.
Qualifications:
Bachelor's degree in Computer science or a related field (MBA a plus) with 3+ years of practical work experience or the equivalent combination of education and experience.
Comprehensive knowledge of database technologies including, but not limited to Google Cloud, AWS, SQL, Hadoop, SAP HANA, and Alteryx.
Hands-on experience developing platforms that translate big data into business insight.
Strong knowledge of data structures and operating systems.
Knowledge of database maintenance and administration techniques.
Experience with Machine Learning languages is a plus.
Desire to work in a high-paced environment.
Strong problem-solving skills and the ability to work independently.
Strong written and verbal communication skills.
Interested yet? Good. Us too. We're pretty sure you'll want to know we offer one of the most generous benefits packages around. Things like a 401(k) retirement plan, casual work environment, and a host of other perks we don't have room to mention here.

We're interested in learning more about you and appreciate you taking the time to apply online at www.bauer.com.
Only those persons chosen for an interview will be contacted.
Peak Achievement Athletics is committed to employing a diverse workforce."
Big Data Engineer,"Toronto, ON",Rackspace,None,Organic,"As a full spectrum AWS integrator, we assist hundreds of companies to realize the value, efficiency, and productivity of the cloud. We take customers on their journey to enable, operate, and innovate using cloud technologies – from migration strategy to operational excellence and immersive transformation.

If you like a challenge, you’ll love it here, because we’re solving complex business problems every day, building and promoting great technology solutions that impact our customers’ success. The best part is, we’re committed to you and your growth, both professionally and personally.

Overview

Our Big Data Engineers are experienced technologists with technical depth and breadth, along with strong interpersonal skills. In this role, you will work directly with customers and our team to help enable innovation through continuous, hands-on, deployment across technology stacks. You will work to build data pipelines and by developing data engineering code ( as well as writing complex data queries and algorithms.

If you get a thrill working with cutting-edge technology and love to help solve customers’ problems, we’d love to hear from you. It’s time to rethink the possible. Are you ready?
What You’ll Be Doing:
Build complex ETL code
Build complex SQL queries using MongoDB, Oracle, SQL Server, MariaDB, MySQL
Work on Data and Analytics Tools in the Cloud
Develop code using Python, Scala, R languages
Work with technologies such as Spark, Hadoop, Kafka, etc.
Build complex Data Engineering workflows
Create complex data solutions and build data pipelines
Establish credibility and build impactful relationships with our customers to enable them to be cloud advocates
Capture and share industry best practices amongst the community
Attend and present valuable information at Industry Events
Traveling up to 50% of the time
Qualifications & Experience:
3+ years design & implementation experience with distributed applications
2+ years of experience in database architectures and data pipeline development
Demonstrated knowledge of software development tools and methodologies
Presentation skills with a high degree of comfort speaking with executives, IT management, and developers
Excellent communication skills with an ability to right level conversations
Technical degree required; Computer Science or Math background desired
Demonstrated ability to adapt to new technologies and learn quickly
#Onica

About Rackspace Technology
We are the multicloud solutions experts. We combine our expertise with the world’s leading technologies — across applications, data and security — to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.

More on Rackspace Technology
Though we’re all different, Rackers thrive through our connection to a central goal: to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know."
Data Engineer,"Toronto, ON",Affinity Staffing,None,Organic,"Requirements:
Experience working on building data pipelines, ingesting and transforming multiple events per minute and terabytes of data/per day.
Passionate about producing maintainable, clean and testable code, as part of a real-time data pipeline.
Understanding of how microservices work, and familiar with the concepts of data modelling.
Ability to connect different services and processes together, even if you have not worked with them before, and follow the flow of the data through multiple pipelines to debug any data issues.
Experience working with Kafka and Spark before, and have experimented, or heard about Druid/Flink/Ignite/Athena/Presto and understand when to use which one.
Ability to understand issues with ingesting data from applications in multiple data-centres, across different geographies, cloud and on-premise and will find a way to solve those issues.
Proficient in Scala/Java/Spark/Python

About Affinity:
Affinity is a full service Information Technology agency that takes a unique approach to recruiting. We believe recruiting is about creating long term relationships that foster a mutually beneficial partnership - an affinity. Bringing a new style of recruiting founded on four core principles – Transparency – Flexibility – Efficiency – Agility.

#AFF1

Experience : 5 - 10 Years"
Data Engineer/Analyst,"Montréal, QC",RCN Call Center Services,None,Organic,"Data Engineer/Analyst
We are looking for a Data Engineer/Analyst to join our growing technology team in the exciting space of financial services. Our technology integrates big data, analytics, data science and machine learning with distributed computing architectures to deliver a suite of data-driven web and mobile applications. This position is located in our Montreal office.
Role Description
This position focuses on the design, implementation, and operation of data management systems to meet our data-driven business needs. This includes designing how the data will be stored, consumed, integrated, and managed by different data entities and digital systems. You will work closely with business and operations stakeholders to determine, create, and implement systems to gain insights, support decisions, and prescriptive analytics.
You will help plan, design, and optimize for data acquisition, throughput and query performance issues. This requires constantly updating your expertise in technologies such as cloud services and platforms, AI/ML technologies, infrastructure management, and more in building true data-driven value. You will play a key role in the designing and developing real-time and batch processes and pipelines for predictive and prescriptive modeling and analytics to support the insights and analytics required by the business.
Duties & Responsibilities
Create and maintain optimal data pipeline architecture using databases, services, and data warehousing technologies.
Assemble large, complex data sets that meet functional and non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Build analytic tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the executive, product, development and design teams to assist with data-related technical issues and support their data infrastructure needs.
Requirements & Characteristics
The key success criteria for this position is solving data-based analytical problems by combining traditional database skills and machine learning skills.
Three years of professional experience in a data engineering or analytics role.
Excellent collaborative, team-oriented communication skills, able to extract technical requirements from business-level requirements.
Excellent problem solving and critical thinking skills.
Expert-level understanding of integrating AI/ML solutions with web application architectures.
Excellent understanding stream processing and event-driven fundamentals.
Excellent skills working with relational databases such as MySQL or PostgreSQL.
Experience with NoSQL and in-memory databases a strong plus.
Strong experience with managing ML and data systems on Linux.
Please provide Resume/CV in English
Job Types: Full-time, Permanent
Benefits:
Casual Dress
Dental Care
Extended Health Care
Life Insurance
On-site Parking
Paid Time Off
Vision Care
Schedule:
Monday to Friday
Experience:
specific, professional data analytics: 3 years (Required)
specific, professional machine learning: 1 year (Required)
Location:
Montréal, QC (Required)
Work remotely:
Yes, temporarily due to COVID-19"
Data Engineer,"Toronto, ON",SADA,None,Organic,"Join SADA as a Data Engineer!
Your Mission
As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ.
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. May involve conversion between relational and NoSQL data stores, or vice versa.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
About SADA
Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Senior Data Engineer,"Toronto, ON",SADA,None,Organic,"Join SADA as a Sr. Data Engineer!
Your Mission
As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success
#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.
Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.
As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
About SADA
Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.
Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
IT Expert - Data Science (MLOps) Engineer,"Mississauga, ON",Roche,None,Organic,"Position-IT Expert- Data Scientist
Location- Mississauga
Duration- Permanent
Department- Global IT Solution Centre
As an IT Expert - Data Scientist, you will be working as a part of a squad that is designed to respond to the business needs of our organization quickly. The specific role will be to perform Data Modeling analysis, run the Data Analysis and find patterns.
Your primary responsibilities:
Research and implement comprehensive statistical, mathematical and computing science algorithms across business contexts including research, development and quality.
Investigate available tools and technologies in machine learning and deep learning
Support collaboration with third parties including academia and industry.
The skills we are searching are amongst others:
Analyzing large amounts of data from several sources
Knowledge of classification techniques
Good knowledge of data matching/entity resolution techniques
Superb analytical and conceptual thinking skills, attention to details
Strong problem-solving skills (taking a significant, complex problem and breaking it down into components, involve others as needed, drive resolution)
Excellent communication skills/ability to interact with business stakeholders
Ability to solve problems by using machine learning or deep learning techniques
Quick learner and passionate about continually adapting your skills and knowledge
Ability to work in the interdisciplinary area and can interpret and translate the very abstract and technical approaches to a healthcare and business-relevant solution
Active team player and can work effectively in a collaborative, fast-paced, multi-tasking environment
Excellent communication skills and a demonstrated ability to interact with different teams and parties of the organization, such as business, technical and academic partners
Knowledge of R or Python is essential
Knowledge of Cloud environments (GCP, AWS) would be an asset

This position is not eligible for relocation support.
This position is open to applicants legally authorized to work in Canada.
NOTE: All employment is conditional upon the completing and obtaining a satisfactory background check, including educational, employment, references and criminal records (for which a pardon has not been granted) checks.
AGENCY NOTICE: Please note that Roche Canada does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed Services Agreement with agency/recruiter, Roche Canada will not consider or agree to payment of any referral compensation or recruiter fee. In the event a recruiter or agency submits a resume or candidate without a previously signed agreement, Roche Canada explicitly reserves the right to pursue and hire those candidate(s) without any financial obligation to the recruiter or agency.
Roche is an equal opportunity employer.
Information Technology, Information Technology > Application Development / Programming"
"Senior Software Engineer, Data Analytics","Toronto, ON",Autodesk,None,Organic,"Job Requisition ID #
20WD41615
Position Overview
As a global leader in 3D design, engineering, and entertainment software, Autodesk helps people imagine, design, and create a better world. Autodesk enables better design through an unparalleled depth of experience and a broad portfolio of software to give customers the power to solve their design, business, and environmental challenges. In addition to designers, architects, engineers, and media and entertainment professionals, Autodesk helps students, educators, and casual creators unlock their creative ideas through user-friendly applications.

The XA Product Analytics Team is a centralized Analytics team working closely with product line development teams across the company to define, implement and evolve Autodesk’s Product Analytics best practices. We help those teams with all their internal and customer facing Analytics needs by way of instrumentation, experimentation, reporting, analysis, in-product analytics and machine learning. We are looking for technically savvy data engineers and analysts with a passion for data analysis and who are eager to help us bring data driven thinking in day to day product engineering practices. We are seeking individuals who are attracted by complicated problems, who can think creatively and work hard to help us solve them.
Our Team
Our team is part of a larger Experience Design and Analytics (XA) team that provides design leadership and excellence across all product lines at Autodesk. Key disciplines include experience design, visual design, research, content strategy, and program management. Together, we support Autodesk’s growing design community, and share a common goal of being customer-centric, with connected experiences across our products.

Responsibilities
Experience with translating business requirements into suitable data schemas and managing meta data for data models.
Collaborate and work with stakeholders; to address data related problems in regard to systems integration and compatibility.
Determine best patterns to store and access data in line with usage of the system and transactional needs
Create and optimize our data pipeline architecture
Build data access platforms for data scientist and analysts
Implement ETL processes through cloud-based solutions (S3, Redshift)
Develop large-scale data structures for business intelligence analytics by using data mining tools
Write processes to ingest, transform and distribute data into our internal applications
Improve product instrumentation to ensure analysis objectives can be accomplished
Provide ad hoc queries and analysis as needed

Minimum Qualifications
3+ years professional experience
BS or MS in CS or similar
Strong experience with databases and writing and debugging SQL
Experience working with big data environments such as Athena, BigQuery, Snowflake, Hadoop (Hive, Spark)
Experience with data pipeline and workflow management tools: Oozie, AWS Glue, Airflow, Azkaban, Airflow, etc.
Experience working with BI tools such as QlikView, Tableau, and Looker
Experience with cloud-based data solutions (AWS preferred)
Experience with automation/configuration management/enterprise schedulers
System monitoring and alerting, dashboarding experience
Working knowledge of code and script (Java, Python, JavaScript, bash)
Production-level coding experience
Prefer experience with graph databases
At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.
Are you an existing contractor or consultant with Autodesk? Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact Autodesk Careers ."
Machine Learning Engineer/Data Scientist,"Toronto, ON",Crescendo Technology,None,Organic,"About the role:
We are looking for a candidate that will be responsible for developing algorithms which will form the basis of our mathematical models for our understanding of sports betting markets. These models will be used for automation. The candidate MUST have a strong background in Machine Learning and Algorithm Development experience.
Our ideal candidate should have:
Degree/Diploma in Computer Science/Software Engineering/Statistics or equivalent
3+ years of relevant experience with R or Python (NumPy, SciPy, Pandas, etc)
2+ years of relevant experience as Software Developer, preferably using Microsoft .NET Framework (C# or VB.NET)
Strong background in statistics
Experience with Machine Learning algorithms and Probabilistic Models
Experience using cloud computing platforms such as EC2 (AWS)
Experience with modern R packages and technologies such as dplyr, tidyR, data.table, shinyR
Domain experience in on-line gaming industry, financial markets or other 2-sided markets is a plus
Experience with Neural Networks or Deep Learning on large problems, Hadoop, MapReduce or High Performance Computing is a plus
Experience in SQL and SQL server is also a plus
We offer:
An environment passionate about growth and learning
Competitive salary with bonus
Fitness subsidy program
Workplace that is conveniently located along the Yonge/Sheppard line
What we are looking for:
This is a key role within the team and would suit someone who is passionate about working with data/data science. We are looking for someone with strong background in statistics, modelling and algorithms (machine learning or other) and who has the ability to convey complex information through data visualization. A thorough understanding and passion for sports and sports betting markets is ideal. Experience with cloud computing as well as python is a plus.
The above is intended to describe the general nature and level of work being performed. They are not intended to be an exhaustive list of all responsibilities, duties and skills required.
Crescendo Technology thanks all candidates applying but only those selected for an interview will be contacted. Selected candidates may be asked to complete an on-line technical assessment.
Crescendo Technology is an equal opportunity employer which values diversity in the workplace and we encourage candidates to apply directly and provide a copy of an updated resume. Should you require an accommodation for the recruitment/interview process, please do not hesitate to reach out to us.
To apply please send your resume with cover letter preferred to hr@crescendotechnology.com
Job Features
Job Category
Development"
"Consultant, Data Engineer - Toronto","Toronto, ON",Avanade,None,Organic,"Data Engineer

Do you love making sure that information is accessible and easy to use? So do we.

You are a data designer who knows how to find, store, and present a range of information from different sources so that everyone can access what they need quickly and simply, and use it effectively.

About you:
You draw on your experience in bringing data to life to aim sometimes complex problems, and you’re able to use concepts around storing, transforming, and visualizing data along the way.

About the job:
As a Data Engineer, you know the importance of data to business. You design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.

Day-to-day you:
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Your skills:
You have got a great experience in data and analysis, and how to source, store and share information. You’re a problem solver who’s happy to work autonomously and to share their knowledge and skills, as well as guiding other team members.

Your skills and experience include:
Strong knowledge of Python, Spark, and T-SQL
Database, storage, collection and aggregation models, techniques, and technologies - and how to apply them in business
Experience in structured problem solving
Strong knowledge of Python, Spark, and T-SQL
Superb communication skills
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
Desirable skills:
Knowledge of Azure tools such as Azure Data Factory, Azure Data Lake, Azure SQL DW or Azure SQL
Knowledge of Big Data tools such as Hadoop / Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics
Experience preparing data for Data Science and Machine Learning
Crafting and building Data Pipelines using streams of IoT data
Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals
You’re likely to have a Bachelor’s degree in IT, Applied Mathematics, Statistics or another meaningful field, or an equivalent combination of education and experience. You also have several years of relevant professional experience."
Senior Full Stack Software Engineer - Data Insights,Ontario,CircleCI,None,Organic,"CircleCI is looking for a senior, full-stack engineer to help us build the exciting next generation of our Insights product. With Insights, we're leveraging the wealth of data CircleCI has to offer - through data aggregation services, APIs, and UIs - to help customers make better engineering decisions. As a senior member of our Insights Engineering team, you will work closely with product, design, and your engineering teammates to build, test, and rapidly iterate on a product that will make a huge impact in how CircleCI's customers build the next generation of software.
About the Team
The Insights team is working on a greenfield product that provides our customers an avenue to track success/failure rates, throughput, and mean time to recovery, as well as duration metrics, credit burn and more for all of their jobs, workflows, and pipelines. The Insights service truly spans the full stack from a front-end micro-application through to our event-driven back-end service, supported by RabbitMQ queues.
What you'll do:
Bring an API-first approach to the development of new features in our Insights product and simplify and scale our systems while we rapidly grow and evolve across backend services, APIs, and UIs.
Interact with product management and designers helping to brainstorm on new features and working closely with your engineering teammates on building those features
Write plenty of sustainable, testable, high-quality code.
Learn about and participate in a culture of observability and monitoring: using operational data to help the team improve our systems' stability and performance.
What we're looking for:
We're looking for someone who enjoys collaboration, is curious and interested in learning, brings strong communication and teamwork skills, and participates in a highly collaborative culture by sharing their expertise and encouraging best practices. If this sounds like you, here's the additional experience we're looking for:
Experience writing and deploying web application code anywhere within the technology stack (we use React, Typescript, GraphQL and Clojure, but it's ok if you have not used them yet).
You write code that's easily readable, testable, and maintainable.
Demonstrable experience building applications and services - primarily those with user interfaces - using well-accepted design patterns to allow for iterative development.
You're excited by working within a small, rapidly-growing team: adjusting to changing priorities, quickly learning new skills, and growing through collaboration.
Experience in the day-to-day practices of continuous delivery and agile development.
Why you'll love working here:
We value transparency and collaboration across distributed teams.
We favor regular, incremental delivery of value over perfection.
We encourage continuous learning and improvement for teams and team members
Working remotely at CircleCI
We're a distributed company with teammates across the world. For this role, we can support you working remotely anywhere in Canada or Ireland.
CircleCI Engineering Competency Matrix
This role equals level E3 - Senior Software Engineer - on our Engineering Competency Matrix, our internal career growth system for engineers. These are the minimum expectation for this position, but we are always willing to discuss bringing people on at more senior positions when appropriate. Find more about the matrix in this blog post.
We know there's no such thing as an ""ideal"" candidate - we're all a work in progress and are growing new skills and capabilities all the time. CircleCI welcomes those who are enthusiastic about learning and evolving, so however you identify and whatever your background, if this looks like a role where you could do work that excites you, we hope you'll apply.
About CircleCI
CircleCI is the world's largest shared continuous integration and continuous delivery (CI/CD) platform, and the central hub where code moves from idea to delivery. As one of the most-used DevOps tools that processes more than 1 million builds a day, CircleCI has unique access to data on how engineering teams work, and how their code runs. Companies like Spotify, Coinbase, Stitch Fix, and BuzzFeed use us to improve engineering team productivity, release better products, and get to market faster.
CircleCI is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law."
"Sr. Consultant, Data Engineer - Ottawa","Ottawa, ON",Avanade,None,Organic,"Do you enjoy making sure that information is accessible and easy to use? So do we.

You're a data designer who knows how to find, store and present a range of information from different sources so that everyone can access what they need quickly and simply, and use it effectively.

About you

You draw on your considerable experience in bringing data and statistics to life to solve sometimes complex problems, and you're comfortable looking after several projects at once. You're able to make your own decisions while at the same time supporting more junior team members.

About the job

As a Senior Consultant, Data Engineering, you know the importance of data to business. You design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.

Day to day, you will:
*

Use your knowledge to plan and deliver data warehouse and storage
*

Take part in designing and running bespoke data services for individual projects
*

Stay up to date with business best practice in using and retrieving data
*

Design, develop, adapt and maintain data warehouse architecture and relational databases that support data mining
*

Customize storage and extraction, metadata, and information repositories
*

Create and use effective metrics and monitoring processes
*

Help to develop business intelligence tools
*

Support deal teams by providing subject knowledge and solutions for client proposals
*

Author reports that include key performance indicators, show where current operations can be improved, and identify the causes of any problems
*

Create and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
*

Travel as required.

Your skills

You're got great experience in data and analysis, and how to source, store and share information. You're a problem solver who's happy to work autonomously and to share their knowledge and skills, as well as guiding other team members.

Your skills and experience include:
*

Database, storage, collection and aggregation models, techniques and technologies - and how to apply them in business
*

Employing statistical and data visualization tools and techniques
*

Experience in structured problem solving
*

Great project and people management
*

Using SharePoint, PowerPivot, SRRS, Excel - including pivot tables and macros
*

Working with SQL.

You're likely to have a Bachelor's degree in Applied Mathematics, Statistics or another relevant field, or an equivalent combination of education and experience. You also have five to seven years of relevant professional experience."
Data Scientist/Machine Learning Engineer,"Toronto, ON",Staffinity Inc.,"$80,000 - $120,000 a year",Organic,"About Staffinity Inc.
We are your trusted bilingual recruiting, short and long-term staffing service provider. Our primary goal has been matching sought-after employers with talented candidates. We are your source to get you in front of desirable employers who are now hiring in your area. Please visit us at https://staffinity.ca
Why Work With Us?
We are awesome: We push the boundaries of new technology and are always trying to stay ahead of the curve
We have a great team dynamic with a more client centered approach
We offer competitive salaries and great bonuses and perks!
Staffinity is looking for an Data Scientist/Machine Learning Engineer for a permanent, full time position in Toronto ON. The work will be done primarily with R.
Responsibilities:
Develop math models and algorithms
Evaluating state-of-the-art statistical modeling and Machine Learning approaches using historical data
Deploy models using .NET framework (C#)
Data Analysis, Visualization and Modeling with large datasets
Data Acquisition, Cleaning, and Transformation
Qualifications:
Degree or Diploma in Computer Science, Software Engineering, or equivalent experience.
5+ years .NET Development experience.
Devops experience is a strong asset
5+ years of experience as Software Developer, preferably using Microsoft .NET Framework (C# or VB.NET)
5+ years of experience with statistical programming languages, R strongly preffered
Betting or markets experience is a strong asset
Job Types: Full-time, Permanent
Salary: $80,000.00-$120,000.00 per year
Additional pay:
Bonus Pay
Benefits:
Company Events
Dental Care
Extended Health Care
Flexible Schedule
On-site Parking
Paid Time Off
RRSP Match
Vision Care
Schedule:
8 Hour Shift
Day shift
Monday to Friday
No Weekends
Experience:
.NET: 2 years (Required)
AWS: 2 years (Required)
Machine Learning algorithms and Probabilistic Models: 2 years (Required)
R: 3 years (Required)
2 sided market: 2 years (Required)
Work remotely:
No"
Data Engineer,"Vancouver, BC",Skillz Inc.,None,Organic,"About Skillz:
Skillz is driving the future of entertainment by accelerating the convergence of sports, video games and media for an exploding mobile-first audience worldwide. The company's platform empowers mobile game developers and players with democratized access to fun, fair and skill-based competition for real prizes, shifting the paradigm to make eSports accessible to anyone, anywhere with a mobile device.

Skillz helps developers build multi-million dollar game franchises by turning content into competitive social gaming properties for the world's 2.6 billion gamers. The company has already worked with 13,000 game developers, leveraging its patented technology to host over 800 million tournaments for 18 million players worldwide.

This year, Skillz was recognized as one of Fast Company's Most Innovative Companies and CNBC Disruptor 50 (for the second time). In 2018, Skillz was listed as one of Forbes' Next Billion-Dollar Startups and Entrepreneur Magazine's 100 Brilliant Companies. In 2017, Inc. Magazine ranked Skillz the No. 1 fastest-growing private company in America.
The company is backed by leading venture capitalists, media companies, and professional sports luminaries, ranging from Liberty Global, Accomplice, Wildcat Capital, Telstra Ventures, and a founder of Great Hill Partners to the owners of the New England Patriots, Milwaukee Bucks, New York Mets, and Sacramento Kings.
Who we're looking for:
You're ready to take the next step in your Data Engineering career - to a fast-moving, successful company building out their next-generation streaming analytics infrastructure! You love data consistency and integrity. You consider yourself scrappy and a technologist, passionate about data infrastructure... with your attention to detail and insistence on doing things correctly, you know you can make a big impact on a small team! You're an excellent communicator and know that you grow faster from being able to mentor others.
What You'll Do:
Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture
Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure
Building data integration toolkit for backend services
Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection
Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people
Improve monitoring and alarms that impact data integrity replication lag
Support our product development team in creating new events to measure/track
Your Skillz:
Basic Qualifications:
At least 1+ years of experience in Scala/Java or Python programming
AWS data products (Data pipelines, Athena, Pinpoint, S3, etc)
Experience deploying data infrastructure
Experience with recognized industry patterns, methodologies, and techniques
Bonus:
Familiarity with Agile engineering practices
1+ years experience on Kubernete, Helm chart
1+ years of experience with Spark, Scala and/or Akka
1+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies
1+ years of experience working with Kafka or similar data pipeline backbone
1+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
1+ years' experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus)
At least 1 year of experience with Unix/Linux systems with scripting experience
Familiarity with Alooma, Snowflakes
Familiarity with Kinesis, Lamda
Prior experience in gaming
Prior experience in finance
Skillz embraces diversity and is proud to be an equal opportunity employer. As part of our commitment to diversifying our workforce, we do not discriminate on the basis of age, race, sex, gender, gender identity, color, religion, national origin, sexual orientation, marital status, citizenship, veteran status, or disability status."
"Big Data Engineer, Omnia AI Vancouver","Vancouver, BC",Deloitte,None,Organic,"Job Type: Permanent
Primary Location: Vancouver, British Columbia, Canada
All Available Locations: Vancouver

Learn from deep subject matter experts through mentoring and on the job coaching.
Partner with clients to solve their most complex problems.
Be empowered to lead and have impact with clients, our communities and in the office.

“At Deloitte we value the opportunity to network and build relationships with skilled individuals, even in periods where we are not actively hiring. If you would like to apply to this future opportunity role, and have the required qualifications, you can expect to be contacted by the recruitment team within a few days""
You love to wrestle down data puzzles, you embrace the potential that data represents, you aspire to solve data problems no one else can, and above all, you want to use data to make impacts that matter – if that is you, then Omnia AI is where you want to be.
What will your typical day look like?

As a Big Data Engineer on our Data & Analytics Modernization team within the Omnia AI practice, you are passionate about data and technology solutions, are driven to learn about them and keep up with market evolution. You will play an active role throughout the entire engagement cycle, specializing in modern data solutions including data ingestion frameworks, data pipeline development, Hadoop-based data lake architectures and orchestration. You are enthusiastic about all things data, have strong problem-solving and analytical skills, are tech savvy and have a solid understanding of software development.
Specifically, in this role, you will:
Engineer Big Data ingestion and pipeline frameworks to populate on-premise or cloud-based data lakes
Translate business rules and requirements into data objects, produce associated data models and source to target mappings and write abstracted, reusable code components accordingly
Plan/schedule tasks, lead small development teams, and mentor junior colleagues
Facilitate technical meetings with client staff, and advise client with technical option analyses based on leading practices
About the team

Omnia AI, Deloitte’s Artificial Intelligence (AI) practice is comprised of a collaborative team of experts who use their hands-on experience with cutting-edge information assets to facilitate successful AI transformations. We develop AI-enabled solutions to address all aspects of a client’s transformative journey with disciplined focus on business outcomes.
Our Data & Analytics Modernization team helps clients design and implement the data platform architectures – be it in the cloud or on-premise – required to enable cutting-edge AI solutions. You will be part of a practice to deliver a breadth of solutions to solve our clients most challenging business problems, with a focus on Big Data, BI/DW, Data Integration, Data Governance, Master Data and Analytics applications. Each of these applications leverages a different mix of traditional and innovative technologies to achieve business outcomes.
Enough about us, let’s talk about you

You are someone with:
3+ years implementation experience leveraging Hadoop ecosystem technologies such as HDFS, MapReduce, Pig, Sqoop, Spark, Hive, Kafka, etc. on-premise and/or in the cloud (e.g. AWS, Azure, GCP)
3+ years experience with analysis, design, development, testing and deployment of data pipeline (ETL) services leveraging the Big Data technology stack for batch and/or real-time messaging/streaming environments
Experience writing complex SQL queries, extracting and importing disparate data from source systems, and data manipulation based on requirements
Experience with Agile development methods in data-oriented projects
Completed Bachelor’s Degree (or higher) in quantitative areas such as Computer Science, Information Management, Big Data & Analytics, or related field is desired
If you believe you have what it takes to be a successful member of our team, please apply now. We know your career is important to you and it's important to us, too. This role is just the first step of a highly successful career we can help you build.
The time is right for you to join Deloitte. Get your career off to great start. What impact will you make?
Why Deloitte?
Launch your career with The One Firm where you can make an impact that matters in a way that you never thought possible. With endless opportunities at every turn, and a culture built to support and develop our people to be the very best they can be, Deloitte is The One Firm for you to learn, grow, create, connect, and lead. We do this by making three commitments to you:
You will lead at every level: We grow the world’s best leaders so you can achieve the impact you seek, faster.
You can work your way: We give you the means to be flexible in how you need and want to work, and we have innovative spaces, arrangements and the mindset to help you be wildly successful.
You will feel included and inspired: We create a deep sense of belonging where you can bring your whole self to work.

The next step is yours
Sound like The One Firm. For You?
At Deloitte we are all about doing business inclusively – that starts with having diverse colleagues of all abilities! We encourage you to connect with us at accessiblecareers@deloitte.ca if you require an accommodation in the recruitment process, or need this job posting in an alternative format. We’d love to hear from you!
By applying to this job you will be assessed against the Deloitte Global Talent Standards. We’ve designed these standards to provide our clients with a consistent and exceptional Deloitte experience globally."
Sr. Data Engineer,"Vancouver, BC",Skillz Inc.,None,Organic,"About Skillz:
Skillz is the leading mobile games platform connecting players in fair, fun, and meaningful competition.
The gaming industry is larger than movies, music, and books, with more than 2.7 billion gamers playing monthly and 10 million developers worldwide. Mobile is the fastest-growing segment of the gaming market, expected to increase from $68 billion in 2019 to $150 billion in 2025.
Skillz is pioneering the competitive mobile gaming experience, leveraging its patented technology to power over two billion esports tournaments a year. The company is headquartered in San Francisco and backed by leading venture capitalists, media companies, and professional sports leagues and franchises.
Through its philanthropic initiatives, Skillz has harnessed the power of its platform to transform the way nonprofits engage with donors, enabling anyone with a mobile device to support causes such as the American Red Cross, Susan G. Komen, American Cancer Society, and NAACP by playing in Skillz tournaments.
Skillz has also earned recognition as one of Fast Company's Most Innovative Companies, a two-time winner of CNBC's Disruptor 50, one of Forbes' Next Billion-Dollar Startups, and the #1 fastest-growing company in America on the Inc. 5000.
Who we're looking for:
You're ready to take the next step in your Data Engineering career - to a fast-moving, successful company building out their next-generation streaming analytics infrastructure! You love data consistency and integrity. You consider yourself scrappy and a technologist, passionate about data infrastructure... with your attention to detail and insistence on doing things correctly, you know you can make a big impact on a small team! You're an excellent communicator and know that you grow faster from being able to mentor others.
What You'll Do:
Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture
Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure
Building data integration toolkit for backend services
Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection
Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people
Improve monitoring and alarms that impact data integrity replication lag
Support our product development team in creating new events to measure/track
Your Skillz:
Basic Qualifications:
At least 4-5 years of experience in Scala/Java or Python programming
AWS data products (Data pipelines, Athena, Pinpoint, S3, etc)
Experience deploying data infrastructure
Experience with recognized industry patterns, methodologies, and techniques
Bonus:
Familiarity with Agile engineering practices
2+ years experience on Kubernete, Helm chart
4+ years of experience with Spark, Scala and/or Akka
4+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies
2+ years of experience working with Kafka or similar data pipeline backbone
4+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
3+ years' experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus)
At least 4-5 years of experience with Unix/Linux systems with scripting experience
Familiarity with Snowflake or OLAP
Familiarity with Kinesis, Lamda
Prior experience in gaming
Prior experience in finance
Skillz embraces diversity and is proud to be an equal opportunity employer. As part of our commitment to diversifying our workforce, we do not discriminate on the basis of age, race, sex, gender, gender identity, color, religion, national origin, sexual orientation, marital status, citizenship, veteran status, or disability status."
DATA ENGINEER (SOPHI),"Toronto, ON",The Globe and Mail,None,Organic,"DATA ENGINEER (SOPHI)

POSITION CODE: 2020-063
LOCATION: The Globe and Mail, Toronto
SALARY: Commensurate with qualifications and experience

POSITION OVERVIEW:

We’re looking for experienced individuals with deep knowledge of data streaming, serialization, databases and distributed systems, and proficient in writing custom libraries but also know when to use off-the-shelf solutions when necessary. Ideal candidates are self-motivated engineers with a passion for both business and technology innovation, more importantly they quickly adapt with changing technologies. We value people who are passionate about system design and have an eye for improving product quality. We currently work with Scala, Kotlin, Java, Python, NodeJS, Postgres, Go, Kafka, and Flink.

As a Data Engineer you will:
Develop and optimize system components for maximum performance and scalability across a vast array of environments
Have a commitment to collaborative problem solving, sophisticated design, and product quality
Ensure that system components and the overall application are robust and easy to maintain
Contribute to backlog reviews, technical solutions design and implementations
Be disciplined in implementing software in a timely manner while ensuring product quality isn’t compromised
MINIMUM QUALIFICATIONS:
Strong analysis and problem solving skills
Deep understanding of good programming practices, design patterns, Functional Programming, and Object Oriented Analysis and Design
Successfully implemented and released a large number of data pipelines and web services using modern engineering frameworks in the past 3 years
Formal training in software engineering, computer science or computer engineering.
Worked as part of a mature engineering team
IDEAL CANDIDATE:
Have strong working knowledge with Scala and/or Kotlin.
Understands reactive programming, Threads and Futures.
Successfully implemented realtime and batch analytics using Kafka, Flink, Apache Beams and/or Google DataFlow.
Strong working knowledge of data warehouses include Redshift, Snowflake, and/or Apache Druid.
Have a working knowledge with containerization and build pipelines
Successfully implemented data systems for very large data volumes such as click streams and/or IoT sensors data.

THE GLOBE AND MAIL INC. IS DEDICATED TO EQUITY IN THE WORKPLACE
At The Globe and Mail, we are committed to fostering an inclusive, accessible work environment, where all employees feel valued, respected and supported. The Globe and Mail offers accommodation for applicants with disabilities as part of its recruitment process. If you are contacted to arrange for an interview, please advise us if you require an accommodation."
Data/Software Engineer Co-op,"Toronto, ON",Smart Nora,None,Organic,"The Company
Smart Nora is a Toronto based growth phase company with strong ties to tech, design, hardware, IoT, and consumer health communities. Our focus is on enhancing the sleep and wellbeing of others by inventing practical software and hardware products. Our debut product is the world's most comfortable snoring solution and has been listed on Oprah’s Favorite Things, as well as Good Morning America, TIME, TODAY, and BBC. Good health starts with a great night's sleep.
The Team
We are an ambitious, tight-knit team with an open work environment and a self-directed approach. We work remotely at the moment and have an office at King+Spadina.
The Product
Smart Nora is an over-the-counter, contact-free snoring solution relevant to the 40% of adults who snore. Smart Nora is loved by customers for its comfortable contact-free design that enables users to sleep without any attachments to their face or body.
The Role
We are looking for a Software / Data Engineer. This is a co-op placement, full-time, for the length of your school's coop placement. Ideally we are looking for a 4th-5th year student.
Location
Remote
What you would do
Support the current FW project conducted with our third party project partners by implementing structured testing and documentation
Build supporting tools in Python, Node, or other scripting languages to validate firmware and hardware
Actively participate in Mobile App development project with our third party project partners
Support acoustic performance optimization including microphone and codec gain settings, assisted by automated tests
Be part of the Product team developing the next generation Smart Nora device
Wrangle data and decipher meaning from quantitative tests and analytics
Write clear documentation
Our requirements
Comprehensive understanding of Software Development processes (SDLC)
Comprehensive understanding of Object Oriented Programming (OOP) principles
Experience with Pandas, R, Tableau, or other data processing/visualization tools
Experience with GitHub (send us your profile!)
Exposure to cloud (AWS) and APIs is a plus
Experience in audio processing is a plus
Experience in machine learning is a plus
Majoring in Software Engineering, Computer Science, Industrial & Systems Engineering, or related field.
How to Apply
Please apply below using your resume and links to your LinkedIn, GitHub (if have), and Kaggle (if have) profiles.
Accommodations are available on request for candidates throughout the application process. Please let us know your needs so that we may accommodate. Email careers@smartnora.com if you would like to discuss this role before applying."
CATO - Big Data Engineer,"Toronto, ON",CAPCO,None,Organic,"Data Engineer - Streaming
LOCATION: TORONTO
Capco – The Future. Now.
Capco is a distinctly and positively different place to work. Much more than consultants, we are active participants in the global financial services industry. Our passionate business and technology professionals enjoy a unique environment where they are actively encouraged to apply intellect, innovation, experience and teamwork. We are dedicated to fully supporting our world class clients as they respond to challenges and opportunities in: Banking, Capital Markets, Finance Risk & Compliance, Insurance, and Wealth and Investment Management. Experience Capco for yourself at capco.com
Let’s Talk About You
You want to Own Your Career. You’re serious about rising as far and as fast as your work and achievements can take you. And you’re ready to write the next chapter of your career story: a challenging and rewarding role as a Capco Big Data Engineer.
Let’s Get Down To Business
Capco is looking for talented, innovative, and creative people to join our development team to work on a number of projects and applications with a Data focus within the Digital practice.
Fitting that description, you will also need to be personally motivated to work in a team where clients become colleagues too.
Responsibilities
Produces high quality complex, deliverables with minimal input from stakeholders
Manage full software lifecycle for medium complexity projects from requirements, to design, to implementation, to testing
Develop and maintain back end solutions using cutting edge technologies and products
Work with Scrum Masters and product owners to priorities and deliver solutions using an Agile environment
Build reusable code and libraries for future use and follow emerging technologies
Mentor and train junior developers
Education/Experience
Bachelor’s degree (preference given to Computer Science, Engineering, Gaming and STEM-based majors) or equivalent experience
Five (5) or more years of experience as a Full-stack Data Engineer/developer on Data driven projects
Strong experience in designing and implementing real-time stream processing services such as Apache Kafka
Strong understanding of the full development lifecycle including requirements, architecture, design, development and testing
Strong development experience with Scala/Spark
Experience working with REST APIs/Springboot.
Familiarity working with Java and Hive.
Ability to balance competing priorities in a very dynamic and fast-paced environment
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)
Show Us What You’ve Got
It will be very useful if you have some or all of the following skills:
Understanding of big data and distributed programming concepts
Experience working with ASW, GCloud, Docker, Kubernetes
Experience working with Spring, Akka, Spark
Experience working with Reactive Streams (Rx, Akka, Reactor)
Strong organizational and communication skills
Experience working in an Agile environment
Experience working with code versioning tools
Experience working with build, packaging and continuous integration tools and frameworks

Professional experience is important. But it’s paramount you share our belief in disruptive innovation that puts clients ahead in a tough market. From day one, your key skill will be to perceive new and better ways of doing things to give your clients an unfair advantage.

Now Take the Next Step
If you’re looking forward to progressing your career with us, then we’re looking forward to receiving your application.
Capco is well known for its thought leadership and client-centric model that distinguishes it from other consulting firms. Capco’s strong technology and digital knowledge base, it’s global experience of the Financial Service enables us to deliver projects from strategy through to delivery. We are committed to providing new areas of expertise from which our clients will greatly benefit. We have:
Access to industry-focused talent globally
Ability to leverage best-of-breed, innovative products and solutions for complex architecture and large-scale transformation
Extended global geographic market reach
Ability to capitalize on our client footprint and deep domain expertise within financial services
For more information about Capco, visit www.Capco.com.
Capco is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics."
Intermediate Data Engineer,"Toronto, ON",QUESTRADE INC,None,Organic,"The Intermediate Data Engineer works in one of the agile BI teams.
The ideal candidate will be an experienced Data Engineer that demonstrates in-depth knowledge and understanding of data warehousing, data integration, reporting and business intelligence. Open-minded and flexible and prepared to work in a very dynamic environment, supporting multiple business units.
JOB RESPONSIBILITIES:
Creating, supporting, and maintaining ongoing operational, managerial, and executive business intelligence infrastructure.
Attention to detail, in particular as it relates to compliance and accuracy of data.
Developing understanding of information sources and correct interpretation of data
Gathering, documenting and analyzing requirements from stakeholders
Meeting and interacting with all levels of management as needed to elicit, define, analyze and document requirements for new business intelligence initiatives.
Designing the conceptual, logical and physical data models necessary to support new reporting and data analysis
Developing data integration processes
QUALIFICATION:
Minimum 3 years of related experience.
Understanding of Data Warehouse lifecycle is a must.
Good knowledge in cloud technologies (preferably GCP)
Advanced knowledge in Python scripting language
Good knowledge in Message Broker systems (Kafka, RabbitMQ, PubSub)
Excellent proficiency in writing SQL queries.
Advanced proficiency with Microsoft BI Suite - SQL Server 2014-2019, SSIS, SSRS.
Understanding relational and dimensional data modeling concepts.
Strong knowledge and comprehension of technology and data management used in the process of collecting, storing and retrieving data.
Post-secondary education, preferably in Math/Statistics or Computer Science.
Superior writing, editing, and communication skills, capacity to interact with all levels of the organization.
Knowledge of latest Microsoft self-service BI tools – Power BI (both desktop and cloud) an asset.
Experience with DAX an asset
Experience and/or personal interest in the financial industry an asset."
Data Engineer (Cloud),"Toronto, ON",StackPros,None,Organic,"StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.
The Cloud Data Engineer will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.
StackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros’ quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.
Key Responsibilities:
Data Engineering-Specific Responsibilities
Participate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projects
Design and develop ETL workflows and datasets to be used in data visualization tools
Write complex SQL queries with multiple joins to automate and manipulate data extracts
Perform end to end Data Validation to maintain accuracy of data sets
Build tools for deployment, monitoring and operations
Troubleshoot and resolve issues in the development, test and production environments
Develop re-useable processes that can be leveraged and standardized for multiple instances
Prepare technical specifications and documentation for projects
Stay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and tools
Understand, implement, and automate security controls, governance processes, and compliance validation
Design, manage, and maintain tools to automate operational processes
Data Science-Specific Responsibilities
Perform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insights
Design experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experiments
Create and design dashboards by using different data visualization tools to present reports and insights, and support business decision making
Help the DRVN Intelligence Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future
Qualifications:
3+ years experience in Data Engineering
Understanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.)
Strong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariate
Excellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholders
Candidate must exhibit an analytical, detail-oriented approach to problem solving
Experience with Jira / Atlassian project management tools is an asset
Company-Wide Responsibilities:
Maintain and exceed client satisfaction with StackPros Inc.’s deliverables, day-to-day work and overall value as a partner
Cultivate opportunities for company growth, always seek areas where StackPros Inc.’s role could be expanded
Adapt to ever-changing client needs and expectations
Maintain dedication toward achieving excellence in StackPros Inc.’s delivery against client needs, and overall success as an organization
Be an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner"
Data Engineer,Remote,Wavo,None,Organic,"Wavo is looking for intermediate and senior data engineers to join our technical team.

Digital advertising, technology, & data are changing the way artists & brands approach advertising. With Wavo you will get the rare chance to create services that will be used by today’s top artists, managers and brands to help them grow their business through Wavo’s advertising products.

Wavo’s technical team is small and efficient, and while we work to a high standard we don’t over-manage with specs. We strive to build a workplace where everyone works hard and gets passionate about the big challenges.

We can promise you will never be bored. We are constantly experimenting, testing new approaches and challenging ourselves to master new skills. We try to think outside of the box and build an environment that’s both exciting and meaningful.

Responsibilities:
Collaborate with other engineers and teams to implement new features, improvements and fixes needed to build and extend our advertising, and data management platform

You might be a good candidate if you:
Have a high degree of proficiency with Python.
Very comfortable with SQL. Experience with Postgres is a plus.
Knowledge of Apache Spark & pySpark required. Good understanding of ETL pipelines is a plus.
Familiarity with Docker, CircleCI, and Kubernetes is a plus.
Are passionate about working with data, and building systems that are highly reliable, maintainable, and scalable
Are a good communicator and enjoy interacting with people
Enjoy being part of a highly collaborative, remote enabled environment
Are comfortable having ownership and control of a project.

Also...
Today we use AWS for most of our Data infrastructure, so familiarity with that platform is a huge plus.
Having experience in ad-tech, machine learning, or data ETL /Data ingestion is also to your advantage.
Finally, all our future projects will be done in a continuous deployment fashion; supported by integrations tests / BDD.

Benefits
======
Competitive compensation based on experience
Competitive Equity
Group health and dental insurance plan
Flexible hours and vacation
Free tickets to shows and festivals
Delicious office snacks
Company outings & activities
A dynamic work environment
Being a part of innovation at the nexus of music, marketing/advertising, and technology

-

If you think you’d be a good fit, please contact careers@wavo.me with a copy of your resume.

Equal Opportunity Employer
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Brampton, ON",Capgemini,None,Organic,"Job Description:
3-6 years
Expertise on Spark Scala.
Ability to develop ETL jobs to implement business logic using Scala (Spark Framework)
Conversant with Hive Database, Able to create HQL scripts and work on Hive tables for data analysis
Performance tuning of the existing Hadoop jobs, able to trouble shoot and fix existing bugs.
Good understanding of Oracle Exadata RDBMS, able to profile telecom data residing in Exadata and derive business rules.
Co lace with business, have working session with business to identify and freeze business logic.
Understanding / experience working on scrum based Agile set up.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
DevOps/SRE Engineer (Big Data),"Vancouver, BC",Global Relay,None,Organic,"Your Role::
Global Relay delivers enterprise services to 23,000 customers in 90 countries, including 22 of the top 25 global banks. Our infrastructure teams provide fantastic opportunities to DevOps Engineers, Site Reliability Engineers and System Administrators who are passionate about massively scalable, big-data architecture, with a strong focus on security.

Deploy, update, and monitor distributed systems
Support multiple on-premise environments
Scale and manage infrastructure with high throughput, availability and storage requirements
Automate all the things

Required Skills:

Experienced system administrator of bare metal, VM and orchestrated deployments
Automation tools such as Ansible/Puppet/Chef/Salt/Fabric
A desire to dig deep to troubleshoot, debug and decouple the layers that comprise distributed systems
Scripting (Bash, Python, etc.)
Ability to analyze complex systems and problems and express them in simple terms

Nice-to-have Skills:

Experience researching and advising on new technology implementations
Experience with any distributed data stores (ArangoDB, Aerospike, Cassandra, CockroachDB, Couchbase, Elasticsearch, Kafka, MinIO, Pulsar, Redis, Hadoop, Scylla etc.)
SQL administration (Postgres, MySQL, SQL Server, Oracle)
Containerization experience (Docker, Swarm, Kubernetes)
Networking experience (DNS, VLANs)
Experience with load balancers (HAProxy)
Building and maintaining CI/CD pipelines (Jenkinsfiles)
Monitoring/logging infrastructure (Splunk, ELK, Graphite, Grafana, Prometheus)

About You:

A problem solver who takes initiative
Effortlessly self-motivates while working on team-based projects
A well organized, thorough and detail oriented person
Able to keep the ""bigger picture"" in mind while prioritizing conflicting demands and tasks
Confident enough to voice your opinion, ask questions and not afraid to suggest a better solution, without being abrasive

About Us::
Global Relay is the leading provider of cloud-based archiving, information governance, surveillance, and eDiscovery solutions to the global finance sector. We help financial services firms preserve and supervise their communications data for regulatory compliance, risk mitigation, and litigation readiness. We deliver our services to over 20,000 customers in 90 countries, including 22 of the top 25 global banks. Our market-leading archiving service supports email, instant messaging, text, market data chat, social media, enterprise social networking, voice, trade data, websites, and more. Recently, we also launched a compliant messaging platform specifically for the finance industry.

Our Global Operations & Development Center is located in Vancouver, BC, Canada. In addition, we have offices in eight other cities across the world, including major financial centers like New York, Chicago, and London.

Over the years, we have won several major awards, including:
Company of the Year from the BC Tech Association
Canada’s 50 Best Small and Medium Employers
Canada’s Top Employers for Young People
Canada’s Top 10 Most Admired Corporate Cultures
Canada’s Best Managed Companies – Platinum
Technology Fast 50 – Leadership
We provide fantastic opportunities to individuals passionate about business and technology. For those with international business aspirations, we offer invaluable opportunities for doing business with some of the world’s largest, most influential firms. Our company is also perfect for those who want to create cool technology using massively scalable, big-data architecture, with a strong focus on mobile.

To learn more about our business, culture, and community involvement, visit www.globalrelay.com."
Senior Software Engineer - Data Engineering,"Toronto, ON",Instacart,None,Organic,"OVERVIEW
We're looking for experienced Data Engineers to join our fast moving team. We work on a range of interesting and challenging problems, from supporting thousands of concurrent shoppers and processing millions of data points in real time, to building enterprise grade solutions for our retailer partners to help them understand their customers better. Our platform is complex, rapidly scaling and processing millions of transactions in real-time all the time. There is a tremendous amount of opportunity in front of us, and joining now gives you a chance to grow your career and interests as we succeed.
ABOUT THE JOB
Be part of a small team, with a large amount of ownership and responsibility for managing things directly.
Ship high-quality solutions with a sense of urgency and speed.
Architect, develop, test and maintain big data pipelines and processing systems
Collaborate with Data Scientists, Machine Learning Engineers and Software Engineers and provide recommendations and actionable strategies for performance enhancements and development of best practices
Own a large part of the process to enforce data governance and privacy while improving data quality and reliability
Have the freedom to suggest and drive organization-wide initiatives
ABOUT YOU
5+ years of experience
Bachelor's degree in Computer Science, Computer Engineering, Electrical Engineering, or equivalent work experience
A blend of product, system and people knowledge that lets you jump into a fast paced environment and contribute from day one
Experience with big data tools and databases (Apache Spark, Apache Hive, Presto, Snowflake, PostgreSQL)
Experience with data pipeline and workflow management tools (Snowplow, Azkaban, Luigi, Airflow, etc)
Strong programming skills in Python and/or Go
Extensive experience working with large codebases and cross-functional teams
Experience with cloud native infrastructure (AWS, Docker, Kubernetes, etc)
Excellent written and verbal communication skills; able to effectively collaborate with diverse teams.
Ability to balance a sense of urgency with shipping high quality and pragmatic solutions
Experience in distributed systems and scale
Experience with AI/Machine Learning/Data Modeling"
Senior Data Engineer,"Victoria, BC",BC IMC,None,Organic,"DEPARTMENT DESCRIPTION
The Technology department is responsible for developing technology solutions that contribute to the achievement of BCI’s mission and long-term goals. The department manages the Corporation’s business applications and information technology infrastructure, providing support to a large group of financial professionals. The department is also responsible for authoring technology-related directives and conducting disaster recovery planning to minimize risk to the Corporation’s delivery of investment services.
The Data & Analytics function is responsible for the governance, architecture and engineering of BCI’s data assets. It also provides reporting, insights and data science capabilities to its customers.
POSITION DESCRIPTION
Reporting to the Director, Data & Analytics, the Senior Data Engineer is responsible for the design, development and implementation of the data and analytics products and projects that enable data science efforts in the organization. The Senior Data Engineer will deliver business value to multiple business areas across the organization and works closely with internal Technology and business area stakeholders. S/he will drive data modernization and innovation, and contributes to a strong data and analytics competency for BCI.
The position can be based in either Vancouver or Victoria with travel between the two cities.
QUALIFICATIONS
MUST HAVE:
Bachelor’s Degree in Technology, Computer Science, Mathematics or a related discipline
A minimum of 5 years of experience as a Data Engineer or Software Engineer professional
Experience with data warehouse and data lake design, development and sustainment
Coding skills and deep proficiency with SQL, Python, etc.
Competent with general scripting/software development
Understanding of data processing performance concerns and issues (configuring database server/data schema for performance, optimizing SQL)
Experience with reporting tools (e.g. Excel, Power BI, Tableau)
Experience with version control systems (e.g. Git)
Experience with cloud platforms
Strong knowledge of data modeling, data architecture and data structures
Strong understanding Agile and DevOps, including CI/CD technologies and practices
Excellent listening, communication, collaboration and problem-solving skills
PREFERRED:
Knowledge of the investment management industry
PRIMARY RESPONSIBILITIES
Collaborates with team members, other IT teams, and customers to understand the organization’s business objectives, data needs and infrastructure needs
Provides technical leadership and creates a culture of customer-centricity, accountability and high performance
Designs, develops and implements the data pipelines and ETL tools and workflows that enable data science efforts within the organization
Works with Data Architects and Data Scientists on the design, development and implementation of operational, transactional and analytical modeling
Proactively identifies risks and issues and proposes solutions to remove barriers
Applies knowledge of DevOps practices including continuous deployment, continuous integration, test-driven development and automated testing
Mentors junior engineers, follows best practices, performs code reviews and architects resilient infrastructure
Solves challenging problems about scale, statistics, infrastructure reliability, latency and more
Leads data mining and collections procedures
Robustly sources, structures, profiles, validates and transforms data for reporting, analysis and data science purposes
Engages with stakeholders to define, design and deliver data sourcing, analysis and reporting solutions
Makes recommendations about the methods used to collect, analyse and manage structured and unstructured data to drive outcomes
Develops solutions (and code) to automate and productionize data sourcing, data structuring and analytical modelling
Analyzes data sources, evaluating and remediating data quality, designing and implementing data sets that can be consumed and re-used by the analytics community across BCI
Helping the business interpret the results of analyses to determine the appropriate course of action
Proactively identifies opportunities to utilize data and analytics to business advantage and prototyping for ‘proof of value’
Assists in troubleshooting and guiding resolution of data analytics related problems in a timely and accurate fashion
Undertakes special projects or assignments as required
Performs other related duties as required
COMPETENCIES
Learning Agility
Effective performers continuously seek new knowledge. They are curious and want to know “why”. They learn quickly and use new information effectively. They create and foster a culture of interest, curiosity, and learning.
Relationship Building
Effective performers establish and proactively maintain a broad network of relationships (e.g. colleagues, co-workers, vendors, suppliers, etc.). They value these relationships and work effectively across the organization by maintaining positive working relationships with peers and others.
High Standards
Effective performers possess a high inner work standard and shows pride in their work. They consistently strive to ensure work is complete within deadlines and that all work performed is of a high quality.
Organization & Planning
Effective performers have strong organizing and planning skills that allow them to be highly productive and efficient. They manage their time wisely and effectively prioritize multiple competing tasks. They follow through on tasks to ensure changes in technology are communicated effectively.
Results Orientation
Effective performers maintain appropriate focus on outcomes and accomplishments. They are motivated by achievement, and persist until the goal is reached. They convey a sense of urgency to make things happen. They respect the need to balance short- and long-term goals. They are driven by a need for closure.
Communicativeness
Effective performers clearly and articulately convey technical and other information both orally and in writing to others in a manner appropriate to the listener. They write clearly, accurately and concisely, composing project, technical and other required documentation as required.
Change Mastery
Effective performers are adaptable. They embrace needed change and modify their behaviour when appropriate to achieve organizational objectives. They are effective in the face of ambiguity. They understand and use change management techniques to help ensure smooth transitions.
Business Thinking
Effective performers see the organization as a series of integrated and interlocking business processes. They understand how their work connects with and affects other areas of the organization."
Data Engineer,"Toronto, ON",OMERS,None,Organic,"Why join us?
Are you looking to join a dynamic pension plan that embodies the strong values of its 500,000 members and is an industry leading global investor? If so, we would love to tell you our story.At OMERS we put our people first and are proud to embrace the diversity of thought and leadership that comes from having locations in Toronto, London, New York, Singapore, Sydney and other major cities across North America and Europe. Our culture is truly one of a kind. We get stuff done, and have fun doing it! We take great pride in contributing to the communities where we live with an ever-constant eye to the global investment markets.

OMERS Products and Technology is looking for Data Engineer passionate about all sides of data analytics and eager to build a data platform for our pension Data engineering team. The individual should be extremely motivated and want to constantly learn and apply new technologies. In this role, the qualified candidate is expected to drive actionable insights; transforming, modelling, processing and extracting value from datasets.

As a member of this team, you will be responsible for:
Develop and maintain data pipelines
Design and implement ETL processes
Hands on experience on Data Modeling – Design conceptual, logical and physical data models with type 1 and type2 dimensions.
Knowledge to move the ETL code base from On-premise to Cloud Architecture
Understanding data lineage and governance for different data sources
Maintaining clean and consistent access to all our data sources
Hands on experience to deploy the code using CI/CD pipelines
Assemble large and complex data sets strategically to meet business requirements
Enable business users to bring data-driven insights into their business decisions through reports and dashboards
To succeed in this role, you have:
Experience with data platform cloud technologies for creating data pipelines – preference GCP Cloud(i.e. Cloud Composer, Cloud Dataflow, Cloud Pub/Sub, Big query etc.)
Experience doing semantic data modeling on Analysis service and publish datasets accordingly.
Understanding security rules and roles in projects, workbooks in Tableau or similar
Possess in-depth understanding of SQL, database management systems and ETL (Extract, transform, load) frameworks
Experience with SSIS, Informatica ETL, data modelling, data warehousing, and business intelligence architecture
Knowledge of programming, scripting (e.g. SQL, PowerShell, Python, C#, Power Query, Visual Studio and SSMS)
Proven track record of learning new technologies quickly
Bachelor’s Degree in Computer Science, Engineering or a related technical field
Experience building data visualizations, preferably Power BI/Tableau
4+ years of experience working with data
Our story:
ABOUT OMERS
Founded in 1962, OMERS is one of Canada’s largest defined benefit pension plans, with $109 billion in net assets as at December 31, 2019. OMERS is a jointly-sponsored pension plan, with 1,000 participating employers ranging from large cities to local agencies, and over half a million active, deferred and retired members. OMERS members include union and non-union employees of municipalities, school boards, local boards, transit systems, electrical utilities, emergency services and children’s aid societies across Ontario. Contributions to the Plan are funded equally by members and employers. OMERS teams work in Toronto, London, New York, Amsterdam, Luxembourg, Singapore, Sydney and other major cities across North America and Europe – serving members and employers and originating and managing a diversified portfolio of high-quality investments in public markets, private equity, infrastructure and real estate. OMERS is committed to having a workforce that reflects the communities in which we live and work. We are an equal opportunity employer committed to a barrier-free recruitment and selection process. At OMERS inclusion and diversity means belonging. How we create a sense of belonging is through our employees and our vast network of Employee Resource Groups. Whether you are passionate about gender, pride, or visible minorities, we have groups that are focused on making a difference in all of our lives."
Data Engineer,"Montréal, QC",Coveo,None,Organic,"Delivering a strong foundation for our data-driven teams
At Coveo, it is our constant obsession to find out innovative ways to put our customer's data to work for them. Luckily for us, said customers are pouring in an ever increasing volume of that primordial data which allows our teams to have a lot of material to work with. But all that data is difficult to collect and even more challenging to make sense of without the right data platform. And that is where you come in as a premier data engineer!

What a typical day looks like:


You start your day with a good coffee, reading news about some new stream processing library Netflix has recently released to enable more efficient pipelines. You take a few notes to discuss with your colleagues at the scrum.
After the scrum, you get to work with the In-Product experience team to define the data format and implement a new data pipeline to gather, analyse and learn from all interactions happening in their clients applications. Based on the client's metrics, you estimate the load at several millions of events daily, so you work hard on optimizing and load testing the new data pipeline.
You get to chat with Mike as you go fill your water bottle: the data pipeline you've set for the ML team a month ago really made the data scientists' lives easier. High-fives all around for that!
You get to lunch, and the team wanted to watch the streaming of a presentation of a new AWS feature coming up that might provide new options to scale our platform.
Back to your desk (or some other location) to continue working on the development of the new in-product experience data pipeline using the latest streaming and data processing technologies.
You show your design, submit your code and the results of the tests for review by your colleagues. Great work: the new pipeline can transform hundreds of millions of events daily in real-time without problems.
A new big client is being on boarded by the Commerce team, and they would like to make sure that all the events have been integrated properly. You take a look at what happens under the hood to make sure everything is flawless!
A member of the Usage Analytics team has a question about the dataset she wants to extract and more specifically how to get the right columns for an analysis she is working on. You take a quick 10 min to discuss with her, and then get back to your code.

What is expected of you:
As a Data Engineer at Coveo, we'll encourage you to innovate, and share ideas on a daily basis. We are looking for candidates with at least 3 years of working experience. Some of the core expertise we're looking for:

Extensive knowledge of SQL.
Experience with Kafka, Kinesis or other streaming platforms.
Experience in Data Lake Architecture, and high volume real-time streaming.
Experience in distributed computing and big data.
Knowledge of data warehouses like Redshift or Snowflake.
Knowledge of best practices in CI/CD, as well as in DevSecOps.
Familiarity with AWS.
Strong software development experience with proficiency in at least one high-level programming language (Java, Scala, Python or equivalent)."
Big Data Engineer - Analytics,"Ottawa, ON",Interset,None,Organic,"Help us catch bad guys with math.

Our team is growing and we’re looking to add a Big Data Engineer - Analytics that can focus on extending our existing analytics platform and related capabilities to add unprecedented analytics flexibility for our customers. This will include enabling Data Scientists to manipulate and combine events and models to extend and customize the analytics in ways that provide unique value for each customer.

Although there is a lot of uncertainty in the market today, especially considering the COVID-19 crisis, we are set up to accommodate fully remote work, and a fully virtual interview, selection, and onboarding process.

We are looking for someone who is passionate about what they do, takes a creative approach to problem-solving and will be the champion for creating innovative machine learning hooks that deliver real value and perform in big data environments.

Here’s what you'll do:

Implement model data flows to support running cutting-edge machine learning techniques on massive amounts of data.
Work with product managers and data scientists to turn new features and algorithms into beautiful, battle-tested code.
Work with the technologies we use to analyze and identify cyber-security threats for our customers (Elasticsearch, Spark, HBase, Kafka, Vertica, NiFi, using Java and Scala).
Work side by side with some of the smartest minds in the fields of machine learning and behavioural analytics.
Create efficient and robust cloud-based solutions, leveraging the best in cloud technologies.

In order to be considered, you must have:

An undergraduate or Master’s degree in Computer Science or equivalent engineering experience.
Strong interest in software design, distributed computing, and databases.
Experience developing in a JVM environment (Java, Scala, Clojure).
At least two years of experience developing with or using Big Data & Analytics stacks/tools such as Hadoop, HBase, Spark, Presto, and Vertica.
Experience implementing and using streaming platforms such as SparkSQL, Flink, Kafka, Storm, etc.
Experience with Kubernetes, Docker, Ansible or any other infrastructure or containerization management/automation platform.
Familiarity leveraging AWS EMR, Azure, GCP cloud technologies best practices to enable the distribution and analysis of big data on the cloud would be considered an asset.

We’d also love it if you had the following (though not required):

Familiarity with data science or machine learning packages (pandas, R, TensorFlow, etc…).
Familiarity with virtualization technologies (VMWare ESX, Docker).
Contributions to open-source software (code, docs or mailing list posts).
Interest in understanding and analyzing diverse types of data.

Interset is an equal opportunity employer. Should you require accommodation in any aspect of our selection process, please contact our recruitment team at hiring (at) interset (dot) com.

About Interset:

We use big data and advanced behavioural analytics to detect and prevent the theft of intellectual property...simply put, we catch bad guys with math. Part of the Micro Focus group of companies, we are a fast-paced, all hands on deck kind of environment where you are respected and listened to from day one. We have a startup feel within the stability and structure of a large global company. We hire people with a wide scope of knowledge and experience that want to jump into self-organizing, cross-functional teams. We manage our own schedules, we support our teammates, and we always make time for fun."
Data Engineer,"Engineer, BC",Dufrain Consulting Ltd,None,Organic,"Data Engineer
London, Edinburgh OR Manchester
We are Dufrain. We’re a market-leading Data Management, Analytics and BI consultancy with offices across the UK, working with some of the biggest names in the Financial Services industry. We are proud to be an agile, client focused consultancy where we all share a common goal.

As we continue to build and strengthen our capability, a number of exciting opportunities have arisen to join our market-leading Data Management and Analytics Consultancy. We are looking for dynamic individuals, with proven experience and strong technical skills to join our teams in our Edinburgh, London and Manchester offices.

What we offer you: Working as a Data Engineer at Dufrain you’ll have the opportunity to work with a creative and highly skilled team of consultants who have expertise in technical delivery, technologies and concepts in areas such as Data Storage, Data Ingestion, Data Integration, Data Warehousing, Data Preparation and Cloud Infrastructure. You will also have your own ‘people coach’ to guide and support you with your career journey with us. Dufrain offers an autonomous environment where you have opportunities to have access to training tools & technical community groups and provides multi sector and international client exposure.

Who we’re looking for: We are looking for Data Engineers who have a genuine interest of rich data and have experience on delivery that contributes to wider business outcomes and be able to concisely articulate to stakeholders and interested parties their role and solutions in a way that can be easily understood.


Essential Requirements:

Strong cloud engineering experience
Skilled in multiple languages such as SQL, Python, Java, Scala, Julia
Hands on experience of cloud platforms such as Azure, AWS & GCP - (Certified to practitioner level in at least one provider)
Experience working with one or more of the tools - Spark, Kafka, Snowflake, Hadoop
Experience working with both SQL and NoSQL foundational tools and databases such as Cassandra, MongoDB
Experience delivering multiple solutions using key techniques such as Data Modelling, DWH, Lakes, ETL, ELT, Virtualisation & Streaming
Solid knowledge of development principles such as ETL, DWH & Streaming
Minimum 2 years’ experience as Data Engineer with previous experience in industry or as SQL developer
Flexibility to travel to and work on client sites within the UK and occasionally Europe
Excellent track record in executive stakeholder management and maintaining valuable relationships.
Takes ownership and accountability for critical initiatives and deliverables both internally and for clients
Awareness of current market trends in data having the ability to influence opinion and decisioning across the Data Management spectrum
A passionate desire and attitude to learn new tools


What to do next: : Please apply via the application link below. Once your application has been received a representative from the Dufrain Recruitment team will review your profile and inform you of the next steps.

Please note: The recruitment process will require any candidates that are shortlisted to complete an online technical assessment.





#LifeAtDufrain #GetBusyLiving
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Toronto, ON",International Financial Group,None,Organic,"Job Title: Data Engineer
Location: Downtown Toronto

The individual our client seeks is highly analytical, self motivated, curious and has a track record of quantitative and qualitative analysis. If this sounds like you please apply below!
Our technology client is past the boot strapping stage of a start up and continues to grow at an accelerated rate. The environment is fast paced and at times both structured and unstructured. This organization will provide you with the challenges you are looking for and the continued growth for learning you are looking for in your career. The office environment is fantastic and the client is all about social change. Our client seeks a passionate and talented Data Engineer who is comfortable working in a very fast paced environment.

Responsibilities:
Help build, scale and maintain the data platform.
Play a key role in data infrastructure, analytics projects, and systems design and development.
Extract the data, transform the data and load the data into a database or data warehouse (ETL).
Introduce new technologies to the environment through research and POCs.
Ensure high quality standards are met (documentation is in place; quality checks are working and data in dashboards is updated according to data sources).
Will assure standards to be followed by the data analysts & data scientists in terms of analytical data gathering and transformations.
Delivering an enterprise data platform that will accelerate the delivery of business intelligence, machine learning and the ability to generate new insights.
You will focus on integrations and data modelling, ETL along with the automation of data sets.

Requirements:
BA/BS degree in Mathematics, Computer Science, Mathematics or related technical field, or equivalent practical experience.
Have a deep technical understanding, hands-on experience in distributed computing, big data, ETL, dimensional modeling, columnar databases and data visualization.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools and environments.
Experience with data modelling techniques for modern data architectures.
Hands-on implementation experience with cloud data platforms (e.g. Azure, GCP, AWS).
Experience in writing software in one or more languages such as Java, C++, Python, Go, Ruby and/or JavaScript.
Soft skills: High performer, polished/high work ethic, smart learner, flexible, organized, has initiative/fast producer.
Knowledge of Ruby on Rails, experience building data visualizations with D3 or in JavaScript / React and familiarity with Postgres would be a “Nice to have”

For consideration please email your resume to ""Eileen@ifgpr.com"" with ""Data Engineer"" in the subject line."
Data Engineer,"Toronto, ON",TES - The Employment Solution,None,Organic,"Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and compliance
Improve and extend ETL, data processing, and analytics processes
Facility with PowerBI, including creating dashboards and data sources
Developing high complexity, fast performing SELECT queries.
Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.
Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.
Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.
In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQL
Enthusiasm for applying good data design, testing, documentation, and support practices
Experience building and optimizing data pipelines, architectures, and data sets
Knowledge of message queueing, stream processing, and data stores/warehouses
Working knowledge of AWS products related to data engineering
Bachelor's degree in Computer Science, Software Engineering or an equivalent

Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net"
Data Engineer,Canada,Keyrus,None,Organic,"Keyrus Canada, a leader in Data Intelligence is looking for a Data Engineer.
The Toronto team is expanding rapidly! Our team has doubled in the last two years and is continuing to grow. If you are looking for an innovative startup-style company with a good team-spirit that has the support of an internationally recognized brand, we encourage you to apply and join us!
Who we are:
With offices in 18 countries and more than 20 years of experience in North America, Keyrus is a trusted leader in Data Intelligence.
Keyrus Canada offers stimulating projects to increase companies’ performance, with 2 areas of expertise:
Data Strategy to help organizations identify business objectives, build a strategy, and leverage their data to achieve their goals (BI Roadmap | Master Data Management | Data Governance and Architecture…)
Data Intelligence to enable companies to draw critical insights from their data and shape business decisions (Data Integration | Cloud Migration | Business Intelligence | Analytics & Machine Learning…)
About the role:
We are looking for a Data Engineer expert to join our Toronto office! As a part of the Keyrus team, you will combine your business acumen and statistical knowledge with strong problem solving abilities to analyze large sets of data and deliver insightful, actionable results to our clients.
You will play a key role in providing our clients with business intelligence and ETL solutions to manage their data assets. You will work directly with our client's business to add value to their data intelligence environment through the use of tools such as Alteryx and Tableau. You will be responsible for the entire end to end analytics solution, from backend data engineering, manipulation, and cleansing, to front-end data visualization.
Our ideal candidate:
At least 3+ years of experience as a Business Intelligence Developer, working directly with a tool such as Alteryx, Talend or SSIS for ETL, data manipulation, etc. as well as with a data visualization tool such as Tableau
Experience working with a relational database (SQL, Oracle, etc.) and data modeling
Experience working with business teams to translate functional requirements into technical requirements
Knowledge of data visualization best practices and cloud warehouses (i.e. Snowflake, Redshift) would be an asset
Minimum of a Bachelor's Degree in IT or related field.
What we offer:
A stimulating environment driving you to discover new horizons and surpass yourself
A strong innovative and entrepreneurship DNA
A space that promotes mutual respect, where you can express your ideas and share your opinion
A positive, multicultural work atmosphere and a strong team spirit
Lots of opportunities to celebrate your successes: afterworks, team activities, birthdays, breakfasts and other special events
Benefits such as: insurance, RRSPs, almost full repayment of the transport card, etc."
Machine Learning/Data Engineer,"Toronto, ON",Xanadu Quantum Technologies Inc.,None,Organic,"Summary of Position and Responsibilities

As part of Xanadu’s Machine Learning team, the selected candidate will be responsible for working with a multidisciplinary team of machine learning experts and quantum algorithm developers to bring machine learning models into production. They will develop, deploy, and maintain code, models, and pipelines leveraging various cloud providers and services; automate model training, testing, deployment, and monitoring; and design solution architectures for data driven applications.

Prospective applicants must have strong technical, programming, and mathematical skills. They must possess the ability to evaluate established methods and tools, learn new ones quickly, and apply their knowledge to solve practical problems. Applicants should be self-motivated and demonstrate the ability to successfully meet objectives. Familiarity with quantum computing is not essential for this position, but is a definite plus.

Basic Qualifications and Experience
MSc in Machine Learning, Mathematics, Computer Science, Physics, Engineering, or a related field.
Experience building and deploying production-grade machine learning applications at scale.
Strong software engineering skills across multiple languages (Python, Scala, Java, C++, etc.)
Experience building and supporting development environments for Machine Learning/Data Science teams.
Experience with distributed computing frameworks like Spark, Dask, or Hadoop.
Preferred Qualifications and Experience
PhD in Machine Learning, Mathematics, Computer Science, Physics, Engineering, or a related field.
Solid mathematical understanding of machine learning, statistical modelling, probability theory, and linear algebra.
Experience with frontend and backend web application development.
Passionate about agile software processes, data-driven development, reliability, testing, and continuous delivery.
Familiarity with and experience working in a fast-growing technology start-up environment.
If you are interested in this opportunity, please submit a copy of your CV along with a cover letter outlining why you think this is the right role for you!

Mission (https://xanadu.ai/about)
To build quantum computers that are useful and available to people everywhere. Learn more about our mission here (https://xanadu.ai/about).

Values (https://docs.google.com/presentation/d/1W_9jD_SxlVWMWzRxs1Nx7lHMDPX42aTLCiNTwkUli4s/edit?usp=sharing)
Our values are everything. They are fundamental and lay the foundation for culture at Xanadu. Learn more about our values here (https://docs.google.com/presentation/d/1W_9jD_SxlVWMWzRxs1Nx7lHMDPX42aTLCiNTwkUli4s/edit?usp=sharing).

At Xanadu, we are committed to building an inclusive, safe, and equitable culture and fostering an environment where our employees feel included, valued, and heard. We are committed to meeting the needs of all individuals and support a barrier-free workplace. Should you require accommodations at any point during the recruitment process please contact Human Resources at hr@xanadu.ai."
Data Engineer,"Halifax, NS",MobSquad,None,Organic,"ABOUT MOBSQUAD
We are a well-funded, hyper-growth, scale-up looking for an experienced Data Engineer. If you've ever dreamed of working with a top tier technology company scale-up, on leading edge technologies, backed by the very best venture capitalists in the world, then this is your chance.
Some details about MobSquad:
MobSquad solves the significant and growing technology talent shortage faced by US-based start-ups and scale-ups by enabling our clients to quickly have a turnkey ""virtual"" Canadian subsidiary, where Canadian-based technology professionals work with our clients individually on an exclusive basis
We've been featured on the front page of The Washington Post, on NPR multiple times, The Financial Times (UK), The Globe and Mail, the Calgary Herald, BetaKit, CBC, Global News, and many other places. other media outlets
We're a Certified B Corporation, were recognized as the third Best Place to Work in Canada in 2020, and have made numerous contributions to charitable organizations as well as a financial commitment to the Upside Foundation. We believe we are playing a key role in enhancing Canada's innovation economy, and have received financial support from the Government of Canada, Province of Alberta, Province of Nova Scotia, and City of Calgary, to support this ambition
You can learn more about us on our website
ABOUT THE ROLE
As a Data Engineer, you will be part of a Canada-based team working remotely with a leading US scale-up. Your team will operate alongside many other talented developers and data scientists in Canada, and you will be an integral part of the tech community that MobSquad has built.
This role requires someone who has demonstrated an ability to develop, test, optimize, and maintain scalable databases, architectures, and pipelines that enable data scientists and software developers to easily analyze and work with data. The ideal candidate has worked closely with data scientists and data architects and is an expert at optimizing data flow for use across broader teams. The candidate should be able to generate ideas and create tools that add greater functionality and usability to data systems within the company.
ABOUT YOU
You have a bachelor's degree in Computer Science, Information Technology, Data Science, Applied Math, Physics, Engineering, or a comparable analytical field from an accredited institution
You are expert in modeling, working with database architectures, and relational databases
You have over three years of experience with big data tools, such as Hadoop (MapReduce, Hive, Pig), Spark, and Kafka
You have experience with SQL databases (PostgreSQL, MySQL) and NoSQL databases (Cassandra, MongoDB)
You have experience creating and working with ETL data transformation and integration processes
You have experience working with enterprise-grade cloud computing platforms such as Microsoft Azure, Amazon Web Services (EC2, EMR, RDS, Redshift), and Google Cloud
You have expertise in relevant programming languages (Python, R, C/C++, Java, Pearl, Scala)
You have a deep understanding of data modeling tools (ERWin, Enterprise Architect, Visio)
You have experience optimizing big data pipelines and extracting value from large disconnected datasets
You have the ability to develop high-quality code adhering to industry best practices (i.e., code review, unit tests, revision control)
WHAT YOU'LL GET @MOBSQUAD
A full-time position that offers competitive compensation
A benefits program delivered through our bespoke digital platform, giving you control, choice, and flexibility. We give you the ability to build your package of benefits covering health (e.g., medical, dental, vision), wellness (e.g., gym, workout gear, massage, transit), and RRSP (retirement savings)
A downtown office location with first-rate amenities, surrounded by great restaurants and easily-accessible transit
For international candidates, sponsorship for an immediate work permit, expedited permanent residency, and Canadian citizenship within four years
At MobSquad, we support and encourage building a work environment that is diverse, inclusive, and safe for all. We invite and welcome applicants of all backgrounds, regardless of race, religion, sexual orientation, gender identity, national origin, or disability."
Data Engineer,"Montréal, QC",MarketMuse,None,Organic,"Experienced Data Engineer
Overview
Marketmuse Inc.'s M4 Lab is seeking an experienced Python & Java/Scala engineer to help create the next generations of content analytics and content generation technologies. This role blends production software development, big data, and machine learning. The engineer will work on natural language processing systems that try to understand the semantics, intent, and topical structure of vast amounts of web content at scale. This role requires extensive knowledge and experience in architecting and developing data-intensive applications.
About Us
Marketmuse Inc. is a rapidly growing institutionally-backed content planning technology firm with offices in Montreal, Boston, and New York City. We are the premier provider of enterprise content planning technologies and are recognized as a leading technology for content marketing functions. MarketMuse's new M4 Lab (the MarketMuse Montreal Machine Monograph Lab) will be a hub for our advanced machine learning and data sciences teams working on cutting-edge R&D to improve our systems' quality of content understanding, knowledge representation, and machine learning powered content generation assistance tools. Our software also helps our clients optimize or create content ranging from short blog posts to long whitepapers.
Responsibilities
Design and implement reliably distributed data pipelines
Implement scalable architectures of machine learning prototype solutions geared towards solving natural language processing, knowledge representation, and natural language generation problems
Create parallelized and/or distributed versions of existing algorithms
Maintain and develop existing machine learning models
Collaborate with research scientists, architects and product management to design and program innovative strategic and tactical solutions that meet market needs with respect to functionality, performance, reliability, realistic implementation schedules, and adherence to development goals and principles
Gather and determine requirements for new features from internal colleagues
Required Skills
Experience with architecting data-intensive applications
Experience writing software in Python, Scala and/or Java. Experience working with data structures, algorithms and software design
Knowledge of data warehousing concepts, including data warehouse technical architectures, infrastructure components, tools and environments (such as Apache Beam, Hadoop, Spark, Pig, Hive, MapReduce, Flume)
Experience with test-driven development
Experience working effectively with software engineering teams
Mentor others in achieving their career growth potential
Required Education and Experience Level
At least 5 years of engineering experience with Java/Scala/Python
At least 2 years of distributed or highly threaded software development experience
Preferred But Optional Skills
MS in Computer Science, Computer Engineering or related fields
Experience with workflow tools like Airflow, Luigi, etc.
Experience with data mining or machine learning applications
Hands-on experience implementing new research ideas with a neural network training framework such as Tensorflow, Keras, or PyTorch"
Senior Data Engineer,"Toronto, ON",ASSURANCE,None,Organic,"About Assurance
At Assurance we are disrupting the antiquated and inefficient world of insurance and financial services. Our team of world class software engineers, data scientists, and business professionals are modernizing how people obtain and manage their financial life all through our powerful platform ecosystem. We are rapidly growing as we expand our product offerings and global footprint, and this growth continues to present new and exciting challenges as we push our industry into its future. We eliminate waste throughout the industry and calculate the complex into simple, valuable solutions to improve people's lives. We are humble, driven, and committed to improving the lives of millions.

About the Position
As we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.
To be successful in this role, you must possess the following:
Expertise in modeling data
Experience with Spark, Hadoop/EMR, SQL
Ability to optimize data access for speed/reliability/velocity as needed by the business
Comfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct
Comfort with learning new technologies to help the team explore new solutions to existing problems
A drive to move fast and deliver business value
Excellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.
Business Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.
Enthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.
Bachelors degree in mathematics, statistics, data science or related field of study.
The following additional experience is desired:
Capable of modifying an existing job to add a new field and get it into production within a day.
Capable of creating a new data pipeline/job within 2-3 days.
You have a proven ability to drive business results by building the right infrastructure that enables data-based insights. You are comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes. We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail. You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity. You can work independently, with little oversight or guidance.

At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise. If this sounds like a good fit for you, give us a shout, we’d love to chat!

Please review our CCPA policies here."
Senior Data Engineer- QuantumBlack,"Montréal, QC",McKinsey & Company,None,Organic,"QUALIFICATIONS
Meaningful experience with at least two of the following technologies: Python, Scala, SQL, Java
Commercial client-facing project experience is helpful, including working in close-knit teams
Experience in software engineering best practices such as code reviews, testing frameworks, maintainability and readability
Experience deploying applications into production environments e.g. code packaging, integration testing, monitoring, release management
The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
Meaningful experience in multiple database technologies such as Distributed Processing (Spark, Hadoop, EMR), traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Teradata), NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan)
A confirmed ability in clearly communicating complex solutions
Deep understanding of Information Security principles to ensure compliant handling and management of client data
Experience and interest in Cloud platforms such as: AWS, Azure, Google Platform or Databricks
Confirmed experience in traditional data warehousing / ETL tools (Informatica, Talend, Pentaho, DataStage)
Extraordinary attention to detail
Flexibility to travel regional or internationally up to 80% depending on client and base location.
WHO YOU'LL WORK WITH
You'll join us in Montreal to work closely with our clients and our Data Scientists in order to curate, transform and construct features which feed directly into our modelling approach.
This would be a hybrid client-facing/technical role using state of the art technologies, whilst also being able to communicate complex intractable ideas to non-technical audiences. Collecting clear requirements is a key part of this role and will define the technical strategy the team employs on the study.
Who you are
A core value at QuantumBlack is fusion and at the heart of our multi-disciplinary teams is the belief that the sum of individual parts will always be less than the impact of the entire team. You are a highly collaborative individual who is capable of laying aside your own agenda, listening to and learning from colleagues, challenging thoughtfully and prioritising impact. You search for ways to improve things and work collaboratively with colleagues. You believe in iterative change, experimenting with new approaches, learning and improving to move forward quickly. Trust between colleagues is paramount here – you are an individual who can always be trusted to work in the best interests of all colleagues and to achieve the best outcome for QuantumBlack and our clients. You are naturally enthusiastic and enjoy sharing your passion with others.
WHAT YOU'LL DO
As a Senior Data Engineer at QuantumBlack in Montreal...
You will work in multi-disciplinary environments harnessing data to provide real-world impact for organisations globally. You will influence many of the recommendations our clients need to positively change their businesses and enhance performance.
Role responsibilities
Work with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Understanding, assessing and mapping the data landscape.
Maintaining our Information Security standards on the engagement.
Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models.
Defining the technology stack to be provisioned by our infrastructure team.
Building modular pipeline to construct features and modelling tables.
Use new and creative techniques to deliver impact for our clients as well as internal R&D projects.
Mentoring and developing junior Data Engineers on engagements.
What you’ll learn
How successful projects on real world problems across a variety of industries are completed through referencing past deliveries of end to end pipelines.
Build products alongside the Core engineering team and evolve the engineering process to scale with data, handling complex problems and advanced client situations.
Be focused on the wrangling, clean-up and transformation of data by working alongside the Data Science team which focuses on modelling the data.
Using new technologies and problem-solving skills in a multicultural and creative environment.
You will work on the frameworks and libraries that our teams of Data Scientists and Data Engineers use to progress from data to impact. You will guide global companies through data science solutions to transform their businesses and enhance performance across industries including healthcare, automotive, energy and elite sport.
Real-World Impact– No project is ever the same; we work across multiple sectors, providing unique learning and development opportunities internationally.
Fusing Tech & Leadership– We work with the latest technologies and methodologies and offer first class learning programmes at all levels.
Multidisciplinary Teamwork- Our teams include data scientists, engineers, project managers, UX and visual designers who work collaboratively to enhance performance.
Innovative Work Culture– Creativity, insight and passion come from being balanced. We cultivate a modern work environment through an emphasis on wellness, insightful talks and training sessions.
Striving for Diversity– With colleagues from over 40 nationalities, we recognise the benefits of working with people from all walks of life.
Our projects range from helping pharmaceutical companies bring lifesaving drugs to market quicker to optimising a Formula1 car’s performance. At QuantumBlack you have the best of both worlds; all the benefits of being part of one of the leading management consultancies globally and the autonomy to thrive in a fast growth tech culture:
Healthcare Efficiency– We helped a healthcare provider improve their clinical trial practices by identifying congestion in diagnostic testing as a key indicator of admissions breaches.
Environmental Impact– We designed and built the first data-driven application for a state of the art centre of excellence in urban innovation by collecting real-time data from environmental sensors across London and deploying proprietary analytics to find unexpected patterns in air pollution.
Product Development– We worked with the CEO of an elite automotive organisation to reduce the 18-month car development timeframe by improving processes, designs and team structures.
Please submit your CV in English
Visit our Careers site to watch our video and read about our interview processes and benefits."
Data Engineer,"Montréal, QC",Paytm,None,Organic,"About Paytm Labs:
At Paytm Labs, we’re on a mission to provide useful technological solutions that enrich and empower millions of people in their daily lives. We apply big data, artificial intelligence and machine learning to bring the next generation of financial products and services to the Indian, Japanese and Canadian markets.

As a company, we’re committed to offering the most transparent, secure, and personalized consumer experience to over 500 million users and over 17 million merchants. Since our journey began 6 years ago, we’ve launched the Paytm Canada app (our bill management app), and PayPay (a QR-based payment app in Japan), all while powering the Paytm India app.

Job Description:
If working with billions of events, petabytes of data and optimizing for last millisecond is something that excites you then read on! We are looking for Data Engineers who have seen their fair share of messy data sets and have been able to structure them for building useful AI products.

You will be working on writing frameworks building for real time and batch pipelines to ingest and transform events(108 scale) from 100’s of applications every day. Our ML and Software engineers consume these for building data products like personalization and fraud detection. You will also help optimize the feature pipelines for fast execution and work with software engineers to build event driven microservices.

You will get to put cutting edge tech in production and freedom to experiment with new frameworks, try new ways to optimize and resources to build next big thing in Fintech using data!
Responsibilities:
Work directly with Machine Learning Engineers and Platform Engineering Team to create reusable experimental and production data pipelines.
Understand, tune, and master the processing engines (like Spark, Hive, Samza, etc) used day-to-day.
Keep the data whole, safe, and flowing with expertise on high volume data ingest and streaming platforms (like Spark Streaming, Kafka, etc).
Sheppard and shape the data by developing efficient structures and schema for the data in storage and transit.
Explore as many new technology options for data processing, storage, and share them with the team.
Develop tools and contribute to open source wherever possible.
Adopt problem solving as a way of life – always go to root cause
Qualifications:
Degree in Computer Science, Engineering or a related field
You have previously worked on building serious data pipelines ingesting and transforming > 10^6 events per minute and terabytes of data per day.
You are passionate about producing clean, maintainable and testable code as part of a real-time data pipeline.
You understand how microservices work and are familiar with concepts of data modelling.
You can connect different services and processes together even if you have not worked with them before and follow the flow of data through various pipelines to debug data issues.
You have worked with Spark and Kafka before and have experimented or heard about Flink/Druid/Ignite/Presto/Athena and understand when to use one over the other.
On a bad day maintaining zookeeper and bringing up cluster doesn’t bother you.
You may not be a networking expert but you understand issues with ingesting data from applications in multiple data centres across geographies, on-premise and cloud and will find a way to solve them.
Proficient in Java/Scala/Python/Spark
What we Offer!
Due to the pandemic, we have been and will continue to WFH until it is safe to open our office. Our company culture and values remain at the core of everything we do.
For the third year in a row, we are proud to announce that we have been certified as a Great Place to Work.
We were also certified as one of the Best Workplaces for Mental Wellness in 2020
We are an open work environment that fosters collaboration, ownership, creativity, and urgency
We ensure flexible hours outside of our core working hours
Enrolment in the Group Health Benefits plan right from day 1, no waiting period
To keep things fun and stress-free during COVID-19 we started Virtual Daily, Virtual Weekly and Monthly team bonding activities including: Trivia, Games Nights, Movies Nights, Arts & Crafts (e.g. Origami), Lunch & Learns (e.g. Sign Language 101), Virtual Wellness Sessions (e.g. Meditation, Morning stretches), Virtual Team Ubereats Lunches, and so much more!
We also created and began publishing a monthly internal newsletter with various topics that keeps the tone lighthearted and interesting.

When we are able to open our office, our in-office experience consists of:
Team building events (anything from axe throwing, go-karting, bike riding, etc.)
Fuel for the day: Weekly delivery of groceries, and all types of snacks
Catered lunches and desserts on a monthly basis
Flexibility with WFH
Daily fun in the office with our competitive games of Ping Pong, Pool, Smash Bros competitions, or FIFA
And of course, an unlimited amount of freshly made coffee! We’re pretty serious about our coffee beans.
Notice for Job Applicants
Following the advice of Canadian health authorities, to mitigate the risk of the potential spread of COVID-19 and support social distancing, all recruiting activities including interviews and new hire onboarding will be conducted remotely. While we are doing our best to ensure reasonable response times, please expect potential delays during the recruiting process due to the current situation.

We are an equal opportunity employer and value diversity and uniqueness at our company. We thank all applicants, however, only those selected for an interview will be contacted.

Paytm Labs is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please let us know.

Don't have Paytm Canada App yet?
Check us out in the Google Play or App Store."
Senior Scala Engineer - Data,"Winnipeg, MB",SkipTheDishes,None,Organic,"Description:
SkipTheDishes, an arm of Just Eat Takeaway, is searching for a Senior Data Engineer to join the Data Systems - Core platform team. You’ll have the opportunity to work with big data technologies, building scalable and reliable solutions to support real-time analytics, advanced data science and critical operational projects powered by data.
What We Do
The Data Systems team’s role is to build a transformational data platform in order to democratize data in Just Eat. Our team is built on the following ideals:
Open Data: We ingest all data produced across Just Eat using batch and real-time pipelines and make it available to every employee in Just Eat. This data is then used to drive analytics, business intelligence, data science and critical business operations.
Self Service: We build tools, frameworks and processes to support self-service modelling and activation of data. Our goal is to empower our users to find, process and consume our data without barriers.
Single Truth: We build services that host all metadata about Just Eat’s data in a single store and promote governance, data culture and Single Source of Truth.
Intelligent Personalisation: We build and maintain a machine learning platform that supports data scientists in developing and deploying ML models at the production scale. This allows us to deliver insights, personalization and predictions to our customers at scale.

How We Do It
Our team is built on the following tenets:
Innovate: We are always learning, growing, inquisitive and keen on new technologies and open source tooling. We love like-minded engineers with a passion to keep our code-base and infrastructure best in class.
Build for Scale: All our tools and components are built for scale and we use Kubernetes and other tools to help us scale automatically.
Cloud-based: We use serverless technologies where possible to simplify our estate, technologies like BigQuery, PubSub, Dataflow and Cloud functions allow us to move quickly. In addition, we run a Kubernetes cluster on GKE with many workloads including instances of Apache Airflow.
DevOps culture: Everyone in the team contributes to infrastructure, we have a CI/CD pipeline and we define our infrastructure as code. Our stack includes Terraform, Jenkins and Helm. Teams monitor their applications using Prometheus, Grafana and alert manager.
Collaboration & Ownership: All code is owned by the team and we have multiple avenues for collaboration - rotation, pairing and technical showcases. We also encourage team members to own their own code and promote self-governance.
We’re looking for enthusiastic engineers to join our team in Data Systems
Core Systems
Our team’s mission is to build base functionalities that power a leading data platform. Our portfolio includes a variety of highly critical tools including (but not limited to) Real-time Ingestion, GDPR, Metadata Catalogues, and Access Controls. Here are snippets of some of our portfolio
Real-time Ingestion - A generic data ingestion pipeline currently running up to 10k events per second from 2.5k topics.
Metadata Catalogues - Infer and store context around data (about 1 PB in 500 datasets) and keep it secure and searchable.
Our stack is primarily Scala using libraries like ZIO, Cats, Akka streams and Apache beam. Python is also used in lots of areas.

You should apply if
You are confident in a functional programming language like Scala both in and outside of the data domain.
You love writing well tested, readable and performant code, capable of processing large volumes of data.
You love working with Cloud technologies and have experience in working with AWS, Azure or Google Cloud. We use Google Cloud with a mix of services - Kubernetes, Dataflow, PubSub etc.
You can contribute to architecture discussions and influence peers and stakeholders to make better decisions.
You have the inclination to collaborate and the ability to communicate technical ideas clearly.
You understand the entire product development lifecycle, from coding to deployments, to monitoring, alerting etc... Our teams maintain all aspects of our product lifecycle, but we don’t expect everyone to be an expert in all of it.
You understand the fundamentals of computing and distributed systems.
Why Work At Skip?
Picture this: you, dressed in your fave casual attire, amongst a team of friendly and passionate colleagues. You feel pride knowing your input and uniqueness are not only embraced but make an impact on a major Canadian company and its satisfied customers. As the company grows, so do you — you meet and surpass new challenges every day.

That’s just a small taste of what it’s like to work at one of Canada’s leading tech companies. If you’re hungry for opportunity, growth, and something meaningful in a dynamic, yet casual environment, we’d love to hear from you.

Skip The Dishes is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We are committed to accommodating applicants with disabilities throughout the recruitment activities and will work with all applicants requesting accommodation at any stage of the hiring process.

Note: All employees will be asked to sign a Consent for Disclosure of Personal Information in order to complete a background check. Job offers will be conditional upon results that the Company determines to be satisfactory."
Data Engineer - Level 3,"Montréal, QC",SSENSE,None,Organic,"Company Description
SSENSE, pronounced [es-uhns], is a Montreal-based fashion platform with global reach. Founded in 2003, SSENSE is pacing the vanguard of directional retail with a mix of luxury, streetwear, and avant-garde labels. We produce industry-leading original content and take pride in building our own technology solutions and systems from scratch. Our field of focus has grown beyond that of a typical e-commerce entity as we explore the nexus of content, commerce, and culture. Currently serving 150 countries, generating an average of 76 million monthly page views, and achieving high double digit annual growth since inception, SSENSE is becoming a cultural protagonist in its own right.

Job Description
SSENSE is looking for a Data Engineer to join our rapidly growing technology team. The level 3 Data Engineer will join a squad and deepen their knowledge of software development and data pipelines. They will break down, with minimal guidance, large tasks into smaller, manageable steps to deliver complex tasks required for well-defined features of the Product roadmap. The ideal candidate will contribute to knowledge dissemination within the organization and participate in the recruiting and onboarding of new employees.
RESPONSIBILITIES
Product Delivery
Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets into a readable and accessible format for end-user facing reports, data sciences and ad-hoc analyses
Understand the high-level product roadmap and immediate features to be developed, contributing to high-level estimation and layout of the development sequences
Complete complex development tasks with minimal guidance
Constantly and actively contribute to pushing code to production with the objective of becoming a main contributor
Review Pull Requests
Write testable, efficient, and reusable code suitable for continuous integration and deployment, that respect best practices and SSENSE development standards
Review Unified Modeling Language (UML) diagrams and technical documentation, ensuring its quality
Ownership and accountability:
Be accountable for code quality and conduct adequate testing
Review and contribute to technical documentation
Knowledge sharing and coaching
Join SSENSE University (the internal peer learning platform) sessions to ramp up on various technologies and host at least two sessions per year
Lead the onboarding of new data engineers
Architecture:
Contribute actively to the design of the solution, challenging other members on technical decisions
Help more junior Data Engineers understand the technical design so they can write documentation for the rest of the team
Recruiting:
Participate in HR recruiting events, helping to identify and recruit top tech talent

Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field
A minimum of 3 years of Object Oriented Programming (OOP) and/or functional programming experience
Knowledge of Apache Spark for big data processing
Knowledge of Python programming language
Knowledge of the data modelling concepts and ability to define the architecture with minimal guidance to develop a complex microservice
Familiar with various database systems and able to write complex queries independently
Knowledge of cloud concepts and the ability to follow instructions to use them with minimal guidance
Knowledge of the AWS services (Glue, Athena, S3, Databricks, etc.) and Apache Airflow, an asset
Proficiency in Git
Strong English written and verbal communication skills, French is an asset
SKILLS
Fast learner and detail oriented
Solution-oriented mindset and can-do attitude to overcome challenges
Team player with a high sense of accountability and ownership
Ability to thrive in a fast-paced environment and master frequently changing technologies and techniques

Additional Information

null"
Embedded Software Engineer - Data Plane - Multiple Roles (OT...,"Ottawa, ON",Ciena,None,Organic,"Since 1992, Ciena has been driven by a relentless pursuit of network innovation. We believe in a network that grows smarter, more agile, and more responsive every day. This means that when you digitally interact in your world - picking up the phone, streaming video, texting a friend or loved one – your interactions are being enabled by Ciena technologies. Ciena makes your social / entertainment / business existence REAL.
What will you do at Ciena?
Ciena is a network strategy and technology company known for its commitment to customer success. With nearly 25 years of industry leadership, we support more than 1,300 of the world’s largest, most reliable networks. Our technology is complemented with a high-touch consultative business model. We’re committed to developing and applying technologies that facilitate openness, virtualization, automation, collaboration, and a common experience. Technologies that offer the greatest degree of choice deliver the most rewarding customer experiences and business outcomes.
JOB DESCRIPTION & RESPONSIBILITIES
Optical networks continue to aggressively move towards Layer 0/1/2/3 converged infrastructures. Ciena’s Packet Optical Transport Switching (POTS) portfolio of Layer 2, MPLS and IP networking products provide carrier grade packet networking services in metro networks.
The POTS Datapath software group is seeking multiple embedded software engineers with experience in designing and developing embedded carrier-grade software on custom hardware platforms. As a member of the team, the successful candidate will participate in all states of the software development life cycle, including:
Design and develop software written in C for Layer 2, MPLS and IP Datapath applications and the hardware abstraction layer. This includes enhancements of existing software as well as development of new networking features.
Write software which runs on an embedded Linux/VxWorks platform.
Participate in backlog defect reduction.
Troubleshoot issues and work with hardware, software and system engineers to identify the root cause.
Help investigate and collect information to resolve process or design issues found in the codebase.
Identify software performance improvements through test driven development, automated unit testing, and comprehensive integration testing.
WHAT YOU MUST HAVE
Bachelor’s degree in Electrical/Computer Engineering or Computer Science.
5 + years’ work experience (Dependent on level) focused on embedded software development.
Extensive C programming language experience.
Experience with real-time, embedded software development for some or all of the following data plane technologies:
Fastpath hardware (ASICs, NPUs, FPGAs, TCAMs) Layer 2 and Layer 3 forwarding engines
End-to-end slow path forwarding and packet exception handling
MPLS, BGP and Segment Routing centric solutions
IPv4, IPv6, E-VPN and IP-VPN
Embedded fast protection mechanisms such as BFD, BGP-PIC and FRR
Other data-plane services including traffic management, metering, ACLs
Experience with Broadcom DNX/XGS packet processing chipsets is a strong asset
Experience with datapath programming using DPDK is an asset
Ability to work in a lab environment and integrate software onto custom and off-the-shelf data plane hardware components such as ASICs, NPUs, FPGAs and TCAMs and associated SDKs.
Familiarity with developing software on multi-processor, highly concurrent systems
Good knowledge of mutual exclusion, synchronization, interrupt handling, inter-process communication, etc.
Ability to seek out answers and work independently.
Positive work attitude, highly motivated and a willingness to learn.
Strong verbal and technical writing skills.
Collaborates well in a team environment.
Independent self-starter and commitment to delivering on aggressive deadlines.
LI-EP
About Ciena
Ciena is a network strategy and technology company with a passion to provide an experience, to you and our customers that is as rewarding as the outcome. We attract the best and brightest– those with outstanding talent, motivation, and the right attitude to contribute to our success. Our culture balances our openness and informality with professionalism and trust and is built on the foundation of our core values: Customer First, Integrity, Velocity, Innovation, and Outstanding People.
Ciena enables everyone to have a voice and a network that supports them while on the journey to discovering their passion and purpose. We trust each individual to do what they can to reach their full potential and make an impact on the business, whenever, wherever they are in the world. With Ciena’s highly innovative, forward-thinking business practices, we reward people for pushing the boundaries. Unlock your potential at Ciena!

Being You @ Ciena
As part of our commitment to diversity and inclusion, we want to foster an environment that values and respects all individual’s strengths, perspectives, ideas, and ability to meet the needs of our customers globally. Ciena values the diversity of its workforce and respects its employees as individuals, regardless of race, ethnicity, religion, gender, age, national origin, disability, sexual orientation, veteran or marital status or any other category protected by applicable law. We do not tolerate any form of discrimination.
Ciena is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
If contacted in relation to a job opportunity, you should advise Ciena in a timely fashion of the specific accommodation measures required for you to be assessed in a fair and equitable manner."
Senior Data Engineer,"Burnaby, BC",Infoblox,None,Organic,"Infoblox is looking for a Senior Data Engineer to augment our Cyber Security Data Science Team. This growing team supports the Infoblox mission to thwart cybersecurity threats in our customer’s networks. This is an opportunity to work closely with data scientists and threat analysts to curate the data that makes this mission possible.
The ideal candidate is a savvy software engineer with experience in data engineering and a solid background in Spark and Python. Preferably you know that countMinSketch is not a children’s game. You are comfortable wearing several hats in a small organization with a wide range of responsibilities and have worked in a cloud environment, such as Amazon EMR. You know that Big Data is both a blessing and a curse; without good data engineering, it loses its potential. You are passionate about the nexus between data and computer science-driven to figure out how best to represent and summarize data in a way that informs good decisions and drives new products. When someone says, “my Spark job failed”, your first question is “what’s the skew?”.

Come join our growing Cyber Threat Intelligence team and help us build world-class solutions!
Responsibilities:
Curate very large-scale data from a multitude of sources into appropriate sets for research and development for data science, threat analysts, and developers across the company
Design, test, and implement storage solutions for various consumers of the data
Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods
Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics
Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines
Collaborate on design, implementation, and deployment of applications with the rest of software engineering
Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data
Build and maintain tools for automation, deployment, monitoring, and operations
Create test plans, test cases, and run tests with automated tools
Requirements:
5+ years of experience with Python3, and 2+ years experience with Spark. Scala experience is helpful
5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments
3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL
3+ years of experience with developing ETL pipelines and data manipulation scripts
Proficient in Object-Oriented Design and S.O.L.I.D principles
Strong emphasis on unit testing and code quality
Proficient with AWS products (EMR S3, L.ambda, VPC, EC2, API Gateway, etc)
Preferred Experience:
Very strong Python and PySpark experience
Very strong back end development experience
Strong experience with cloud deployments and CI/CD
Experience with virtualization, containers, and orchestration (Docker, Kubernetes, XEN)
Experience with NoSQL Non-Relational databases (AWS DynamoDB)
Education:
MS or BS in Computer Science or a related field, or equivalent work experience required
Perks:
Work with a world-class technology team in a rapidly growing company
A career path with opportunities to grow
Boutique office space with state of the art amenities, located in the heart of Metro Vancouver area; steps from SkyTrain and Metrotown Mall
Cross-functional break room stocked with complimentary snacks and beverages
And many, many more perks!
It’s an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies—and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team—our future looks bright, and so will yours. To check out what it’s like to be a Bloxer, click here.

#LI-AB1"
Senior Software Engineer - Data Pipeline,"Toronto, ON",BenchSci,None,Organic,"BenchSci's vision is to bring medicine to patients 50% faster by 2025. We're doing this by empowering scientists with the world’s most advanced biomedical artificial intelligence to run more successful experiments. Backed by F-Prime, Gradient Ventures (Google’s AI fund), and Inovia Capital, our platform accelerates science at 15 top 20 pharmaceutical companies and over 4,300 leading research centers worldwide. We're a CIX Top 10 Growth company, certified Great Place to Work®, and top-ranked company on Glassdoor.

We are currently seeking a Senior Software Engineer to join our Data Pipeline Team. Reporting into the Data Engineering Manager, you will work on evolving our data models in several styles of datastores, improve internal tooling to allow data self-service, and operationalize production-grade data pipelines.
You Will:
Scale data pipelines to allow data to go from research to platform as fast as possible
Develop data access mechanisms for downstream applications consumption
Manage sources which contain both semi-structured as well as unstructured data
Develop and apply suitable frameworks to detect data drift, and then calibrate and redeploy them to production seamlessly
Collaborate closely with other engineers to solve interesting and challenging data problems
You Have:
5+ years' experience working as a professional developer
Expertise in Python
Expertise with SQL
Expertise in Spark 2.x, Dataset/DataFrame API and performance tuning
Experience with cloud reference architectures and developing specialized stacks on cloud services
Experience with Pandas
Nice to haves, but not mandatory qualifications:
Background in Life Science
Experience with Airflow or other workflow management systems in a distributed setup
Experience with graph data modelling and scaling graph databases
Experience with Kubernetes in production
Experience with technical design and applying architectural patterns
Our benefits and perks:
A compensation package that includes equity options in the company
An annual Executive Health Assessment at Medcan: All employees get the “executive treatment”
Effectiveness coaching for managers: Onsite, personalized coaching from a trained clinical psychologist
Mental health tools and support: Optional mindfulness sessions and a free Headspace account
Complimentary genome sequencing from 23andMe: Find out what your DNA says about your health, traits, and ancestry
Three weeks of vacation, plus another week: Get 15 days to use anytime, and we’re closed Dec 25-Jan 1
Additional days off: Company summer day, your birthday, and earn +1 vacation day annually
Work from anywhere flexibility: Every day right now, and up to 4 days per week once we return to the office
An onsite gym: Keep fit, conveniently, with a Peloton and other great equipment
A great benefits package: Including health and dental

Here at BenchSci, these are our core values:
Focused: We focus on what will drive the greatest impact at all times.
Advancement: We believe in continuous growth, and discovering new ways to do things better. This applies to our product and business, but also to ourselves.
Speed: We recognize that without a sense of urgency, our team, our product and our mission lose their value.
Tenacity: What we’re trying to do isn’t easy, but we hire the best people, and give them the autonomy, tools, and resources to succeed. The hard work is up to them.
Transparency: We believe that sharing diverse ideas and information creates strong teams. Our success stems from research, collaboration, feedback, and trust.
BenchSci is an equal opportunity employer. We value diversity and are committed to fostering an inclusive environment. All four of our cofounders are immigrants to Canada, as are many of our employees. We welcome your fresh perspectives and ideas."
Senior Data Engineer,"Toronto, ON",Myant,None,Organic,"About us:
At Myant, we are creating the world’s first textile computing platform, integrating technology directly into the only thing we’ve been wearing our entire life – clothing. SKIIN is our first consumer facing brand, and SKIIN’s vision is to enhance human ability through connected clothing - think Ironman’s suit, but comfortable. The sensors and actuators embedded within our apparel create your Digital Identity, which will be consumed by those who matter to you - your family members, doctors, coaches, other IoT devices - without you consciously having to think about it. Imagine a world where you walk into your house and the temperature automatically adjusts to your optimal body temperature, the lights adjust to your mood, you can monitor and adjust your everyday lifestyle based on your vital signs, or your doctor is aware of the onset of a disease before you even visit. The line between the digital and physical world is becoming increasingly blurry, and we believe textile is the next medium to bridge that gap.
We’re looking for people who believe in our mission to make wearable technology truly ubiquitous and convenient, so that everyone can benefit from it. We are a cross-functional team solving big challenges at the intersection of fashion, electronics, software, and data science.
Responsibilities:
Test the performance of the algorithms developed by the Data Science team
Leverage native APIs for integration of AWS platforms
Take ownership of all your deliverables and communicate your results to timely project delivery
Prepare reports and some technical documentations

Qualifications Required:
Bachelor’s Degree in Computer Science, Computer Engineering, or equivalent work experience
Proficiency with JavaScript and Python language
Basic knowledge of machine learning algorithm and libraries like PyTorch, Keras, TensorFlow, sklearn
Experience in building RESTful APIs following Micro-Services Architecture
Experienced in NodeJS, PostgreSQL, and GraphQL.
Significant experience in building microservices leveraging various AWS features (AWS Lambda, IAM, SQS, DynamoDB, Kinesis, Redshift, Aurora, EC2, S3, API Gateway, etc.)
Experience in Biomedical signal processing and data mining related to physiological patient data is a bonus
Powered by JazzHR
8SQJftU9nl"
Data Engineer,"Ottawa, ON",Veem,None,Organic,"About Veem
Veem empowers businesses who spend too much time and money dealing with inefficient financial payment systems. Our transparent, relationship-based payments model makes it easy to build trust with your vendors, contractors and customers by providing a quick and seamless payable and receivable process. We make the process even easier for these clients by supporting integration with all major accounting software including QuickBooks, Netsuite, and Xero. Backed by top investors such as Goldman Sachs, Kleiner Perkins and Google Ventures, Veem is a fast-growing financial technology company that is changing the way companies pay and get paid.
Veem is looking for an experienced data engineer to be a key member of our Data Team. The position will be based full-time in Ottawa. You are passionate about leading initiatives to deliver core insights that are rigorous and reliable. Your work will be leveraged throughout the company including product, sales, marketing, and leadership. You have deep technical skills and are excited about building a green field data platform to help make data-informed decisions.
Guiding Principles and Expected Responsibilities
The Data Team's objective is to answer central questions that are foundational for our business: What influences customer behavior? How do these influences impact our business?
Define and develop the program and architecture for data collection, modeling, metrics creation, data validation, model training, and reporting of intelligence.
Create pipelines (data processing, data analysis, optimization, implementation, validation)
Define data schemas and services, focussed on accessibility/use-case for the consuming process.
Understand customer behavior to develop predictive models to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.
What We Believe Will Lead To Your Success
We're looking for someone with 5-10 years of experience in data engineering or platform engineering with an emphasis working with creating data architectures to collect and analyze diverse datasets (e.g. large and small, structured and unstructured, behavioral and self-reported).
Highly effective with SQL and understand how to write and tune complex queries.
Hands-on experience with languages like R, Python or Java for data manipulation.
Working knowledge of relational & analytical databases and distributed systems with tech stack that can include - MySQL, Snowflake, Redshift, Hive/Hadoop, AWS S3 services, GraphDB, Spark, etc.
Experience analyzing data from 3rd party providers like: Google Analytics, Adwords, Segment, Salesforce, Pardot, Mandrill, etc.
Nice to have: Data analysis using tools like: Looker, Tableau, D3, ggplot, etc.
Nice to have: Familiarity with machine learning and statistical approaches (Clustering, Decision Trees, Bayesian, GLM/Regression, Random Forest, Neural Networks, etc.)
BS/BA or greater in Mathematics, Physics, Statistics, Computer Science, Engineering, or another quantitative field.
Perks:
Competitive salary
Comprehensive benefits package (Health, Dental, Medical, Vision)
Group RRSP Plan (after 3 months)
3 weeks vacation
Friday afternoon unwind"
Sr. Data Engineer,"Mississauga, ON",SOTI Inc.,None,Organic,"SOTI is committed to providing its employees with endless possibilities; learning new things, working with the latest technologies and making a difference in the world.
Job Title:
Senior Data Engineer
Location:
Mississauga
Who We Are
At SOTI, we are committed to delivering best in class mobile and IoT device management solutions. We are looking for out of the box thinkers that appreciate the art of creating great software. To us, being visionary is more important than doing things the way they’ve always been done.
What’s in it for you?
The People - From our humble origins in our founder’s basement, to our industry leading position today, SOTI has worked hard to foster a company culture that we can all believe in. A culture that emphasizes personal growth, continuous innovation and fun.
The Growth - Our environment fosters new ideas, fresh perspectives, and the ability to take them over the goal line. SOTI is a fast-paced environment with a global reach that encourages you to make your mark and be part of something big!
The Technology - You’ll get the chance to work with leading edge technologies and take on complex and interesting projects, as part of highly collaborative and agile teams. You will work alongside SOTI’s partners which include leading tech giants that will keep you on the cusp of emerging technologies.
What You’ll Do
Ability to translate and document business requirements into technical documentation, supporting document management and knowledge sharing
Ensure assigned deliverables are within business / audit control requirements
Take ownership of end to end design and all aspects related to development and ensure design and development standards and followed
Create project documentation (Detailed design, Source-to-target mappings, Implementation plans, etc.)
Develop data and database-oriented solutions in order to solve complex business problems leading to data driven decision making
Develop data integration processes to integrate disparate data sets into a cohesive data model in support of BI and analytical requirements
Works closely with data architecture to ensure proper adherence to architectural guidelines and principles
Experience You’ll Bring:
7+ years of hands-on advanced experience designing and developing BI Solutions and providing technical expertise
7+ years hands-on advanced experience using MS SQL
7+ years of experience with MS SQL Business Intelligence Stack (SSAS, SSIS, and SSRS)
5+ years hands-on advanced experience using Power BI or similar BI platforms
Experience using Cloud architecture, NoSQL databases and R/Python
Experience using building data pipelines to integrate with unstructured data sources
Experience in designing and building unstructured data stores using Azure or AWS technologies
Experience with Data Warehouse concepts, including the use of Extract, Transform, and Load (ETL) tools
Excellent analytical, troubleshooting, problem-solving and research skills
Must be able to multitask and have experience with interacting within a diverse user/customer base
Excellent written, verbal, and interpersonal communication skills
About SOTI
SOTI is the world's most trusted provider of mobile and IoT management solutions, with more than 17,000 enterprise customers and millions of devices managed worldwide. SOTI's innovative portfolio of solutions and services provide the tools organizations need to truly mobilize their operations and optimize their mobility investments. SOTI extends secure mobility management to provide a total, flexible solution for comprehensive management and security of all mobile devices and connected peripherals deployed in an organization.
At SOTI, we celebrate the uniqueness of our global teams and are proud to be an equal opportunity workplace. We are curious problem solvers who are committed to bringing the best mobile and IoT management solutions to market. We offer careers with #EndlessPossibilities.
What are you waiting for? Apply today: https://www.soti.net/careers
If you want to bring your ideas to life, apply at SOTI today.
We are committed to providing accessible employment practices that are in compliance with the requirements under the Human Rights Code and the Accessibility for Ontarians with Disabilities Act (AODA). If you require accommodation during any stage of the recruitment process, please notify People & Culture at careers@soti.net .
Please note that SOTI does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed Services Agreement with agency/recruiter, SOTI will not consider or agree to payment of any referral compensation or recruiter fee."
Data Support Engineer,"Toronto, ON",Capgemini,None,Organic,"Duration: 5+ months

Requirement:
Primary Skill:
Talend Data Warehouse Engineer

Talend Data Catalog

Talend Metadata Management

Snowflake Data Warehouse

Secondary Skill:
DevOps

Microsoft Azure

AWS

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
(Senior) Principal Data Engineer - Platform,"Vancouver, BC",Absolute Software,None,Organic,"Description
Do you want to be part of a team that is on the leading edge of innovation in endpoint visibility and control? Absolute is seeking an experienced technical leader with a passion for data, architecture and design. As a senior member of the platform team you will have the opportunity to drive the technical direction of Absolute’s next generation big data platform and champion the use of architecture standards and best practices.
Responsibilities:
Manage the definition of data architecture to ensure that software solutions are built within a consistent framework
Contribute to the evolution of the data and technology architecture, combining technology selection and operational expertise
Partner with teams in building a highly scalable, reliable, resilient and available big data platform
Hands-on development creating proof of concepts for technological innovations and leading their adoption
Participate in technical reviews
Evangelize use of data architecture standards and best practices; mentor and coach
What You Will Need:
BSc degree in Computer Science is minimum, M.S. in Computer Science is preferred
7+ years of Java EE development, working with Hibernate, Spring or similar JPA frameworks
10+ years of experience with a variety of persistence technologies (Relational and NoSQL) optimized for big data processing, querying and analysis. Those would include document data stores (MongoDB), time series (InfluxDB), and full text search engines (Elastic).
Experience building high-throughput, low-latency multi-threaded systems with parallel pipeline processing.
Experience taking a lead role in designing and building sophisticated fault-tolerant distributed systems that have been successfully delivered to customers
Knowledge of cloud IaaS/PaaS and container orchestration tools (Kubernetes, Docker)
Outstanding problem solving and organizational skills
Communication is crucial – so excellent verbal and written communication skills are a must
Continuously looks for ways to improve and sets a very high bar in terms of quality
Why Work For Us:
Headquartered in Vancouver, Canada with international offices in San Jose - CA, Boulder - CO, Ankeny –IA, Austin - TX, Reading - UK and Ho Chi Minh City - Vietnam, Absolute serves as the benchmark for Endpoint Resilience, ensuring connectivity, visibility and control, independent of the operating system – embedded in more than a billion endpoints, we empower devices to recover automatically from any state to a secure operational state without user intervention. Our unique value supports our aspirational journey - to become the World’s Most Trusted Security Company. Nothing short of bold, and nothing less than achievable for this team. Whether it’s our commitment to the cybersecurity industry, our customers, or to one another, we are relentless about protecting people’s devices and the sensitive information found on them. And those common goals foster a work environment where collaboration, big ideas and world-class execution are rewarded with success through our mantra of One Team | One Number. At Absolute, we incorporate the ideals of Resilience in all we do to safeguard our customers’ data and information, so they can focus on saving lives, fighting fraud, moving markets and protecting passengers, to name a few. Our innovation journey has blossomed from within, so we foster that mindset by investing in our employees – fueling our employee’s creative expression, and resulting in our own cyber capabilities. Our momentum is palpable – Forbes noticed too and recognized Absolute as one of the top-10 cybersecurity companies to watch in 2019 and 2020. The New Reality of Remote Work and Distance Learning has further connected our teams and our passion to drive to solve our customers challenges. We pride ourselves on our agile, high energy culture that rewards exceptional achievements and the contributions of those passionate about our collective growth and success. We also respect the need for downtime and believe in a sound work / life balance, reflected in our ‘Take What You Need’ vacation policy and our annual employee retreat where it’s all about friends and family. To learn more about Absolute, visit our website at www.absolute.com or visit our YouTube channel.
Absolute is an equal opportunity employer."
(Senior) Principal Data Engineer - Platform,"Vancouver, BC",Absolute,None,Organic,"Do you want to be part of a team that is on the leading edge of innovation in endpoint visibility and control? Absolute is seeking an experienced technical leader with a passion for data, architecture and design. As a senior member of the platform team you will have the opportunity to drive the technical direction of Absolute’s next generation big data platform and champion the use of architecture standards and best practices.
Responsibilities:
Manage the definition of data architecture to ensure that software solutions are built within a consistent framework
Contribute to the evolution of the data and technology architecture, combining technology selection and operational expertise
Partner with teams in building a highly scalable, reliable, resilient and available big data platform
Hands-on development creating proof of concepts for technological innovations and leading their adoption
Participate in technical reviews
Evangelize use of data architecture standards and best practices; mentor and coach
What You Will Need:
BSc degree in Computer Science is minimum, M.S. in Computer Science is preferred
7+ years of Java EE development, working with Hibernate, Spring or similar JPA frameworks
10+ years of experience with a variety of persistence technologies (Relational and NoSQL) optimized for big data processing, querying and analysis. Those would include document data stores (MongoDB), time series (InfluxDB), and full text search engines (Elastic).
Experience building high-throughput, low-latency multi-threaded systems with parallel pipeline processing.
Experience taking a lead role in designing and building sophisticated fault-tolerant distributed systems that have been successfully delivered to customers
Knowledge of cloud IaaS/PaaS and container orchestration tools (Kubernetes, Docker)
Outstanding problem solving and organizational skills
Communication is crucial – so excellent verbal and written communication skills are a must
Continuously looks for ways to improve and sets a very high bar in terms of quality
Why Work For Us:
Headquartered in Vancouver, Canada with international offices in San Jose - CA, Boulder - CO, Ankeny –IA, Austin - TX, Reading - UK and Ho Chi Minh City - Vietnam, Absolute serves as the benchmark for Endpoint Resilience, ensuring connectivity, visibility and control, independent of the operating system – embedded in more than a billion endpoints, we empower devices to recover automatically from any state to a secure operational state without user intervention. Our unique value supports our aspirational journey - to become the World’s Most Trusted Security Company. Nothing short of bold, and nothing less than achievable for this team. Whether it’s our commitment to the cybersecurity industry, our customers, or to one another, we are relentless about protecting people’s devices and the sensitive information found on them. And those common goals foster a work environment where collaboration, big ideas and world-class execution are rewarded with success through our mantra of One Team | One Number. At Absolute, we incorporate the ideals of Resilience in all we do to safeguard our customers’ data and information, so they can focus on saving lives, fighting fraud, moving markets and protecting passengers, to name a few. Our innovation journey has blossomed from within, so we foster that mindset by investing in our employees – fueling our employee’s creative expression, and resulting in our own cyber capabilities. Our momentum is palpable – Forbes noticed too and recognized Absolute as one of the top-10 cybersecurity companies to watch in 2019 and 2020. The New Reality of Remote Work and Distance Learning has further connected our teams and our passion to drive to solve our customers challenges. We pride ourselves on our agile, high energy culture that rewards exceptional achievements and the contributions of those passionate about our collective growth and success. We also respect the need for downtime and believe in a sound work / life balance, reflected in our ‘Take What You Need’ vacation policy and our annual employee retreat where it’s all about friends and family. To learn more about Absolute, visit our website at www.absolute.com or visit our YouTube channel.
Absolute is an equal opportunity employer."
Big Data Engineer,"Toronto, ON",Rackspace,None,Organic,"As a full spectrum AWS integrator, we assist hundreds of companies to realize the value, efficiency, and productivity of the cloud. We take customers on their journey to enable, operate, and innovate using cloud technologies – from migration strategy to operational excellence and immersive transformation.

If you like a challenge, you’ll love it here, because we’re solving complex business problems every day, building and promoting great technology solutions that impact our customers’ success. The best part is, we’re committed to you and your growth, both professionally and personally.

Overview

Our Big Data Engineers are experienced technologists with technical depth and breadth, along with strong interpersonal skills. In this role, you will work directly with customers and our team to help enable innovation through continuous, hands-on, deployment across technology stacks. You will work to build data pipelines and by developing data engineering code ( as well as writing complex data queries and algorithms.

If you get a thrill working with cutting-edge technology and love to help solve customers’ problems, we’d love to hear from you. It’s time to rethink the possible. Are you ready?
What You’ll Be Doing:
Build complex ETL code
Build complex SQL queries using MongoDB, Oracle, SQL Server, MariaDB, MySQL
Work on Data and Analytics Tools in the Cloud
Develop code using Python, Scala, R languages
Work with technologies such as Spark, Hadoop, Kafka, etc.
Build complex Data Engineering workflows
Create complex data solutions and build data pipelines
Establish credibility and build impactful relationships with our customers to enable them to be cloud advocates
Capture and share industry best practices amongst the community
Attend and present valuable information at Industry Events
Traveling up to 50% of the time
Qualifications & Experience:
3+ years design & implementation experience with distributed applications
2+ years of experience in database architectures and data pipeline development
Demonstrated knowledge of software development tools and methodologies
Presentation skills with a high degree of comfort speaking with executives, IT management, and developers
Excellent communication skills with an ability to right level conversations
Technical degree required; Computer Science or Math background desired
Demonstrated ability to adapt to new technologies and learn quickly
#Onica

About Rackspace Technology
We are the multicloud solutions experts. We combine our expertise with the world’s leading technologies — across applications, data and security — to deliver end-to-end solutions. We have a proven record of advising customers based on their business challenges, designing solutions that scale, building and managing those solutions, and optimizing returns into the future. Named a best place to work, year after year according to Fortune, Forbes and Glassdoor, we attract and develop world-class talent. Join us on our mission to embrace technology, empower customers and deliver the future.

More on Rackspace Technology
Though we’re all different, Rackers thrive through our connection to a central goal: to be a valued member of a winning team on an inspiring mission. We bring our whole selves to work every day. And we embrace the notion that unique perspectives fuel innovation and enable us to best serve our customers and communities around the globe. We welcome you to apply today and want you to know that we are committed to offering equal employment opportunity without regard to age, color, disability, gender reassignment or identity or expression, genetic information, marital or civil partner status, pregnancy or maternity status, military or veteran status, nationality, ethnic or national origin, race, religion or belief, sexual orientation, or any legally protected characteristic. If you have a disability or special need that requires accommodation, please let us know."
Data Engineer,"Mississauga, ON",Easton Diamond Sports,None,Organic,"Do you have what it takes to win?
Like a championship team, a leading global sports brand is built with a solid foundation of players at all levels who have an unending desire and dedication not only to succeed, but also to win. At Peak Achievement Athletics, our championship team is deeply committed to developing the most innovative sports equipment in the industry and we are always looking to strengthen our roster with talented players.
Want to join our team as a Data Engineer?
The Data Engineer is responsible for developing and delivering a large-scale database platform that can efficiently enable our analysts to transform data into intelligence. The system design will dramatically reduce the time spent on data preparation tasks and have the inherent ability to scale with business growth and complexity. The Data Engineer will become intimately familiar with the architecture of Bauer Hockey systems & enterprise data structures, and be responsible for diving deep into code while simultaneously developing UI solutions for the Sales Operations super users. The Data Engineer will liaise with multiple technical teams and business teams across international geographies. The database management system will operate on a global scale driving automation and scaling for the wider organization. The role of the Data Engineer will include incorporating data management best practices into the scoping, design and development of the database. The Data Engineer will also be responsible for effectively organizing testing, implementation, support, and the development of user and technical documentation such as guidelines or instructions as necessary.
Qualifications:
Bachelor's degree in Computer science or a related field (MBA a plus) with 3+ years of practical work experience or the equivalent combination of education and experience.
Comprehensive knowledge of database technologies including, but not limited to Google Cloud, AWS, SQL, Hadoop, SAP HANA, and Alteryx.
Hands-on experience developing platforms that translate big data into business insight.
Strong knowledge of data structures and operating systems.
Knowledge of database maintenance and administration techniques.
Experience with Machine Learning languages is a plus.
Desire to work in a high-paced environment.
Strong problem-solving skills and the ability to work independently.
Strong written and verbal communication skills.
Interested yet? Good. Us too. We're pretty sure you'll want to know we offer one of the most generous benefits packages around. Things like a 401(k) retirement plan, casual work environment, and a host of other perks we don't have room to mention here.

We're interested in learning more about you and appreciate you taking the time to apply online at www.bauer.com.
Only those persons chosen for an interview will be contacted.
Peak Achievement Athletics is committed to employing a diverse workforce."
Data Engineer,"Toronto, ON",Affinity Staffing,None,Organic,"Requirements:
Experience working on building data pipelines, ingesting and transforming multiple events per minute and terabytes of data/per day.
Passionate about producing maintainable, clean and testable code, as part of a real-time data pipeline.
Understanding of how microservices work, and familiar with the concepts of data modelling.
Ability to connect different services and processes together, even if you have not worked with them before, and follow the flow of the data through multiple pipelines to debug any data issues.
Experience working with Kafka and Spark before, and have experimented, or heard about Druid/Flink/Ignite/Athena/Presto and understand when to use which one.
Ability to understand issues with ingesting data from applications in multiple data-centres, across different geographies, cloud and on-premise and will find a way to solve those issues.
Proficient in Scala/Java/Spark/Python

About Affinity:
Affinity is a full service Information Technology agency that takes a unique approach to recruiting. We believe recruiting is about creating long term relationships that foster a mutually beneficial partnership - an affinity. Bringing a new style of recruiting founded on four core principles – Transparency – Flexibility – Efficiency – Agility.

#AFF1

Experience : 5 - 10 Years"
Data Engineer/Analyst,"Montréal, QC",RCN Call Center Services,None,Organic,"Data Engineer/Analyst
We are looking for a Data Engineer/Analyst to join our growing technology team in the exciting space of financial services. Our technology integrates big data, analytics, data science and machine learning with distributed computing architectures to deliver a suite of data-driven web and mobile applications. This position is located in our Montreal office.
Role Description
This position focuses on the design, implementation, and operation of data management systems to meet our data-driven business needs. This includes designing how the data will be stored, consumed, integrated, and managed by different data entities and digital systems. You will work closely with business and operations stakeholders to determine, create, and implement systems to gain insights, support decisions, and prescriptive analytics.
You will help plan, design, and optimize for data acquisition, throughput and query performance issues. This requires constantly updating your expertise in technologies such as cloud services and platforms, AI/ML technologies, infrastructure management, and more in building true data-driven value. You will play a key role in the designing and developing real-time and batch processes and pipelines for predictive and prescriptive modeling and analytics to support the insights and analytics required by the business.
Duties & Responsibilities
Create and maintain optimal data pipeline architecture using databases, services, and data warehousing technologies.
Assemble large, complex data sets that meet functional and non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Build analytic tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the executive, product, development and design teams to assist with data-related technical issues and support their data infrastructure needs.
Requirements & Characteristics
The key success criteria for this position is solving data-based analytical problems by combining traditional database skills and machine learning skills.
Three years of professional experience in a data engineering or analytics role.
Excellent collaborative, team-oriented communication skills, able to extract technical requirements from business-level requirements.
Excellent problem solving and critical thinking skills.
Expert-level understanding of integrating AI/ML solutions with web application architectures.
Excellent understanding stream processing and event-driven fundamentals.
Excellent skills working with relational databases such as MySQL or PostgreSQL.
Experience with NoSQL and in-memory databases a strong plus.
Strong experience with managing ML and data systems on Linux.
Please provide Resume/CV in English
Job Types: Full-time, Permanent
Benefits:
Casual Dress
Dental Care
Extended Health Care
Life Insurance
On-site Parking
Paid Time Off
Vision Care
Schedule:
Monday to Friday
Experience:
specific, professional data analytics: 3 years (Required)
specific, professional machine learning: 1 year (Required)
Location:
Montréal, QC (Required)
Work remotely:
Yes, temporarily due to COVID-19"
Data Engineer,"Toronto, ON",SADA,None,Organic,"Join SADA as a Data Engineer!
Your Mission
As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ.
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. May involve conversion between relational and NoSQL data stores, or vice versa.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
About SADA
Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Senior Data Engineer,"Toronto, ON",SADA,None,Organic,"Join SADA as a Sr. Data Engineer!
Your Mission
As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success
#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.
Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.
As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction - a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
About SADA
Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.
Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
IT Expert - Data Science (MLOps) Engineer,"Mississauga, ON",Roche,None,Organic,"Position-IT Expert- Data Scientist
Location- Mississauga
Duration- Permanent
Department- Global IT Solution Centre
As an IT Expert - Data Scientist, you will be working as a part of a squad that is designed to respond to the business needs of our organization quickly. The specific role will be to perform Data Modeling analysis, run the Data Analysis and find patterns.
Your primary responsibilities:
Research and implement comprehensive statistical, mathematical and computing science algorithms across business contexts including research, development and quality.
Investigate available tools and technologies in machine learning and deep learning
Support collaboration with third parties including academia and industry.
The skills we are searching are amongst others:
Analyzing large amounts of data from several sources
Knowledge of classification techniques
Good knowledge of data matching/entity resolution techniques
Superb analytical and conceptual thinking skills, attention to details
Strong problem-solving skills (taking a significant, complex problem and breaking it down into components, involve others as needed, drive resolution)
Excellent communication skills/ability to interact with business stakeholders
Ability to solve problems by using machine learning or deep learning techniques
Quick learner and passionate about continually adapting your skills and knowledge
Ability to work in the interdisciplinary area and can interpret and translate the very abstract and technical approaches to a healthcare and business-relevant solution
Active team player and can work effectively in a collaborative, fast-paced, multi-tasking environment
Excellent communication skills and a demonstrated ability to interact with different teams and parties of the organization, such as business, technical and academic partners
Knowledge of R or Python is essential
Knowledge of Cloud environments (GCP, AWS) would be an asset

This position is not eligible for relocation support.
This position is open to applicants legally authorized to work in Canada.
NOTE: All employment is conditional upon the completing and obtaining a satisfactory background check, including educational, employment, references and criminal records (for which a pardon has not been granted) checks.
AGENCY NOTICE: Please note that Roche Canada does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed Services Agreement with agency/recruiter, Roche Canada will not consider or agree to payment of any referral compensation or recruiter fee. In the event a recruiter or agency submits a resume or candidate without a previously signed agreement, Roche Canada explicitly reserves the right to pursue and hire those candidate(s) without any financial obligation to the recruiter or agency.
Roche is an equal opportunity employer.
Information Technology, Information Technology > Application Development / Programming"
"Senior Software Engineer, Data Analytics","Toronto, ON",Autodesk,None,Organic,"Job Requisition ID #
20WD41615
Position Overview
As a global leader in 3D design, engineering, and entertainment software, Autodesk helps people imagine, design, and create a better world. Autodesk enables better design through an unparalleled depth of experience and a broad portfolio of software to give customers the power to solve their design, business, and environmental challenges. In addition to designers, architects, engineers, and media and entertainment professionals, Autodesk helps students, educators, and casual creators unlock their creative ideas through user-friendly applications.

The XA Product Analytics Team is a centralized Analytics team working closely with product line development teams across the company to define, implement and evolve Autodesk’s Product Analytics best practices. We help those teams with all their internal and customer facing Analytics needs by way of instrumentation, experimentation, reporting, analysis, in-product analytics and machine learning. We are looking for technically savvy data engineers and analysts with a passion for data analysis and who are eager to help us bring data driven thinking in day to day product engineering practices. We are seeking individuals who are attracted by complicated problems, who can think creatively and work hard to help us solve them.
Our Team
Our team is part of a larger Experience Design and Analytics (XA) team that provides design leadership and excellence across all product lines at Autodesk. Key disciplines include experience design, visual design, research, content strategy, and program management. Together, we support Autodesk’s growing design community, and share a common goal of being customer-centric, with connected experiences across our products.

Responsibilities
Experience with translating business requirements into suitable data schemas and managing meta data for data models.
Collaborate and work with stakeholders; to address data related problems in regard to systems integration and compatibility.
Determine best patterns to store and access data in line with usage of the system and transactional needs
Create and optimize our data pipeline architecture
Build data access platforms for data scientist and analysts
Implement ETL processes through cloud-based solutions (S3, Redshift)
Develop large-scale data structures for business intelligence analytics by using data mining tools
Write processes to ingest, transform and distribute data into our internal applications
Improve product instrumentation to ensure analysis objectives can be accomplished
Provide ad hoc queries and analysis as needed

Minimum Qualifications
3+ years professional experience
BS or MS in CS or similar
Strong experience with databases and writing and debugging SQL
Experience working with big data environments such as Athena, BigQuery, Snowflake, Hadoop (Hive, Spark)
Experience with data pipeline and workflow management tools: Oozie, AWS Glue, Airflow, Azkaban, Airflow, etc.
Experience working with BI tools such as QlikView, Tableau, and Looker
Experience with cloud-based data solutions (AWS preferred)
Experience with automation/configuration management/enterprise schedulers
System monitoring and alerting, dashboarding experience
Working knowledge of code and script (Java, Python, JavaScript, bash)
Production-level coding experience
Prefer experience with graph databases
At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.
Are you an existing contractor or consultant with Autodesk? Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact Autodesk Careers ."
Machine Learning Engineer/Data Scientist,"Toronto, ON",Crescendo Technology,None,Organic,"About the role:
We are looking for a candidate that will be responsible for developing algorithms which will form the basis of our mathematical models for our understanding of sports betting markets. These models will be used for automation. The candidate MUST have a strong background in Machine Learning and Algorithm Development experience.
Our ideal candidate should have:
Degree/Diploma in Computer Science/Software Engineering/Statistics or equivalent
3+ years of relevant experience with R or Python (NumPy, SciPy, Pandas, etc)
2+ years of relevant experience as Software Developer, preferably using Microsoft .NET Framework (C# or VB.NET)
Strong background in statistics
Experience with Machine Learning algorithms and Probabilistic Models
Experience using cloud computing platforms such as EC2 (AWS)
Experience with modern R packages and technologies such as dplyr, tidyR, data.table, shinyR
Domain experience in on-line gaming industry, financial markets or other 2-sided markets is a plus
Experience with Neural Networks or Deep Learning on large problems, Hadoop, MapReduce or High Performance Computing is a plus
Experience in SQL and SQL server is also a plus
We offer:
An environment passionate about growth and learning
Competitive salary with bonus
Fitness subsidy program
Workplace that is conveniently located along the Yonge/Sheppard line
What we are looking for:
This is a key role within the team and would suit someone who is passionate about working with data/data science. We are looking for someone with strong background in statistics, modelling and algorithms (machine learning or other) and who has the ability to convey complex information through data visualization. A thorough understanding and passion for sports and sports betting markets is ideal. Experience with cloud computing as well as python is a plus.
The above is intended to describe the general nature and level of work being performed. They are not intended to be an exhaustive list of all responsibilities, duties and skills required.
Crescendo Technology thanks all candidates applying but only those selected for an interview will be contacted. Selected candidates may be asked to complete an on-line technical assessment.
Crescendo Technology is an equal opportunity employer which values diversity in the workplace and we encourage candidates to apply directly and provide a copy of an updated resume. Should you require an accommodation for the recruitment/interview process, please do not hesitate to reach out to us.
To apply please send your resume with cover letter preferred to hr@crescendotechnology.com
Job Features
Job Category
Development"
"Consultant, Data Engineer - Toronto","Toronto, ON",Avanade,None,Organic,"Data Engineer

Do you love making sure that information is accessible and easy to use? So do we.

You are a data designer who knows how to find, store, and present a range of information from different sources so that everyone can access what they need quickly and simply, and use it effectively.

About you:
You draw on your experience in bringing data to life to aim sometimes complex problems, and you’re able to use concepts around storing, transforming, and visualizing data along the way.

About the job:
As a Data Engineer, you know the importance of data to business. You design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.

Day-to-day you:
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Your skills:
You have got a great experience in data and analysis, and how to source, store and share information. You’re a problem solver who’s happy to work autonomously and to share their knowledge and skills, as well as guiding other team members.

Your skills and experience include:
Strong knowledge of Python, Spark, and T-SQL
Database, storage, collection and aggregation models, techniques, and technologies - and how to apply them in business
Experience in structured problem solving
Strong knowledge of Python, Spark, and T-SQL
Superb communication skills
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
Desirable skills:
Knowledge of Azure tools such as Azure Data Factory, Azure Data Lake, Azure SQL DW or Azure SQL
Knowledge of Big Data tools such as Hadoop / Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics
Experience preparing data for Data Science and Machine Learning
Crafting and building Data Pipelines using streams of IoT data
Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals
You’re likely to have a Bachelor’s degree in IT, Applied Mathematics, Statistics or another meaningful field, or an equivalent combination of education and experience. You also have several years of relevant professional experience."
Senior Full Stack Software Engineer - Data Insights,Ontario,CircleCI,None,Organic,"CircleCI is looking for a senior, full-stack engineer to help us build the exciting next generation of our Insights product. With Insights, we're leveraging the wealth of data CircleCI has to offer - through data aggregation services, APIs, and UIs - to help customers make better engineering decisions. As a senior member of our Insights Engineering team, you will work closely with product, design, and your engineering teammates to build, test, and rapidly iterate on a product that will make a huge impact in how CircleCI's customers build the next generation of software.
About the Team
The Insights team is working on a greenfield product that provides our customers an avenue to track success/failure rates, throughput, and mean time to recovery, as well as duration metrics, credit burn and more for all of their jobs, workflows, and pipelines. The Insights service truly spans the full stack from a front-end micro-application through to our event-driven back-end service, supported by RabbitMQ queues.
What you'll do:
Bring an API-first approach to the development of new features in our Insights product and simplify and scale our systems while we rapidly grow and evolve across backend services, APIs, and UIs.
Interact with product management and designers helping to brainstorm on new features and working closely with your engineering teammates on building those features
Write plenty of sustainable, testable, high-quality code.
Learn about and participate in a culture of observability and monitoring: using operational data to help the team improve our systems' stability and performance.
What we're looking for:
We're looking for someone who enjoys collaboration, is curious and interested in learning, brings strong communication and teamwork skills, and participates in a highly collaborative culture by sharing their expertise and encouraging best practices. If this sounds like you, here's the additional experience we're looking for:
Experience writing and deploying web application code anywhere within the technology stack (we use React, Typescript, GraphQL and Clojure, but it's ok if you have not used them yet).
You write code that's easily readable, testable, and maintainable.
Demonstrable experience building applications and services - primarily those with user interfaces - using well-accepted design patterns to allow for iterative development.
You're excited by working within a small, rapidly-growing team: adjusting to changing priorities, quickly learning new skills, and growing through collaboration.
Experience in the day-to-day practices of continuous delivery and agile development.
Why you'll love working here:
We value transparency and collaboration across distributed teams.
We favor regular, incremental delivery of value over perfection.
We encourage continuous learning and improvement for teams and team members
Working remotely at CircleCI
We're a distributed company with teammates across the world. For this role, we can support you working remotely anywhere in Canada or Ireland.
CircleCI Engineering Competency Matrix
This role equals level E3 - Senior Software Engineer - on our Engineering Competency Matrix, our internal career growth system for engineers. These are the minimum expectation for this position, but we are always willing to discuss bringing people on at more senior positions when appropriate. Find more about the matrix in this blog post.
We know there's no such thing as an ""ideal"" candidate - we're all a work in progress and are growing new skills and capabilities all the time. CircleCI welcomes those who are enthusiastic about learning and evolving, so however you identify and whatever your background, if this looks like a role where you could do work that excites you, we hope you'll apply.
About CircleCI
CircleCI is the world's largest shared continuous integration and continuous delivery (CI/CD) platform, and the central hub where code moves from idea to delivery. As one of the most-used DevOps tools that processes more than 1 million builds a day, CircleCI has unique access to data on how engineering teams work, and how their code runs. Companies like Spotify, Coinbase, Stitch Fix, and BuzzFeed use us to improve engineering team productivity, release better products, and get to market faster.
CircleCI is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law."
"Sr. Consultant, Data Engineer - Ottawa","Ottawa, ON",Avanade,None,Organic,"Do you enjoy making sure that information is accessible and easy to use? So do we.

You're a data designer who knows how to find, store and present a range of information from different sources so that everyone can access what they need quickly and simply, and use it effectively.

About you

You draw on your considerable experience in bringing data and statistics to life to solve sometimes complex problems, and you're comfortable looking after several projects at once. You're able to make your own decisions while at the same time supporting more junior team members.

About the job

As a Senior Consultant, Data Engineering, you know the importance of data to business. You design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.

Day to day, you will:
*

Use your knowledge to plan and deliver data warehouse and storage
*

Take part in designing and running bespoke data services for individual projects
*

Stay up to date with business best practice in using and retrieving data
*

Design, develop, adapt and maintain data warehouse architecture and relational databases that support data mining
*

Customize storage and extraction, metadata, and information repositories
*

Create and use effective metrics and monitoring processes
*

Help to develop business intelligence tools
*

Support deal teams by providing subject knowledge and solutions for client proposals
*

Author reports that include key performance indicators, show where current operations can be improved, and identify the causes of any problems
*

Create and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
*

Travel as required.

Your skills

You're got great experience in data and analysis, and how to source, store and share information. You're a problem solver who's happy to work autonomously and to share their knowledge and skills, as well as guiding other team members.

Your skills and experience include:
*

Database, storage, collection and aggregation models, techniques and technologies - and how to apply them in business
*

Employing statistical and data visualization tools and techniques
*

Experience in structured problem solving
*

Great project and people management
*

Using SharePoint, PowerPivot, SRRS, Excel - including pivot tables and macros
*

Working with SQL.

You're likely to have a Bachelor's degree in Applied Mathematics, Statistics or another relevant field, or an equivalent combination of education and experience. You also have five to seven years of relevant professional experience."
Data Scientist/Machine Learning Engineer,"Toronto, ON",Staffinity Inc.,"$80,000 - $120,000 a year",Organic,"About Staffinity Inc.
We are your trusted bilingual recruiting, short and long-term staffing service provider. Our primary goal has been matching sought-after employers with talented candidates. We are your source to get you in front of desirable employers who are now hiring in your area. Please visit us at https://staffinity.ca
Why Work With Us?
We are awesome: We push the boundaries of new technology and are always trying to stay ahead of the curve
We have a great team dynamic with a more client centered approach
We offer competitive salaries and great bonuses and perks!
Staffinity is looking for an Data Scientist/Machine Learning Engineer for a permanent, full time position in Toronto ON. The work will be done primarily with R.
Responsibilities:
Develop math models and algorithms
Evaluating state-of-the-art statistical modeling and Machine Learning approaches using historical data
Deploy models using .NET framework (C#)
Data Analysis, Visualization and Modeling with large datasets
Data Acquisition, Cleaning, and Transformation
Qualifications:
Degree or Diploma in Computer Science, Software Engineering, or equivalent experience.
5+ years .NET Development experience.
Devops experience is a strong asset
5+ years of experience as Software Developer, preferably using Microsoft .NET Framework (C# or VB.NET)
5+ years of experience with statistical programming languages, R strongly preffered
Betting or markets experience is a strong asset
Job Types: Full-time, Permanent
Salary: $80,000.00-$120,000.00 per year
Additional pay:
Bonus Pay
Benefits:
Company Events
Dental Care
Extended Health Care
Flexible Schedule
On-site Parking
Paid Time Off
RRSP Match
Vision Care
Schedule:
8 Hour Shift
Day shift
Monday to Friday
No Weekends
Experience:
.NET: 2 years (Required)
AWS: 2 years (Required)
Machine Learning algorithms and Probabilistic Models: 2 years (Required)
R: 3 years (Required)
2 sided market: 2 years (Required)
Work remotely:
No"
"Big Data Engineer, Omnia AI Vancouver","Vancouver, BC",Deloitte,None,Organic,"Job Type: Permanent
Primary Location: Vancouver, British Columbia, Canada
All Available Locations: Vancouver

Learn from deep subject matter experts through mentoring and on the job coaching.
Partner with clients to solve their most complex problems.
Be empowered to lead and have impact with clients, our communities and in the office.

“At Deloitte we value the opportunity to network and build relationships with skilled individuals, even in periods where we are not actively hiring. If you would like to apply to this future opportunity role, and have the required qualifications, you can expect to be contacted by the recruitment team within a few days""
You love to wrestle down data puzzles, you embrace the potential that data represents, you aspire to solve data problems no one else can, and above all, you want to use data to make impacts that matter – if that is you, then Omnia AI is where you want to be.
What will your typical day look like?

As a Big Data Engineer on our Data & Analytics Modernization team within the Omnia AI practice, you are passionate about data and technology solutions, are driven to learn about them and keep up with market evolution. You will play an active role throughout the entire engagement cycle, specializing in modern data solutions including data ingestion frameworks, data pipeline development, Hadoop-based data lake architectures and orchestration. You are enthusiastic about all things data, have strong problem-solving and analytical skills, are tech savvy and have a solid understanding of software development.
Specifically, in this role, you will:
Engineer Big Data ingestion and pipeline frameworks to populate on-premise or cloud-based data lakes
Translate business rules and requirements into data objects, produce associated data models and source to target mappings and write abstracted, reusable code components accordingly
Plan/schedule tasks, lead small development teams, and mentor junior colleagues
Facilitate technical meetings with client staff, and advise client with technical option analyses based on leading practices
About the team

Omnia AI, Deloitte’s Artificial Intelligence (AI) practice is comprised of a collaborative team of experts who use their hands-on experience with cutting-edge information assets to facilitate successful AI transformations. We develop AI-enabled solutions to address all aspects of a client’s transformative journey with disciplined focus on business outcomes.
Our Data & Analytics Modernization team helps clients design and implement the data platform architectures – be it in the cloud or on-premise – required to enable cutting-edge AI solutions. You will be part of a practice to deliver a breadth of solutions to solve our clients most challenging business problems, with a focus on Big Data, BI/DW, Data Integration, Data Governance, Master Data and Analytics applications. Each of these applications leverages a different mix of traditional and innovative technologies to achieve business outcomes.
Enough about us, let’s talk about you

You are someone with:
3+ years implementation experience leveraging Hadoop ecosystem technologies such as HDFS, MapReduce, Pig, Sqoop, Spark, Hive, Kafka, etc. on-premise and/or in the cloud (e.g. AWS, Azure, GCP)
3+ years experience with analysis, design, development, testing and deployment of data pipeline (ETL) services leveraging the Big Data technology stack for batch and/or real-time messaging/streaming environments
Experience writing complex SQL queries, extracting and importing disparate data from source systems, and data manipulation based on requirements
Experience with Agile development methods in data-oriented projects
Completed Bachelor’s Degree (or higher) in quantitative areas such as Computer Science, Information Management, Big Data & Analytics, or related field is desired
If you believe you have what it takes to be a successful member of our team, please apply now. We know your career is important to you and it's important to us, too. This role is just the first step of a highly successful career we can help you build.
The time is right for you to join Deloitte. Get your career off to great start. What impact will you make?
Why Deloitte?
Launch your career with The One Firm where you can make an impact that matters in a way that you never thought possible. With endless opportunities at every turn, and a culture built to support and develop our people to be the very best they can be, Deloitte is The One Firm for you to learn, grow, create, connect, and lead. We do this by making three commitments to you:
You will lead at every level: We grow the world’s best leaders so you can achieve the impact you seek, faster.
You can work your way: We give you the means to be flexible in how you need and want to work, and we have innovative spaces, arrangements and the mindset to help you be wildly successful.
You will feel included and inspired: We create a deep sense of belonging where you can bring your whole self to work.

The next step is yours
Sound like The One Firm. For You?
At Deloitte we are all about doing business inclusively – that starts with having diverse colleagues of all abilities! We encourage you to connect with us at accessiblecareers@deloitte.ca if you require an accommodation in the recruitment process, or need this job posting in an alternative format. We’d love to hear from you!
By applying to this job you will be assessed against the Deloitte Global Talent Standards. We’ve designed these standards to provide our clients with a consistent and exceptional Deloitte experience globally."
Data Engineer,"Vancouver, BC",Skillz Inc.,None,Organic,"About Skillz:
Skillz is driving the future of entertainment by accelerating the convergence of sports, video games and media for an exploding mobile-first audience worldwide. The company's platform empowers mobile game developers and players with democratized access to fun, fair and skill-based competition for real prizes, shifting the paradigm to make eSports accessible to anyone, anywhere with a mobile device.

Skillz helps developers build multi-million dollar game franchises by turning content into competitive social gaming properties for the world's 2.6 billion gamers. The company has already worked with 13,000 game developers, leveraging its patented technology to host over 800 million tournaments for 18 million players worldwide.

This year, Skillz was recognized as one of Fast Company's Most Innovative Companies and CNBC Disruptor 50 (for the second time). In 2018, Skillz was listed as one of Forbes' Next Billion-Dollar Startups and Entrepreneur Magazine's 100 Brilliant Companies. In 2017, Inc. Magazine ranked Skillz the No. 1 fastest-growing private company in America.
The company is backed by leading venture capitalists, media companies, and professional sports luminaries, ranging from Liberty Global, Accomplice, Wildcat Capital, Telstra Ventures, and a founder of Great Hill Partners to the owners of the New England Patriots, Milwaukee Bucks, New York Mets, and Sacramento Kings.
Who we're looking for:
You're ready to take the next step in your Data Engineering career - to a fast-moving, successful company building out their next-generation streaming analytics infrastructure! You love data consistency and integrity. You consider yourself scrappy and a technologist, passionate about data infrastructure... with your attention to detail and insistence on doing things correctly, you know you can make a big impact on a small team! You're an excellent communicator and know that you grow faster from being able to mentor others.
What You'll Do:
Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture
Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure
Building data integration toolkit for backend services
Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection
Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people
Improve monitoring and alarms that impact data integrity replication lag
Support our product development team in creating new events to measure/track
Your Skillz:
Basic Qualifications:
At least 1+ years of experience in Scala/Java or Python programming
AWS data products (Data pipelines, Athena, Pinpoint, S3, etc)
Experience deploying data infrastructure
Experience with recognized industry patterns, methodologies, and techniques
Bonus:
Familiarity with Agile engineering practices
1+ years experience on Kubernete, Helm chart
1+ years of experience with Spark, Scala and/or Akka
1+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies
1+ years of experience working with Kafka or similar data pipeline backbone
1+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
1+ years' experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus)
At least 1 year of experience with Unix/Linux systems with scripting experience
Familiarity with Alooma, Snowflakes
Familiarity with Kinesis, Lamda
Prior experience in gaming
Prior experience in finance
Skillz embraces diversity and is proud to be an equal opportunity employer. As part of our commitment to diversifying our workforce, we do not discriminate on the basis of age, race, sex, gender, gender identity, color, religion, national origin, sexual orientation, marital status, citizenship, veteran status, or disability status."
Sr. Data Engineer,"Vancouver, BC",Skillz Inc.,None,Organic,"About Skillz:
Skillz is the leading mobile games platform connecting players in fair, fun, and meaningful competition.
The gaming industry is larger than movies, music, and books, with more than 2.7 billion gamers playing monthly and 10 million developers worldwide. Mobile is the fastest-growing segment of the gaming market, expected to increase from $68 billion in 2019 to $150 billion in 2025.
Skillz is pioneering the competitive mobile gaming experience, leveraging its patented technology to power over two billion esports tournaments a year. The company is headquartered in San Francisco and backed by leading venture capitalists, media companies, and professional sports leagues and franchises.
Through its philanthropic initiatives, Skillz has harnessed the power of its platform to transform the way nonprofits engage with donors, enabling anyone with a mobile device to support causes such as the American Red Cross, Susan G. Komen, American Cancer Society, and NAACP by playing in Skillz tournaments.
Skillz has also earned recognition as one of Fast Company's Most Innovative Companies, a two-time winner of CNBC's Disruptor 50, one of Forbes' Next Billion-Dollar Startups, and the #1 fastest-growing company in America on the Inc. 5000.
Who we're looking for:
You're ready to take the next step in your Data Engineering career - to a fast-moving, successful company building out their next-generation streaming analytics infrastructure! You love data consistency and integrity. You consider yourself scrappy and a technologist, passionate about data infrastructure... with your attention to detail and insistence on doing things correctly, you know you can make a big impact on a small team! You're an excellent communicator and know that you grow faster from being able to mentor others.
What You'll Do:
Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture
Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure
Building data integration toolkit for backend services
Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection
Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people
Improve monitoring and alarms that impact data integrity replication lag
Support our product development team in creating new events to measure/track
Your Skillz:
Basic Qualifications:
At least 4-5 years of experience in Scala/Java or Python programming
AWS data products (Data pipelines, Athena, Pinpoint, S3, etc)
Experience deploying data infrastructure
Experience with recognized industry patterns, methodologies, and techniques
Bonus:
Familiarity with Agile engineering practices
2+ years experience on Kubernete, Helm chart
4+ years of experience with Spark, Scala and/or Akka
4+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies
2+ years of experience working with Kafka or similar data pipeline backbone
4+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
3+ years' experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus)
At least 4-5 years of experience with Unix/Linux systems with scripting experience
Familiarity with Snowflake or OLAP
Familiarity with Kinesis, Lamda
Prior experience in gaming
Prior experience in finance
Skillz embraces diversity and is proud to be an equal opportunity employer. As part of our commitment to diversifying our workforce, we do not discriminate on the basis of age, race, sex, gender, gender identity, color, religion, national origin, sexual orientation, marital status, citizenship, veteran status, or disability status."
DATA ENGINEER (SOPHI),"Toronto, ON",The Globe and Mail,None,Organic,"DATA ENGINEER (SOPHI)

POSITION CODE: 2020-063
LOCATION: The Globe and Mail, Toronto
SALARY: Commensurate with qualifications and experience

POSITION OVERVIEW:

We’re looking for experienced individuals with deep knowledge of data streaming, serialization, databases and distributed systems, and proficient in writing custom libraries but also know when to use off-the-shelf solutions when necessary. Ideal candidates are self-motivated engineers with a passion for both business and technology innovation, more importantly they quickly adapt with changing technologies. We value people who are passionate about system design and have an eye for improving product quality. We currently work with Scala, Kotlin, Java, Python, NodeJS, Postgres, Go, Kafka, and Flink.

As a Data Engineer you will:
Develop and optimize system components for maximum performance and scalability across a vast array of environments
Have a commitment to collaborative problem solving, sophisticated design, and product quality
Ensure that system components and the overall application are robust and easy to maintain
Contribute to backlog reviews, technical solutions design and implementations
Be disciplined in implementing software in a timely manner while ensuring product quality isn’t compromised
MINIMUM QUALIFICATIONS:
Strong analysis and problem solving skills
Deep understanding of good programming practices, design patterns, Functional Programming, and Object Oriented Analysis and Design
Successfully implemented and released a large number of data pipelines and web services using modern engineering frameworks in the past 3 years
Formal training in software engineering, computer science or computer engineering.
Worked as part of a mature engineering team
IDEAL CANDIDATE:
Have strong working knowledge with Scala and/or Kotlin.
Understands reactive programming, Threads and Futures.
Successfully implemented realtime and batch analytics using Kafka, Flink, Apache Beams and/or Google DataFlow.
Strong working knowledge of data warehouses include Redshift, Snowflake, and/or Apache Druid.
Have a working knowledge with containerization and build pipelines
Successfully implemented data systems for very large data volumes such as click streams and/or IoT sensors data.

THE GLOBE AND MAIL INC. IS DEDICATED TO EQUITY IN THE WORKPLACE
At The Globe and Mail, we are committed to fostering an inclusive, accessible work environment, where all employees feel valued, respected and supported. The Globe and Mail offers accommodation for applicants with disabilities as part of its recruitment process. If you are contacted to arrange for an interview, please advise us if you require an accommodation."
CATO - Big Data Engineer,"Toronto, ON",CAPCO,None,Organic,"Data Engineer - Streaming
LOCATION: TORONTO
Capco – The Future. Now.
Capco is a distinctly and positively different place to work. Much more than consultants, we are active participants in the global financial services industry. Our passionate business and technology professionals enjoy a unique environment where they are actively encouraged to apply intellect, innovation, experience and teamwork. We are dedicated to fully supporting our world class clients as they respond to challenges and opportunities in: Banking, Capital Markets, Finance Risk & Compliance, Insurance, and Wealth and Investment Management. Experience Capco for yourself at capco.com
Let’s Talk About You
You want to Own Your Career. You’re serious about rising as far and as fast as your work and achievements can take you. And you’re ready to write the next chapter of your career story: a challenging and rewarding role as a Capco Big Data Engineer.
Let’s Get Down To Business
Capco is looking for talented, innovative, and creative people to join our development team to work on a number of projects and applications with a Data focus within the Digital practice.
Fitting that description, you will also need to be personally motivated to work in a team where clients become colleagues too.
Responsibilities
Produces high quality complex, deliverables with minimal input from stakeholders
Manage full software lifecycle for medium complexity projects from requirements, to design, to implementation, to testing
Develop and maintain back end solutions using cutting edge technologies and products
Work with Scrum Masters and product owners to priorities and deliver solutions using an Agile environment
Build reusable code and libraries for future use and follow emerging technologies
Mentor and train junior developers
Education/Experience
Bachelor’s degree (preference given to Computer Science, Engineering, Gaming and STEM-based majors) or equivalent experience
Five (5) or more years of experience as a Full-stack Data Engineer/developer on Data driven projects
Strong experience in designing and implementing real-time stream processing services such as Apache Kafka
Strong understanding of the full development lifecycle including requirements, architecture, design, development and testing
Strong development experience with Scala/Spark
Experience working with REST APIs/Springboot.
Familiarity working with Java and Hive.
Ability to balance competing priorities in a very dynamic and fast-paced environment
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)
Show Us What You’ve Got
It will be very useful if you have some or all of the following skills:
Understanding of big data and distributed programming concepts
Experience working with ASW, GCloud, Docker, Kubernetes
Experience working with Spring, Akka, Spark
Experience working with Reactive Streams (Rx, Akka, Reactor)
Strong organizational and communication skills
Experience working in an Agile environment
Experience working with code versioning tools
Experience working with build, packaging and continuous integration tools and frameworks

Professional experience is important. But it’s paramount you share our belief in disruptive innovation that puts clients ahead in a tough market. From day one, your key skill will be to perceive new and better ways of doing things to give your clients an unfair advantage.

Now Take the Next Step
If you’re looking forward to progressing your career with us, then we’re looking forward to receiving your application.
Capco is well known for its thought leadership and client-centric model that distinguishes it from other consulting firms. Capco’s strong technology and digital knowledge base, it’s global experience of the Financial Service enables us to deliver projects from strategy through to delivery. We are committed to providing new areas of expertise from which our clients will greatly benefit. We have:
Access to industry-focused talent globally
Ability to leverage best-of-breed, innovative products and solutions for complex architecture and large-scale transformation
Extended global geographic market reach
Ability to capitalize on our client footprint and deep domain expertise within financial services
For more information about Capco, visit www.Capco.com.
Capco is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics."
Data/Software Engineer Co-op,"Toronto, ON",Smart Nora,None,Organic,"The Company
Smart Nora is a Toronto based growth phase company with strong ties to tech, design, hardware, IoT, and consumer health communities. Our focus is on enhancing the sleep and wellbeing of others by inventing practical software and hardware products. Our debut product is the world's most comfortable snoring solution and has been listed on Oprah’s Favorite Things, as well as Good Morning America, TIME, TODAY, and BBC. Good health starts with a great night's sleep.
The Team
We are an ambitious, tight-knit team with an open work environment and a self-directed approach. We work remotely at the moment and have an office at King+Spadina.
The Product
Smart Nora is an over-the-counter, contact-free snoring solution relevant to the 40% of adults who snore. Smart Nora is loved by customers for its comfortable contact-free design that enables users to sleep without any attachments to their face or body.
The Role
We are looking for a Software / Data Engineer. This is a co-op placement, full-time, for the length of your school's coop placement. Ideally we are looking for a 4th-5th year student.
Location
Remote
What you would do
Support the current FW project conducted with our third party project partners by implementing structured testing and documentation
Build supporting tools in Python, Node, or other scripting languages to validate firmware and hardware
Actively participate in Mobile App development project with our third party project partners
Support acoustic performance optimization including microphone and codec gain settings, assisted by automated tests
Be part of the Product team developing the next generation Smart Nora device
Wrangle data and decipher meaning from quantitative tests and analytics
Write clear documentation
Our requirements
Comprehensive understanding of Software Development processes (SDLC)
Comprehensive understanding of Object Oriented Programming (OOP) principles
Experience with Pandas, R, Tableau, or other data processing/visualization tools
Experience with GitHub (send us your profile!)
Exposure to cloud (AWS) and APIs is a plus
Experience in audio processing is a plus
Experience in machine learning is a plus
Majoring in Software Engineering, Computer Science, Industrial & Systems Engineering, or related field.
How to Apply
Please apply below using your resume and links to your LinkedIn, GitHub (if have), and Kaggle (if have) profiles.
Accommodations are available on request for candidates throughout the application process. Please let us know your needs so that we may accommodate. Email careers@smartnora.com if you would like to discuss this role before applying."
Intermediate Data Engineer,"Toronto, ON",QUESTRADE INC,None,Organic,"The Intermediate Data Engineer works in one of the agile BI teams.
The ideal candidate will be an experienced Data Engineer that demonstrates in-depth knowledge and understanding of data warehousing, data integration, reporting and business intelligence. Open-minded and flexible and prepared to work in a very dynamic environment, supporting multiple business units.
JOB RESPONSIBILITIES:
Creating, supporting, and maintaining ongoing operational, managerial, and executive business intelligence infrastructure.
Attention to detail, in particular as it relates to compliance and accuracy of data.
Developing understanding of information sources and correct interpretation of data
Gathering, documenting and analyzing requirements from stakeholders
Meeting and interacting with all levels of management as needed to elicit, define, analyze and document requirements for new business intelligence initiatives.
Designing the conceptual, logical and physical data models necessary to support new reporting and data analysis
Developing data integration processes
QUALIFICATION:
Minimum 3 years of related experience.
Understanding of Data Warehouse lifecycle is a must.
Good knowledge in cloud technologies (preferably GCP)
Advanced knowledge in Python scripting language
Good knowledge in Message Broker systems (Kafka, RabbitMQ, PubSub)
Excellent proficiency in writing SQL queries.
Advanced proficiency with Microsoft BI Suite - SQL Server 2014-2019, SSIS, SSRS.
Understanding relational and dimensional data modeling concepts.
Strong knowledge and comprehension of technology and data management used in the process of collecting, storing and retrieving data.
Post-secondary education, preferably in Math/Statistics or Computer Science.
Superior writing, editing, and communication skills, capacity to interact with all levels of the organization.
Knowledge of latest Microsoft self-service BI tools – Power BI (both desktop and cloud) an asset.
Experience with DAX an asset
Experience and/or personal interest in the financial industry an asset."
Data Engineer (Cloud),"Toronto, ON",StackPros,None,Organic,"StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.
The Cloud Data Engineer will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.
StackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros’ quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.
Key Responsibilities:
Data Engineering-Specific Responsibilities
Participate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projects
Design and develop ETL workflows and datasets to be used in data visualization tools
Write complex SQL queries with multiple joins to automate and manipulate data extracts
Perform end to end Data Validation to maintain accuracy of data sets
Build tools for deployment, monitoring and operations
Troubleshoot and resolve issues in the development, test and production environments
Develop re-useable processes that can be leveraged and standardized for multiple instances
Prepare technical specifications and documentation for projects
Stay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and tools
Understand, implement, and automate security controls, governance processes, and compliance validation
Design, manage, and maintain tools to automate operational processes
Data Science-Specific Responsibilities
Perform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insights
Design experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experiments
Create and design dashboards by using different data visualization tools to present reports and insights, and support business decision making
Help the DRVN Intelligence Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future
Qualifications:
3+ years experience in Data Engineering
Understanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.)
Strong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariate
Excellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholders
Candidate must exhibit an analytical, detail-oriented approach to problem solving
Experience with Jira / Atlassian project management tools is an asset
Company-Wide Responsibilities:
Maintain and exceed client satisfaction with StackPros Inc.’s deliverables, day-to-day work and overall value as a partner
Cultivate opportunities for company growth, always seek areas where StackPros Inc.’s role could be expanded
Adapt to ever-changing client needs and expectations
Maintain dedication toward achieving excellence in StackPros Inc.’s delivery against client needs, and overall success as an organization
Be an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner"
Data Engineer,Remote,Wavo,None,Organic,"Wavo is looking for intermediate and senior data engineers to join our technical team.

Digital advertising, technology, & data are changing the way artists & brands approach advertising. With Wavo you will get the rare chance to create services that will be used by today’s top artists, managers and brands to help them grow their business through Wavo’s advertising products.

Wavo’s technical team is small and efficient, and while we work to a high standard we don’t over-manage with specs. We strive to build a workplace where everyone works hard and gets passionate about the big challenges.

We can promise you will never be bored. We are constantly experimenting, testing new approaches and challenging ourselves to master new skills. We try to think outside of the box and build an environment that’s both exciting and meaningful.

Responsibilities:
Collaborate with other engineers and teams to implement new features, improvements and fixes needed to build and extend our advertising, and data management platform

You might be a good candidate if you:
Have a high degree of proficiency with Python.
Very comfortable with SQL. Experience with Postgres is a plus.
Knowledge of Apache Spark & pySpark required. Good understanding of ETL pipelines is a plus.
Familiarity with Docker, CircleCI, and Kubernetes is a plus.
Are passionate about working with data, and building systems that are highly reliable, maintainable, and scalable
Are a good communicator and enjoy interacting with people
Enjoy being part of a highly collaborative, remote enabled environment
Are comfortable having ownership and control of a project.

Also...
Today we use AWS for most of our Data infrastructure, so familiarity with that platform is a huge plus.
Having experience in ad-tech, machine learning, or data ETL /Data ingestion is also to your advantage.
Finally, all our future projects will be done in a continuous deployment fashion; supported by integrations tests / BDD.

Benefits
======
Competitive compensation based on experience
Competitive Equity
Group health and dental insurance plan
Flexible hours and vacation
Free tickets to shows and festivals
Delicious office snacks
Company outings & activities
A dynamic work environment
Being a part of innovation at the nexus of music, marketing/advertising, and technology

-

If you think you’d be a good fit, please contact careers@wavo.me with a copy of your resume.

Equal Opportunity Employer
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Brampton, ON",Capgemini,None,Organic,"Job Description:
3-6 years
Expertise on Spark Scala.
Ability to develop ETL jobs to implement business logic using Scala (Spark Framework)
Conversant with Hive Database, Able to create HQL scripts and work on Hive tables for data analysis
Performance tuning of the existing Hadoop jobs, able to trouble shoot and fix existing bugs.
Good understanding of Oracle Exadata RDBMS, able to profile telecom data residing in Exadata and derive business rules.
Co lace with business, have working session with business to identify and freeze business logic.
Understanding / experience working on scrum based Agile set up.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
DevOps/SRE Engineer (Big Data),"Vancouver, BC",Global Relay,None,Organic,"Your Role::
Global Relay delivers enterprise services to 23,000 customers in 90 countries, including 22 of the top 25 global banks. Our infrastructure teams provide fantastic opportunities to DevOps Engineers, Site Reliability Engineers and System Administrators who are passionate about massively scalable, big-data architecture, with a strong focus on security.

Deploy, update, and monitor distributed systems
Support multiple on-premise environments
Scale and manage infrastructure with high throughput, availability and storage requirements
Automate all the things

Required Skills:

Experienced system administrator of bare metal, VM and orchestrated deployments
Automation tools such as Ansible/Puppet/Chef/Salt/Fabric
A desire to dig deep to troubleshoot, debug and decouple the layers that comprise distributed systems
Scripting (Bash, Python, etc.)
Ability to analyze complex systems and problems and express them in simple terms

Nice-to-have Skills:

Experience researching and advising on new technology implementations
Experience with any distributed data stores (ArangoDB, Aerospike, Cassandra, CockroachDB, Couchbase, Elasticsearch, Kafka, MinIO, Pulsar, Redis, Hadoop, Scylla etc.)
SQL administration (Postgres, MySQL, SQL Server, Oracle)
Containerization experience (Docker, Swarm, Kubernetes)
Networking experience (DNS, VLANs)
Experience with load balancers (HAProxy)
Building and maintaining CI/CD pipelines (Jenkinsfiles)
Monitoring/logging infrastructure (Splunk, ELK, Graphite, Grafana, Prometheus)

About You:

A problem solver who takes initiative
Effortlessly self-motivates while working on team-based projects
A well organized, thorough and detail oriented person
Able to keep the ""bigger picture"" in mind while prioritizing conflicting demands and tasks
Confident enough to voice your opinion, ask questions and not afraid to suggest a better solution, without being abrasive

About Us::
Global Relay is the leading provider of cloud-based archiving, information governance, surveillance, and eDiscovery solutions to the global finance sector. We help financial services firms preserve and supervise their communications data for regulatory compliance, risk mitigation, and litigation readiness. We deliver our services to over 20,000 customers in 90 countries, including 22 of the top 25 global banks. Our market-leading archiving service supports email, instant messaging, text, market data chat, social media, enterprise social networking, voice, trade data, websites, and more. Recently, we also launched a compliant messaging platform specifically for the finance industry.

Our Global Operations & Development Center is located in Vancouver, BC, Canada. In addition, we have offices in eight other cities across the world, including major financial centers like New York, Chicago, and London.

Over the years, we have won several major awards, including:
Company of the Year from the BC Tech Association
Canada’s 50 Best Small and Medium Employers
Canada’s Top Employers for Young People
Canada’s Top 10 Most Admired Corporate Cultures
Canada’s Best Managed Companies – Platinum
Technology Fast 50 – Leadership
We provide fantastic opportunities to individuals passionate about business and technology. For those with international business aspirations, we offer invaluable opportunities for doing business with some of the world’s largest, most influential firms. Our company is also perfect for those who want to create cool technology using massively scalable, big-data architecture, with a strong focus on mobile.

To learn more about our business, culture, and community involvement, visit www.globalrelay.com."
Senior Software Engineer - Data Engineering,"Toronto, ON",Instacart,None,Organic,"OVERVIEW
We're looking for experienced Data Engineers to join our fast moving team. We work on a range of interesting and challenging problems, from supporting thousands of concurrent shoppers and processing millions of data points in real time, to building enterprise grade solutions for our retailer partners to help them understand their customers better. Our platform is complex, rapidly scaling and processing millions of transactions in real-time all the time. There is a tremendous amount of opportunity in front of us, and joining now gives you a chance to grow your career and interests as we succeed.
ABOUT THE JOB
Be part of a small team, with a large amount of ownership and responsibility for managing things directly.
Ship high-quality solutions with a sense of urgency and speed.
Architect, develop, test and maintain big data pipelines and processing systems
Collaborate with Data Scientists, Machine Learning Engineers and Software Engineers and provide recommendations and actionable strategies for performance enhancements and development of best practices
Own a large part of the process to enforce data governance and privacy while improving data quality and reliability
Have the freedom to suggest and drive organization-wide initiatives
ABOUT YOU
5+ years of experience
Bachelor's degree in Computer Science, Computer Engineering, Electrical Engineering, or equivalent work experience
A blend of product, system and people knowledge that lets you jump into a fast paced environment and contribute from day one
Experience with big data tools and databases (Apache Spark, Apache Hive, Presto, Snowflake, PostgreSQL)
Experience with data pipeline and workflow management tools (Snowplow, Azkaban, Luigi, Airflow, etc)
Strong programming skills in Python and/or Go
Extensive experience working with large codebases and cross-functional teams
Experience with cloud native infrastructure (AWS, Docker, Kubernetes, etc)
Excellent written and verbal communication skills; able to effectively collaborate with diverse teams.
Ability to balance a sense of urgency with shipping high quality and pragmatic solutions
Experience in distributed systems and scale
Experience with AI/Machine Learning/Data Modeling"
Senior Data Engineer,"Victoria, BC",BC IMC,None,Organic,"DEPARTMENT DESCRIPTION
The Technology department is responsible for developing technology solutions that contribute to the achievement of BCI’s mission and long-term goals. The department manages the Corporation’s business applications and information technology infrastructure, providing support to a large group of financial professionals. The department is also responsible for authoring technology-related directives and conducting disaster recovery planning to minimize risk to the Corporation’s delivery of investment services.
The Data & Analytics function is responsible for the governance, architecture and engineering of BCI’s data assets. It also provides reporting, insights and data science capabilities to its customers.
POSITION DESCRIPTION
Reporting to the Director, Data & Analytics, the Senior Data Engineer is responsible for the design, development and implementation of the data and analytics products and projects that enable data science efforts in the organization. The Senior Data Engineer will deliver business value to multiple business areas across the organization and works closely with internal Technology and business area stakeholders. S/he will drive data modernization and innovation, and contributes to a strong data and analytics competency for BCI.
The position can be based in either Vancouver or Victoria with travel between the two cities.
QUALIFICATIONS
MUST HAVE:
Bachelor’s Degree in Technology, Computer Science, Mathematics or a related discipline
A minimum of 5 years of experience as a Data Engineer or Software Engineer professional
Experience with data warehouse and data lake design, development and sustainment
Coding skills and deep proficiency with SQL, Python, etc.
Competent with general scripting/software development
Understanding of data processing performance concerns and issues (configuring database server/data schema for performance, optimizing SQL)
Experience with reporting tools (e.g. Excel, Power BI, Tableau)
Experience with version control systems (e.g. Git)
Experience with cloud platforms
Strong knowledge of data modeling, data architecture and data structures
Strong understanding Agile and DevOps, including CI/CD technologies and practices
Excellent listening, communication, collaboration and problem-solving skills
PREFERRED:
Knowledge of the investment management industry
PRIMARY RESPONSIBILITIES
Collaborates with team members, other IT teams, and customers to understand the organization’s business objectives, data needs and infrastructure needs
Provides technical leadership and creates a culture of customer-centricity, accountability and high performance
Designs, develops and implements the data pipelines and ETL tools and workflows that enable data science efforts within the organization
Works with Data Architects and Data Scientists on the design, development and implementation of operational, transactional and analytical modeling
Proactively identifies risks and issues and proposes solutions to remove barriers
Applies knowledge of DevOps practices including continuous deployment, continuous integration, test-driven development and automated testing
Mentors junior engineers, follows best practices, performs code reviews and architects resilient infrastructure
Solves challenging problems about scale, statistics, infrastructure reliability, latency and more
Leads data mining and collections procedures
Robustly sources, structures, profiles, validates and transforms data for reporting, analysis and data science purposes
Engages with stakeholders to define, design and deliver data sourcing, analysis and reporting solutions
Makes recommendations about the methods used to collect, analyse and manage structured and unstructured data to drive outcomes
Develops solutions (and code) to automate and productionize data sourcing, data structuring and analytical modelling
Analyzes data sources, evaluating and remediating data quality, designing and implementing data sets that can be consumed and re-used by the analytics community across BCI
Helping the business interpret the results of analyses to determine the appropriate course of action
Proactively identifies opportunities to utilize data and analytics to business advantage and prototyping for ‘proof of value’
Assists in troubleshooting and guiding resolution of data analytics related problems in a timely and accurate fashion
Undertakes special projects or assignments as required
Performs other related duties as required
COMPETENCIES
Learning Agility
Effective performers continuously seek new knowledge. They are curious and want to know “why”. They learn quickly and use new information effectively. They create and foster a culture of interest, curiosity, and learning.
Relationship Building
Effective performers establish and proactively maintain a broad network of relationships (e.g. colleagues, co-workers, vendors, suppliers, etc.). They value these relationships and work effectively across the organization by maintaining positive working relationships with peers and others.
High Standards
Effective performers possess a high inner work standard and shows pride in their work. They consistently strive to ensure work is complete within deadlines and that all work performed is of a high quality.
Organization & Planning
Effective performers have strong organizing and planning skills that allow them to be highly productive and efficient. They manage their time wisely and effectively prioritize multiple competing tasks. They follow through on tasks to ensure changes in technology are communicated effectively.
Results Orientation
Effective performers maintain appropriate focus on outcomes and accomplishments. They are motivated by achievement, and persist until the goal is reached. They convey a sense of urgency to make things happen. They respect the need to balance short- and long-term goals. They are driven by a need for closure.
Communicativeness
Effective performers clearly and articulately convey technical and other information both orally and in writing to others in a manner appropriate to the listener. They write clearly, accurately and concisely, composing project, technical and other required documentation as required.
Change Mastery
Effective performers are adaptable. They embrace needed change and modify their behaviour when appropriate to achieve organizational objectives. They are effective in the face of ambiguity. They understand and use change management techniques to help ensure smooth transitions.
Business Thinking
Effective performers see the organization as a series of integrated and interlocking business processes. They understand how their work connects with and affects other areas of the organization."
Data Engineer,"Toronto, ON",OMERS,None,Organic,"Why join us?
Are you looking to join a dynamic pension plan that embodies the strong values of its 500,000 members and is an industry leading global investor? If so, we would love to tell you our story.At OMERS we put our people first and are proud to embrace the diversity of thought and leadership that comes from having locations in Toronto, London, New York, Singapore, Sydney and other major cities across North America and Europe. Our culture is truly one of a kind. We get stuff done, and have fun doing it! We take great pride in contributing to the communities where we live with an ever-constant eye to the global investment markets.

OMERS Products and Technology is looking for Data Engineer passionate about all sides of data analytics and eager to build a data platform for our pension Data engineering team. The individual should be extremely motivated and want to constantly learn and apply new technologies. In this role, the qualified candidate is expected to drive actionable insights; transforming, modelling, processing and extracting value from datasets.

As a member of this team, you will be responsible for:
Develop and maintain data pipelines
Design and implement ETL processes
Hands on experience on Data Modeling – Design conceptual, logical and physical data models with type 1 and type2 dimensions.
Knowledge to move the ETL code base from On-premise to Cloud Architecture
Understanding data lineage and governance for different data sources
Maintaining clean and consistent access to all our data sources
Hands on experience to deploy the code using CI/CD pipelines
Assemble large and complex data sets strategically to meet business requirements
Enable business users to bring data-driven insights into their business decisions through reports and dashboards
To succeed in this role, you have:
Experience with data platform cloud technologies for creating data pipelines – preference GCP Cloud(i.e. Cloud Composer, Cloud Dataflow, Cloud Pub/Sub, Big query etc.)
Experience doing semantic data modeling on Analysis service and publish datasets accordingly.
Understanding security rules and roles in projects, workbooks in Tableau or similar
Possess in-depth understanding of SQL, database management systems and ETL (Extract, transform, load) frameworks
Experience with SSIS, Informatica ETL, data modelling, data warehousing, and business intelligence architecture
Knowledge of programming, scripting (e.g. SQL, PowerShell, Python, C#, Power Query, Visual Studio and SSMS)
Proven track record of learning new technologies quickly
Bachelor’s Degree in Computer Science, Engineering or a related technical field
Experience building data visualizations, preferably Power BI/Tableau
4+ years of experience working with data
Our story:
ABOUT OMERS
Founded in 1962, OMERS is one of Canada’s largest defined benefit pension plans, with $109 billion in net assets as at December 31, 2019. OMERS is a jointly-sponsored pension plan, with 1,000 participating employers ranging from large cities to local agencies, and over half a million active, deferred and retired members. OMERS members include union and non-union employees of municipalities, school boards, local boards, transit systems, electrical utilities, emergency services and children’s aid societies across Ontario. Contributions to the Plan are funded equally by members and employers. OMERS teams work in Toronto, London, New York, Amsterdam, Luxembourg, Singapore, Sydney and other major cities across North America and Europe – serving members and employers and originating and managing a diversified portfolio of high-quality investments in public markets, private equity, infrastructure and real estate. OMERS is committed to having a workforce that reflects the communities in which we live and work. We are an equal opportunity employer committed to a barrier-free recruitment and selection process. At OMERS inclusion and diversity means belonging. How we create a sense of belonging is through our employees and our vast network of Employee Resource Groups. Whether you are passionate about gender, pride, or visible minorities, we have groups that are focused on making a difference in all of our lives."
Data Engineer,"Montréal, QC",Coveo,None,Organic,"Delivering a strong foundation for our data-driven teams
At Coveo, it is our constant obsession to find out innovative ways to put our customer's data to work for them. Luckily for us, said customers are pouring in an ever increasing volume of that primordial data which allows our teams to have a lot of material to work with. But all that data is difficult to collect and even more challenging to make sense of without the right data platform. And that is where you come in as a premier data engineer!

What a typical day looks like:


You start your day with a good coffee, reading news about some new stream processing library Netflix has recently released to enable more efficient pipelines. You take a few notes to discuss with your colleagues at the scrum.
After the scrum, you get to work with the In-Product experience team to define the data format and implement a new data pipeline to gather, analyse and learn from all interactions happening in their clients applications. Based on the client's metrics, you estimate the load at several millions of events daily, so you work hard on optimizing and load testing the new data pipeline.
You get to chat with Mike as you go fill your water bottle: the data pipeline you've set for the ML team a month ago really made the data scientists' lives easier. High-fives all around for that!
You get to lunch, and the team wanted to watch the streaming of a presentation of a new AWS feature coming up that might provide new options to scale our platform.
Back to your desk (or some other location) to continue working on the development of the new in-product experience data pipeline using the latest streaming and data processing technologies.
You show your design, submit your code and the results of the tests for review by your colleagues. Great work: the new pipeline can transform hundreds of millions of events daily in real-time without problems.
A new big client is being on boarded by the Commerce team, and they would like to make sure that all the events have been integrated properly. You take a look at what happens under the hood to make sure everything is flawless!
A member of the Usage Analytics team has a question about the dataset she wants to extract and more specifically how to get the right columns for an analysis she is working on. You take a quick 10 min to discuss with her, and then get back to your code.

What is expected of you:
As a Data Engineer at Coveo, we'll encourage you to innovate, and share ideas on a daily basis. We are looking for candidates with at least 3 years of working experience. Some of the core expertise we're looking for:

Extensive knowledge of SQL.
Experience with Kafka, Kinesis or other streaming platforms.
Experience in Data Lake Architecture, and high volume real-time streaming.
Experience in distributed computing and big data.
Knowledge of data warehouses like Redshift or Snowflake.
Knowledge of best practices in CI/CD, as well as in DevSecOps.
Familiarity with AWS.
Strong software development experience with proficiency in at least one high-level programming language (Java, Scala, Python or equivalent)."
Big Data Engineer - Analytics,"Ottawa, ON",Interset,None,Organic,"Help us catch bad guys with math.

Our team is growing and we’re looking to add a Big Data Engineer - Analytics that can focus on extending our existing analytics platform and related capabilities to add unprecedented analytics flexibility for our customers. This will include enabling Data Scientists to manipulate and combine events and models to extend and customize the analytics in ways that provide unique value for each customer.

Although there is a lot of uncertainty in the market today, especially considering the COVID-19 crisis, we are set up to accommodate fully remote work, and a fully virtual interview, selection, and onboarding process.

We are looking for someone who is passionate about what they do, takes a creative approach to problem-solving and will be the champion for creating innovative machine learning hooks that deliver real value and perform in big data environments.

Here’s what you'll do:

Implement model data flows to support running cutting-edge machine learning techniques on massive amounts of data.
Work with product managers and data scientists to turn new features and algorithms into beautiful, battle-tested code.
Work with the technologies we use to analyze and identify cyber-security threats for our customers (Elasticsearch, Spark, HBase, Kafka, Vertica, NiFi, using Java and Scala).
Work side by side with some of the smartest minds in the fields of machine learning and behavioural analytics.
Create efficient and robust cloud-based solutions, leveraging the best in cloud technologies.

In order to be considered, you must have:

An undergraduate or Master’s degree in Computer Science or equivalent engineering experience.
Strong interest in software design, distributed computing, and databases.
Experience developing in a JVM environment (Java, Scala, Clojure).
At least two years of experience developing with or using Big Data & Analytics stacks/tools such as Hadoop, HBase, Spark, Presto, and Vertica.
Experience implementing and using streaming platforms such as SparkSQL, Flink, Kafka, Storm, etc.
Experience with Kubernetes, Docker, Ansible or any other infrastructure or containerization management/automation platform.
Familiarity leveraging AWS EMR, Azure, GCP cloud technologies best practices to enable the distribution and analysis of big data on the cloud would be considered an asset.

We’d also love it if you had the following (though not required):

Familiarity with data science or machine learning packages (pandas, R, TensorFlow, etc…).
Familiarity with virtualization technologies (VMWare ESX, Docker).
Contributions to open-source software (code, docs or mailing list posts).
Interest in understanding and analyzing diverse types of data.

Interset is an equal opportunity employer. Should you require accommodation in any aspect of our selection process, please contact our recruitment team at hiring (at) interset (dot) com.

About Interset:

We use big data and advanced behavioural analytics to detect and prevent the theft of intellectual property...simply put, we catch bad guys with math. Part of the Micro Focus group of companies, we are a fast-paced, all hands on deck kind of environment where you are respected and listened to from day one. We have a startup feel within the stability and structure of a large global company. We hire people with a wide scope of knowledge and experience that want to jump into self-organizing, cross-functional teams. We manage our own schedules, we support our teammates, and we always make time for fun."
Data Engineer,"Engineer, BC",Dufrain Consulting Ltd,None,Organic,"Data Engineer
London, Edinburgh OR Manchester
We are Dufrain. We’re a market-leading Data Management, Analytics and BI consultancy with offices across the UK, working with some of the biggest names in the Financial Services industry. We are proud to be an agile, client focused consultancy where we all share a common goal.

As we continue to build and strengthen our capability, a number of exciting opportunities have arisen to join our market-leading Data Management and Analytics Consultancy. We are looking for dynamic individuals, with proven experience and strong technical skills to join our teams in our Edinburgh, London and Manchester offices.

What we offer you: Working as a Data Engineer at Dufrain you’ll have the opportunity to work with a creative and highly skilled team of consultants who have expertise in technical delivery, technologies and concepts in areas such as Data Storage, Data Ingestion, Data Integration, Data Warehousing, Data Preparation and Cloud Infrastructure. You will also have your own ‘people coach’ to guide and support you with your career journey with us. Dufrain offers an autonomous environment where you have opportunities to have access to training tools & technical community groups and provides multi sector and international client exposure.

Who we’re looking for: We are looking for Data Engineers who have a genuine interest of rich data and have experience on delivery that contributes to wider business outcomes and be able to concisely articulate to stakeholders and interested parties their role and solutions in a way that can be easily understood.


Essential Requirements:

Strong cloud engineering experience
Skilled in multiple languages such as SQL, Python, Java, Scala, Julia
Hands on experience of cloud platforms such as Azure, AWS & GCP - (Certified to practitioner level in at least one provider)
Experience working with one or more of the tools - Spark, Kafka, Snowflake, Hadoop
Experience working with both SQL and NoSQL foundational tools and databases such as Cassandra, MongoDB
Experience delivering multiple solutions using key techniques such as Data Modelling, DWH, Lakes, ETL, ELT, Virtualisation & Streaming
Solid knowledge of development principles such as ETL, DWH & Streaming
Minimum 2 years’ experience as Data Engineer with previous experience in industry or as SQL developer
Flexibility to travel to and work on client sites within the UK and occasionally Europe
Excellent track record in executive stakeholder management and maintaining valuable relationships.
Takes ownership and accountability for critical initiatives and deliverables both internally and for clients
Awareness of current market trends in data having the ability to influence opinion and decisioning across the Data Management spectrum
A passionate desire and attitude to learn new tools


What to do next: : Please apply via the application link below. Once your application has been received a representative from the Dufrain Recruitment team will review your profile and inform you of the next steps.

Please note: The recruitment process will require any candidates that are shortlisted to complete an online technical assessment.





#LifeAtDufrain #GetBusyLiving
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Toronto, ON",International Financial Group,None,Organic,"Job Title: Data Engineer
Location: Downtown Toronto

The individual our client seeks is highly analytical, self motivated, curious and has a track record of quantitative and qualitative analysis. If this sounds like you please apply below!
Our technology client is past the boot strapping stage of a start up and continues to grow at an accelerated rate. The environment is fast paced and at times both structured and unstructured. This organization will provide you with the challenges you are looking for and the continued growth for learning you are looking for in your career. The office environment is fantastic and the client is all about social change. Our client seeks a passionate and talented Data Engineer who is comfortable working in a very fast paced environment.

Responsibilities:
Help build, scale and maintain the data platform.
Play a key role in data infrastructure, analytics projects, and systems design and development.
Extract the data, transform the data and load the data into a database or data warehouse (ETL).
Introduce new technologies to the environment through research and POCs.
Ensure high quality standards are met (documentation is in place; quality checks are working and data in dashboards is updated according to data sources).
Will assure standards to be followed by the data analysts & data scientists in terms of analytical data gathering and transformations.
Delivering an enterprise data platform that will accelerate the delivery of business intelligence, machine learning and the ability to generate new insights.
You will focus on integrations and data modelling, ETL along with the automation of data sets.

Requirements:
BA/BS degree in Mathematics, Computer Science, Mathematics or related technical field, or equivalent practical experience.
Have a deep technical understanding, hands-on experience in distributed computing, big data, ETL, dimensional modeling, columnar databases and data visualization.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools and environments.
Experience with data modelling techniques for modern data architectures.
Hands-on implementation experience with cloud data platforms (e.g. Azure, GCP, AWS).
Experience in writing software in one or more languages such as Java, C++, Python, Go, Ruby and/or JavaScript.
Soft skills: High performer, polished/high work ethic, smart learner, flexible, organized, has initiative/fast producer.
Knowledge of Ruby on Rails, experience building data visualizations with D3 or in JavaScript / React and familiarity with Postgres would be a “Nice to have”

For consideration please email your resume to ""Eileen@ifgpr.com"" with ""Data Engineer"" in the subject line."
Data Engineer,"Toronto, ON",TES - The Employment Solution,None,Organic,"Job Title- Data Engineer
Location- Downtown, Toronto
Type - Full Time Permanent
Salary - Negotiable + Benefits



Focus on data architecture, best practices, reliability, security, and compliance
Improve and extend ETL, data processing, and analytics processes
Facility with PowerBI, including creating dashboards and data sources
Developing high complexity, fast performing SELECT queries.
Developing T-SQL procedures, functions, triggers, jobs, scripts, etc.
Development of Advanced T-SQL such as temporal tables, PIVOTs, recursive table expressions and more.
Modeling and implementing Data Mart solution for Power BI analytics
Managing indexes, statistics, query plans alerts, database activity, and overall performance activity.
In-depth experience working with relational databases, such as Microsoft SQL Server or PostgreSQL
Enthusiasm for applying good data design, testing, documentation, and support practices
Experience building and optimizing data pipelines, architectures, and data sets
Knowledge of message queueing, stream processing, and data stores/warehouses
Working knowledge of AWS products related to data engineering
Bachelor's degree in Computer Science, Software Engineering or an equivalent

Excellent communication skills - both written and verbal; ability to speak in Spanish is a bonus

To apply please send an email to sheetalk@tes.net"
Data Engineer,Canada,Keyrus,None,Organic,"Keyrus Canada, a leader in Data Intelligence is looking for a Data Engineer.
The Toronto team is expanding rapidly! Our team has doubled in the last two years and is continuing to grow. If you are looking for an innovative startup-style company with a good team-spirit that has the support of an internationally recognized brand, we encourage you to apply and join us!
Who we are:
With offices in 18 countries and more than 20 years of experience in North America, Keyrus is a trusted leader in Data Intelligence.
Keyrus Canada offers stimulating projects to increase companies’ performance, with 2 areas of expertise:
Data Strategy to help organizations identify business objectives, build a strategy, and leverage their data to achieve their goals (BI Roadmap | Master Data Management | Data Governance and Architecture…)
Data Intelligence to enable companies to draw critical insights from their data and shape business decisions (Data Integration | Cloud Migration | Business Intelligence | Analytics & Machine Learning…)
About the role:
We are looking for a Data Engineer expert to join our Toronto office! As a part of the Keyrus team, you will combine your business acumen and statistical knowledge with strong problem solving abilities to analyze large sets of data and deliver insightful, actionable results to our clients.
You will play a key role in providing our clients with business intelligence and ETL solutions to manage their data assets. You will work directly with our client's business to add value to their data intelligence environment through the use of tools such as Alteryx and Tableau. You will be responsible for the entire end to end analytics solution, from backend data engineering, manipulation, and cleansing, to front-end data visualization.
Our ideal candidate:
At least 3+ years of experience as a Business Intelligence Developer, working directly with a tool such as Alteryx, Talend or SSIS for ETL, data manipulation, etc. as well as with a data visualization tool such as Tableau
Experience working with a relational database (SQL, Oracle, etc.) and data modeling
Experience working with business teams to translate functional requirements into technical requirements
Knowledge of data visualization best practices and cloud warehouses (i.e. Snowflake, Redshift) would be an asset
Minimum of a Bachelor's Degree in IT or related field.
What we offer:
A stimulating environment driving you to discover new horizons and surpass yourself
A strong innovative and entrepreneurship DNA
A space that promotes mutual respect, where you can express your ideas and share your opinion
A positive, multicultural work atmosphere and a strong team spirit
Lots of opportunities to celebrate your successes: afterworks, team activities, birthdays, breakfasts and other special events
Benefits such as: insurance, RRSPs, almost full repayment of the transport card, etc."
Machine Learning/Data Engineer,"Toronto, ON",Xanadu Quantum Technologies Inc.,None,Organic,"Summary of Position and Responsibilities

As part of Xanadu’s Machine Learning team, the selected candidate will be responsible for working with a multidisciplinary team of machine learning experts and quantum algorithm developers to bring machine learning models into production. They will develop, deploy, and maintain code, models, and pipelines leveraging various cloud providers and services; automate model training, testing, deployment, and monitoring; and design solution architectures for data driven applications.

Prospective applicants must have strong technical, programming, and mathematical skills. They must possess the ability to evaluate established methods and tools, learn new ones quickly, and apply their knowledge to solve practical problems. Applicants should be self-motivated and demonstrate the ability to successfully meet objectives. Familiarity with quantum computing is not essential for this position, but is a definite plus.

Basic Qualifications and Experience
MSc in Machine Learning, Mathematics, Computer Science, Physics, Engineering, or a related field.
Experience building and deploying production-grade machine learning applications at scale.
Strong software engineering skills across multiple languages (Python, Scala, Java, C++, etc.)
Experience building and supporting development environments for Machine Learning/Data Science teams.
Experience with distributed computing frameworks like Spark, Dask, or Hadoop.
Preferred Qualifications and Experience
PhD in Machine Learning, Mathematics, Computer Science, Physics, Engineering, or a related field.
Solid mathematical understanding of machine learning, statistical modelling, probability theory, and linear algebra.
Experience with frontend and backend web application development.
Passionate about agile software processes, data-driven development, reliability, testing, and continuous delivery.
Familiarity with and experience working in a fast-growing technology start-up environment.
If you are interested in this opportunity, please submit a copy of your CV along with a cover letter outlining why you think this is the right role for you!

Mission (https://xanadu.ai/about)
To build quantum computers that are useful and available to people everywhere. Learn more about our mission here (https://xanadu.ai/about).

Values (https://docs.google.com/presentation/d/1W_9jD_SxlVWMWzRxs1Nx7lHMDPX42aTLCiNTwkUli4s/edit?usp=sharing)
Our values are everything. They are fundamental and lay the foundation for culture at Xanadu. Learn more about our values here (https://docs.google.com/presentation/d/1W_9jD_SxlVWMWzRxs1Nx7lHMDPX42aTLCiNTwkUli4s/edit?usp=sharing).

At Xanadu, we are committed to building an inclusive, safe, and equitable culture and fostering an environment where our employees feel included, valued, and heard. We are committed to meeting the needs of all individuals and support a barrier-free workplace. Should you require accommodations at any point during the recruitment process please contact Human Resources at hr@xanadu.ai."
Data Engineer,"Halifax, NS",MobSquad,None,Organic,"ABOUT MOBSQUAD
We are a well-funded, hyper-growth, scale-up looking for an experienced Data Engineer. If you've ever dreamed of working with a top tier technology company scale-up, on leading edge technologies, backed by the very best venture capitalists in the world, then this is your chance.
Some details about MobSquad:
MobSquad solves the significant and growing technology talent shortage faced by US-based start-ups and scale-ups by enabling our clients to quickly have a turnkey ""virtual"" Canadian subsidiary, where Canadian-based technology professionals work with our clients individually on an exclusive basis
We've been featured on the front page of The Washington Post, on NPR multiple times, The Financial Times (UK), The Globe and Mail, the Calgary Herald, BetaKit, CBC, Global News, and many other places. other media outlets
We're a Certified B Corporation, were recognized as the third Best Place to Work in Canada in 2020, and have made numerous contributions to charitable organizations as well as a financial commitment to the Upside Foundation. We believe we are playing a key role in enhancing Canada's innovation economy, and have received financial support from the Government of Canada, Province of Alberta, Province of Nova Scotia, and City of Calgary, to support this ambition
You can learn more about us on our website
ABOUT THE ROLE
As a Data Engineer, you will be part of a Canada-based team working remotely with a leading US scale-up. Your team will operate alongside many other talented developers and data scientists in Canada, and you will be an integral part of the tech community that MobSquad has built.
This role requires someone who has demonstrated an ability to develop, test, optimize, and maintain scalable databases, architectures, and pipelines that enable data scientists and software developers to easily analyze and work with data. The ideal candidate has worked closely with data scientists and data architects and is an expert at optimizing data flow for use across broader teams. The candidate should be able to generate ideas and create tools that add greater functionality and usability to data systems within the company.
ABOUT YOU
You have a bachelor's degree in Computer Science, Information Technology, Data Science, Applied Math, Physics, Engineering, or a comparable analytical field from an accredited institution
You are expert in modeling, working with database architectures, and relational databases
You have over three years of experience with big data tools, such as Hadoop (MapReduce, Hive, Pig), Spark, and Kafka
You have experience with SQL databases (PostgreSQL, MySQL) and NoSQL databases (Cassandra, MongoDB)
You have experience creating and working with ETL data transformation and integration processes
You have experience working with enterprise-grade cloud computing platforms such as Microsoft Azure, Amazon Web Services (EC2, EMR, RDS, Redshift), and Google Cloud
You have expertise in relevant programming languages (Python, R, C/C++, Java, Pearl, Scala)
You have a deep understanding of data modeling tools (ERWin, Enterprise Architect, Visio)
You have experience optimizing big data pipelines and extracting value from large disconnected datasets
You have the ability to develop high-quality code adhering to industry best practices (i.e., code review, unit tests, revision control)
WHAT YOU'LL GET @MOBSQUAD
A full-time position that offers competitive compensation
A benefits program delivered through our bespoke digital platform, giving you control, choice, and flexibility. We give you the ability to build your package of benefits covering health (e.g., medical, dental, vision), wellness (e.g., gym, workout gear, massage, transit), and RRSP (retirement savings)
A downtown office location with first-rate amenities, surrounded by great restaurants and easily-accessible transit
For international candidates, sponsorship for an immediate work permit, expedited permanent residency, and Canadian citizenship within four years
At MobSquad, we support and encourage building a work environment that is diverse, inclusive, and safe for all. We invite and welcome applicants of all backgrounds, regardless of race, religion, sexual orientation, gender identity, national origin, or disability."
Data Engineer,"Montréal, QC",MarketMuse,None,Organic,"Experienced Data Engineer
Overview
Marketmuse Inc.'s M4 Lab is seeking an experienced Python & Java/Scala engineer to help create the next generations of content analytics and content generation technologies. This role blends production software development, big data, and machine learning. The engineer will work on natural language processing systems that try to understand the semantics, intent, and topical structure of vast amounts of web content at scale. This role requires extensive knowledge and experience in architecting and developing data-intensive applications.
About Us
Marketmuse Inc. is a rapidly growing institutionally-backed content planning technology firm with offices in Montreal, Boston, and New York City. We are the premier provider of enterprise content planning technologies and are recognized as a leading technology for content marketing functions. MarketMuse's new M4 Lab (the MarketMuse Montreal Machine Monograph Lab) will be a hub for our advanced machine learning and data sciences teams working on cutting-edge R&D to improve our systems' quality of content understanding, knowledge representation, and machine learning powered content generation assistance tools. Our software also helps our clients optimize or create content ranging from short blog posts to long whitepapers.
Responsibilities
Design and implement reliably distributed data pipelines
Implement scalable architectures of machine learning prototype solutions geared towards solving natural language processing, knowledge representation, and natural language generation problems
Create parallelized and/or distributed versions of existing algorithms
Maintain and develop existing machine learning models
Collaborate with research scientists, architects and product management to design and program innovative strategic and tactical solutions that meet market needs with respect to functionality, performance, reliability, realistic implementation schedules, and adherence to development goals and principles
Gather and determine requirements for new features from internal colleagues
Required Skills
Experience with architecting data-intensive applications
Experience writing software in Python, Scala and/or Java. Experience working with data structures, algorithms and software design
Knowledge of data warehousing concepts, including data warehouse technical architectures, infrastructure components, tools and environments (such as Apache Beam, Hadoop, Spark, Pig, Hive, MapReduce, Flume)
Experience with test-driven development
Experience working effectively with software engineering teams
Mentor others in achieving their career growth potential
Required Education and Experience Level
At least 5 years of engineering experience with Java/Scala/Python
At least 2 years of distributed or highly threaded software development experience
Preferred But Optional Skills
MS in Computer Science, Computer Engineering or related fields
Experience with workflow tools like Airflow, Luigi, etc.
Experience with data mining or machine learning applications
Hands-on experience implementing new research ideas with a neural network training framework such as Tensorflow, Keras, or PyTorch"
Senior Data Engineer,"Toronto, ON",ASSURANCE,None,Organic,"About Assurance
At Assurance we are disrupting the antiquated and inefficient world of insurance and financial services. Our team of world class software engineers, data scientists, and business professionals are modernizing how people obtain and manage their financial life all through our powerful platform ecosystem. We are rapidly growing as we expand our product offerings and global footprint, and this growth continues to present new and exciting challenges as we push our industry into its future. We eliminate waste throughout the industry and calculate the complex into simple, valuable solutions to improve people's lives. We are humble, driven, and committed to improving the lives of millions.

About the Position
As we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.
To be successful in this role, you must possess the following:
Expertise in modeling data
Experience with Spark, Hadoop/EMR, SQL
Ability to optimize data access for speed/reliability/velocity as needed by the business
Comfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct
Comfort with learning new technologies to help the team explore new solutions to existing problems
A drive to move fast and deliver business value
Excellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.
Business Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.
Enthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.
Bachelors degree in mathematics, statistics, data science or related field of study.
The following additional experience is desired:
Capable of modifying an existing job to add a new field and get it into production within a day.
Capable of creating a new data pipeline/job within 2-3 days.
You have a proven ability to drive business results by building the right infrastructure that enables data-based insights. You are comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes. We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail. You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity. You can work independently, with little oversight or guidance.

At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise. If this sounds like a good fit for you, give us a shout, we’d love to chat!

Please review our CCPA policies here."
Senior Data Engineer- QuantumBlack,"Montréal, QC",McKinsey & Company,None,Organic,"QUALIFICATIONS
Meaningful experience with at least two of the following technologies: Python, Scala, SQL, Java
Commercial client-facing project experience is helpful, including working in close-knit teams
Experience in software engineering best practices such as code reviews, testing frameworks, maintainability and readability
Experience deploying applications into production environments e.g. code packaging, integration testing, monitoring, release management
The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
Meaningful experience in multiple database technologies such as Distributed Processing (Spark, Hadoop, EMR), traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Teradata), NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan)
A confirmed ability in clearly communicating complex solutions
Deep understanding of Information Security principles to ensure compliant handling and management of client data
Experience and interest in Cloud platforms such as: AWS, Azure, Google Platform or Databricks
Confirmed experience in traditional data warehousing / ETL tools (Informatica, Talend, Pentaho, DataStage)
Extraordinary attention to detail
Flexibility to travel regional or internationally up to 80% depending on client and base location.
WHO YOU'LL WORK WITH
You'll join us in Montreal to work closely with our clients and our Data Scientists in order to curate, transform and construct features which feed directly into our modelling approach.
This would be a hybrid client-facing/technical role using state of the art technologies, whilst also being able to communicate complex intractable ideas to non-technical audiences. Collecting clear requirements is a key part of this role and will define the technical strategy the team employs on the study.
Who you are
A core value at QuantumBlack is fusion and at the heart of our multi-disciplinary teams is the belief that the sum of individual parts will always be less than the impact of the entire team. You are a highly collaborative individual who is capable of laying aside your own agenda, listening to and learning from colleagues, challenging thoughtfully and prioritising impact. You search for ways to improve things and work collaboratively with colleagues. You believe in iterative change, experimenting with new approaches, learning and improving to move forward quickly. Trust between colleagues is paramount here – you are an individual who can always be trusted to work in the best interests of all colleagues and to achieve the best outcome for QuantumBlack and our clients. You are naturally enthusiastic and enjoy sharing your passion with others.
WHAT YOU'LL DO
As a Senior Data Engineer at QuantumBlack in Montreal...
You will work in multi-disciplinary environments harnessing data to provide real-world impact for organisations globally. You will influence many of the recommendations our clients need to positively change their businesses and enhance performance.
Role responsibilities
Work with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Understanding, assessing and mapping the data landscape.
Maintaining our Information Security standards on the engagement.
Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models.
Defining the technology stack to be provisioned by our infrastructure team.
Building modular pipeline to construct features and modelling tables.
Use new and creative techniques to deliver impact for our clients as well as internal R&D projects.
Mentoring and developing junior Data Engineers on engagements.
What you’ll learn
How successful projects on real world problems across a variety of industries are completed through referencing past deliveries of end to end pipelines.
Build products alongside the Core engineering team and evolve the engineering process to scale with data, handling complex problems and advanced client situations.
Be focused on the wrangling, clean-up and transformation of data by working alongside the Data Science team which focuses on modelling the data.
Using new technologies and problem-solving skills in a multicultural and creative environment.
You will work on the frameworks and libraries that our teams of Data Scientists and Data Engineers use to progress from data to impact. You will guide global companies through data science solutions to transform their businesses and enhance performance across industries including healthcare, automotive, energy and elite sport.
Real-World Impact– No project is ever the same; we work across multiple sectors, providing unique learning and development opportunities internationally.
Fusing Tech & Leadership– We work with the latest technologies and methodologies and offer first class learning programmes at all levels.
Multidisciplinary Teamwork- Our teams include data scientists, engineers, project managers, UX and visual designers who work collaboratively to enhance performance.
Innovative Work Culture– Creativity, insight and passion come from being balanced. We cultivate a modern work environment through an emphasis on wellness, insightful talks and training sessions.
Striving for Diversity– With colleagues from over 40 nationalities, we recognise the benefits of working with people from all walks of life.
Our projects range from helping pharmaceutical companies bring lifesaving drugs to market quicker to optimising a Formula1 car’s performance. At QuantumBlack you have the best of both worlds; all the benefits of being part of one of the leading management consultancies globally and the autonomy to thrive in a fast growth tech culture:
Healthcare Efficiency– We helped a healthcare provider improve their clinical trial practices by identifying congestion in diagnostic testing as a key indicator of admissions breaches.
Environmental Impact– We designed and built the first data-driven application for a state of the art centre of excellence in urban innovation by collecting real-time data from environmental sensors across London and deploying proprietary analytics to find unexpected patterns in air pollution.
Product Development– We worked with the CEO of an elite automotive organisation to reduce the 18-month car development timeframe by improving processes, designs and team structures.
Please submit your CV in English
Visit our Careers site to watch our video and read about our interview processes and benefits."
Data Engineer,"Montréal, QC",Paytm,None,Organic,"About Paytm Labs:
At Paytm Labs, we’re on a mission to provide useful technological solutions that enrich and empower millions of people in their daily lives. We apply big data, artificial intelligence and machine learning to bring the next generation of financial products and services to the Indian, Japanese and Canadian markets.

As a company, we’re committed to offering the most transparent, secure, and personalized consumer experience to over 500 million users and over 17 million merchants. Since our journey began 6 years ago, we’ve launched the Paytm Canada app (our bill management app), and PayPay (a QR-based payment app in Japan), all while powering the Paytm India app.

Job Description:
If working with billions of events, petabytes of data and optimizing for last millisecond is something that excites you then read on! We are looking for Data Engineers who have seen their fair share of messy data sets and have been able to structure them for building useful AI products.

You will be working on writing frameworks building for real time and batch pipelines to ingest and transform events(108 scale) from 100’s of applications every day. Our ML and Software engineers consume these for building data products like personalization and fraud detection. You will also help optimize the feature pipelines for fast execution and work with software engineers to build event driven microservices.

You will get to put cutting edge tech in production and freedom to experiment with new frameworks, try new ways to optimize and resources to build next big thing in Fintech using data!
Responsibilities:
Work directly with Machine Learning Engineers and Platform Engineering Team to create reusable experimental and production data pipelines.
Understand, tune, and master the processing engines (like Spark, Hive, Samza, etc) used day-to-day.
Keep the data whole, safe, and flowing with expertise on high volume data ingest and streaming platforms (like Spark Streaming, Kafka, etc).
Sheppard and shape the data by developing efficient structures and schema for the data in storage and transit.
Explore as many new technology options for data processing, storage, and share them with the team.
Develop tools and contribute to open source wherever possible.
Adopt problem solving as a way of life – always go to root cause
Qualifications:
Degree in Computer Science, Engineering or a related field
You have previously worked on building serious data pipelines ingesting and transforming > 10^6 events per minute and terabytes of data per day.
You are passionate about producing clean, maintainable and testable code as part of a real-time data pipeline.
You understand how microservices work and are familiar with concepts of data modelling.
You can connect different services and processes together even if you have not worked with them before and follow the flow of data through various pipelines to debug data issues.
You have worked with Spark and Kafka before and have experimented or heard about Flink/Druid/Ignite/Presto/Athena and understand when to use one over the other.
On a bad day maintaining zookeeper and bringing up cluster doesn’t bother you.
You may not be a networking expert but you understand issues with ingesting data from applications in multiple data centres across geographies, on-premise and cloud and will find a way to solve them.
Proficient in Java/Scala/Python/Spark
What we Offer!
Due to the pandemic, we have been and will continue to WFH until it is safe to open our office. Our company culture and values remain at the core of everything we do.
For the third year in a row, we are proud to announce that we have been certified as a Great Place to Work.
We were also certified as one of the Best Workplaces for Mental Wellness in 2020
We are an open work environment that fosters collaboration, ownership, creativity, and urgency
We ensure flexible hours outside of our core working hours
Enrolment in the Group Health Benefits plan right from day 1, no waiting period
To keep things fun and stress-free during COVID-19 we started Virtual Daily, Virtual Weekly and Monthly team bonding activities including: Trivia, Games Nights, Movies Nights, Arts & Crafts (e.g. Origami), Lunch & Learns (e.g. Sign Language 101), Virtual Wellness Sessions (e.g. Meditation, Morning stretches), Virtual Team Ubereats Lunches, and so much more!
We also created and began publishing a monthly internal newsletter with various topics that keeps the tone lighthearted and interesting.

When we are able to open our office, our in-office experience consists of:
Team building events (anything from axe throwing, go-karting, bike riding, etc.)
Fuel for the day: Weekly delivery of groceries, and all types of snacks
Catered lunches and desserts on a monthly basis
Flexibility with WFH
Daily fun in the office with our competitive games of Ping Pong, Pool, Smash Bros competitions, or FIFA
And of course, an unlimited amount of freshly made coffee! We’re pretty serious about our coffee beans.
Notice for Job Applicants
Following the advice of Canadian health authorities, to mitigate the risk of the potential spread of COVID-19 and support social distancing, all recruiting activities including interviews and new hire onboarding will be conducted remotely. While we are doing our best to ensure reasonable response times, please expect potential delays during the recruiting process due to the current situation.

We are an equal opportunity employer and value diversity and uniqueness at our company. We thank all applicants, however, only those selected for an interview will be contacted.

Paytm Labs is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please let us know.

Don't have Paytm Canada App yet?
Check us out in the Google Play or App Store."
